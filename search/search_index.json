{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome! Tutorials This site contains tutorials for using Galaxy Australia . To use the tutorials: see the list in the left hand panel or, click the menu button (three horizontal bars) in the top left of the page Web address: www.usegalaxy.org.au Support Australian Research Data Commons National Research Infrastructure for Australia UQ RCC QCIF Melbourne Bioinformatics Bioplatforms Australia EMBL-ABR UseGalaxy and The Galaxy Training Network","title":"Welcome"},{"location":"#welcome","text":"","title":"Welcome!"},{"location":"#tutorials","text":"This site contains tutorials for using Galaxy Australia . To use the tutorials: see the list in the left hand panel or, click the menu button (three horizontal bars) in the top left of the page Web address: www.usegalaxy.org.au","title":"Tutorials"},{"location":"#support","text":"Australian Research Data Commons National Research Infrastructure for Australia UQ RCC QCIF Melbourne Bioinformatics Bioplatforms Australia EMBL-ABR UseGalaxy and The Galaxy Training Network","title":"Support"},{"location":"galaxy-resources/","text":"Galaxy Resources Galaxy Training Network - note is for usegalaxy.org","title":"Galaxy Resources"},{"location":"galaxy-resources/#galaxy-resources","text":"Galaxy Training Network - note is for usegalaxy.org","title":"Galaxy Resources"},{"location":"genomics/","text":"Overview This section contains tutorials for basic microbial genomics analyses in Galaxy. These have been designed to be done sequentially. Use the Galaxy Platform : a web-based interface for bioinformatic analyses. Import tutorial data to Galaxy : a set of files to use in these introductory tutorials. Quality control of Illumina data : assess raw Illumina data for quality issues. Assemble a genome with Spades : assemble raw Illumina sequence reads into a draft bacterial genome sequence using the tool Spades . Annotate a genome with Prokka : describe the type and location of proteins and other features on the draft bacterial genome sequence using the tool Prokka . Find variants with Snippy : identify differences (such as nucleotide mutations) between genomes using the tool Snippy .","title":"Overview"},{"location":"genomics/#overview","text":"This section contains tutorials for basic microbial genomics analyses in Galaxy. These have been designed to be done sequentially. Use the Galaxy Platform : a web-based interface for bioinformatic analyses. Import tutorial data to Galaxy : a set of files to use in these introductory tutorials. Quality control of Illumina data : assess raw Illumina data for quality issues. Assemble a genome with Spades : assemble raw Illumina sequence reads into a draft bacterial genome sequence using the tool Spades . Annotate a genome with Prokka : describe the type and location of proteins and other features on the draft bacterial genome sequence using the tool Prokka . Find variants with Snippy : identify differences (such as nucleotide mutations) between genomes using the tool Snippy .","title":"Overview"},{"location":"index_gcc/","text":"Welcome! Galaxy Community Conference 2016 This site contains tutorials for using the Microbial Genomics Virtual Lab to perform bioinformatics tasks on bacterial omics data, either on the Unix command line or using the Galaxy system. Tutorials on Assembly and Annotation can be found under the Microbial Genomics tab in the top panel.","title":"Welcome!"},{"location":"index_gcc/#welcome","text":"","title":"Welcome!"},{"location":"index_gcc/#galaxy-community-conference-2016","text":"This site contains tutorials for using the Microbial Genomics Virtual Lab to perform bioinformatics tasks on bacterial omics data, either on the Unix command line or using the Galaxy system. Tutorials on Assembly and Annotation can be found under the Microbial Genomics tab in the top panel.","title":"Galaxy Community Conference 2016"},{"location":"index_mcgill/","text":"Welcome! This site contains tutorials for using the Microbial Genomics Virtual Lab to perform bioinformatics tasks on bacterial omics data, either on the Unix command line or using the Galaxy system.","title":"Welcome!"},{"location":"index_mcgill/#welcome","text":"This site contains tutorials for using the Microbial Genomics Virtual Lab to perform bioinformatics tasks on bacterial omics data, either on the Unix command line or using the Galaxy system.","title":"Welcome!"},{"location":"template/","text":"Phylogenomics Introduction FIXME Learning Objectives At the end of this tutorial you should be able to: FIXME FIXME FIXME Main Tutorial FIXME Note Please take note of this because we care about you. Hint This is a useful tip for you because we like you. Warning Please be careful here, you could really mess things up. What next? FIXME","title":"Phylogenomics"},{"location":"template/#phylogenomics","text":"","title":"Phylogenomics"},{"location":"template/#introduction","text":"FIXME","title":"Introduction"},{"location":"template/#learning-objectives","text":"At the end of this tutorial you should be able to: FIXME FIXME FIXME","title":"Learning Objectives"},{"location":"template/#main-tutorial","text":"FIXME Note Please take note of this because we care about you. Hint This is a useful tip for you because we like you. Warning Please be careful here, you could really mess things up.","title":"Main Tutorial"},{"location":"template/#what-next","text":"FIXME","title":"What next?"},{"location":"tools/","text":"Tools This section includes short training modules about the various tools in the microbial GVL, either within the Galaxy interface or via the commandline. Additional relevant tools and software are also included, particularly for visualization.","title":"Tools"},{"location":"tools/#tools","text":"This section includes short training modules about the various tools in the microbial GVL, either within the Galaxy interface or via the commandline. Additional relevant tools and software are also included, particularly for visualization.","title":"Tools"},{"location":"about/","text":"About Galaxy-Au information. Authors Support Research Data Services Bioplatforms Australia Nectar","title":"Home"},{"location":"about/#about","text":"Galaxy-Au information.","title":"About"},{"location":"about/#authors","text":"","title":"Authors"},{"location":"about/#support","text":"Research Data Services Bioplatforms Australia Nectar","title":"Support"},{"location":"modules/abricate/","text":"Finding antibiotic-resistant genes In this tutorial we will find antibiotic-resistant genes in a bacterial genome. New to Galaxy? First try the introduction and then learn some key tasks Get data Log in to your Galaxy instance (for example, Galaxy Australia, usegalaxy.org.au ). Use shared data If you are using Galaxy Australia, you can import the data from a shared data library. In the top menu bar, go to Shared Data . Click on Data Libraries . Click on Galaxy Australia Training Material: Annotation: Microbial Annotation . Tick the box next to the file. Click the To History button, select As Datasets . Name a new history and click Import . In the top menu bar, click Analyze Data . You should now have one file in your current history. Or, import from the web Only follow this step if unable to load the data files from shared data, as described above . In a new browser tab, go to this webpage: Find the file called contigs.fasta Right click on file name: select copy link address In Galaxy, go to Get Data - Upload File Click Paste/Fetch data A box will appear: paste in link address Click Start Click Close The file will now appear in the top of your history panel. When it has finished uploading, the file name will be green. Shorten file name Click on the pencil icon next to the file name. In the centre Galaxy panel, click in the box under Name Shorten the file name. Then click Save Find antibiotic-resistant genes We will use the tool called ABRicate to find antibiotic-resistant genes in the (draft) genome. ABRicate uses a database of these genes called ResFinder . In the tools panel seach box, type in abricate Click on ABRicate For Input file choose contigs.fasta (or the name of your own assembly file.) Click Execute . There is one output file. Click on the eye icon to view. It should look like this, although likely with a different number of rows. (We have used a miniature genome in this example, so only have a few rows). This shows a table with one line for each antibiotic-resistant gene found, in which contig, at which position, and the % coverage. In the output from Abricate, column 5 has the list of the antibiotic-resistant gene names. Some of these may be complete, exact matches, and some may have a gap/mutation in their sequence which can affect whether that protein is actually expressed. To find out more about what type of AMR genes these are, you can search Genbank with the gene name (e.g. aadD). See this history in Galaxy If you want to see this Galaxy history without performing the steps above: Log in to Galaxy Australia: https://usegalaxy.org.au/ Go to Shared Data Click Histories Click Completed-Abricate-analysis Click Import (at the top right corner) The analysis should now be showing as your current history. What s next? To use the tutorials on this website: see the list in the left hand panel or, click the menu button (three horizontal bars) in the top left of the page You can find more tutorials at the Galaxy Training Network: http://galaxyproject.github.io/training-material/","title":"Antibiotic-resistant genes"},{"location":"modules/abricate/#finding-antibiotic-resistant-genes","text":"In this tutorial we will find antibiotic-resistant genes in a bacterial genome. New to Galaxy? First try the introduction and then learn some key tasks","title":"Finding antibiotic-resistant genes"},{"location":"modules/abricate/#get-data","text":"Log in to your Galaxy instance (for example, Galaxy Australia, usegalaxy.org.au ).","title":"Get data"},{"location":"modules/abricate/#use-shared-data","text":"If you are using Galaxy Australia, you can import the data from a shared data library. In the top menu bar, go to Shared Data . Click on Data Libraries . Click on Galaxy Australia Training Material: Annotation: Microbial Annotation . Tick the box next to the file. Click the To History button, select As Datasets . Name a new history and click Import . In the top menu bar, click Analyze Data . You should now have one file in your current history.","title":"Use shared data"},{"location":"modules/abricate/#or-import-from-the-web","text":"Only follow this step if unable to load the data files from shared data, as described above . In a new browser tab, go to this webpage: Find the file called contigs.fasta Right click on file name: select copy link address In Galaxy, go to Get Data - Upload File Click Paste/Fetch data A box will appear: paste in link address Click Start Click Close The file will now appear in the top of your history panel. When it has finished uploading, the file name will be green.","title":"Or, import from the web"},{"location":"modules/abricate/#shorten-file-name","text":"Click on the pencil icon next to the file name. In the centre Galaxy panel, click in the box under Name Shorten the file name. Then click Save","title":"Shorten file name"},{"location":"modules/abricate/#find-antibiotic-resistant-genes","text":"We will use the tool called ABRicate to find antibiotic-resistant genes in the (draft) genome. ABRicate uses a database of these genes called ResFinder . In the tools panel seach box, type in abricate Click on ABRicate For Input file choose contigs.fasta (or the name of your own assembly file.) Click Execute . There is one output file. Click on the eye icon to view. It should look like this, although likely with a different number of rows. (We have used a miniature genome in this example, so only have a few rows). This shows a table with one line for each antibiotic-resistant gene found, in which contig, at which position, and the % coverage. In the output from Abricate, column 5 has the list of the antibiotic-resistant gene names. Some of these may be complete, exact matches, and some may have a gap/mutation in their sequence which can affect whether that protein is actually expressed. To find out more about what type of AMR genes these are, you can search Genbank with the gene name (e.g. aadD).","title":"Find antibiotic-resistant genes"},{"location":"modules/abricate/#see-this-history-in-galaxy","text":"If you want to see this Galaxy history without performing the steps above: Log in to Galaxy Australia: https://usegalaxy.org.au/ Go to Shared Data Click Histories Click Completed-Abricate-analysis Click Import (at the top right corner) The analysis should now be showing as your current history.","title":"See this history in Galaxy"},{"location":"modules/abricate/#whats-next","text":"To use the tutorials on this website: see the list in the left hand panel or, click the menu button (three horizontal bars) in the top left of the page You can find more tutorials at the Galaxy Training Network: http://galaxyproject.github.io/training-material/","title":"What's next?"},{"location":"modules/access_data/","text":"Public data assembly, annotation, MLST Overview Download a readset from a public database Check the quality of the data and filter Assemble the reads into a draft genome Find antibiotic resistance genes Annotate the genome Find the sequence type (the MLST) Background Sequencing reads (readsets) for more than 100,000 isolates are available on public molecular sequence databases (GenBank/ENA/DDJB): Most of these have been produced using the Illumina sequencing platform. Most of these have no corresponding draft assembly. Not all readsets are of high quality: There may be insufficient reads (usually ~x20 is the minimum read coverage needed). The reads could be from a mixed colony. The classification could be incorrect (both genus and species). It is VERY important to check that what you find in the readset makes sense! Import data Go to your Galaxy instance. Set up a new History for this Activity. In the History panel, click on the cog icon, select Create New . A new empty history should appear; click on Unnamed history and re-name it (e.g. ENA Activity). Choose an accession number. If you are working on this tutorial in a workshop: assign yourself a readset from the table of isolates provided. Put your name in Column B. The accession number for the readset that relates to each isolate is located in Column A. ERR019289 will be used in this demonstration. Alternatively, use accession number ERR019289. This is Vibrio cholerae . In Galaxy, go to the Tools panel on the left, select Get Data EBI SRA . This causes the ENA website to open. Enter the accession number in the ENA search bar. (The search may find reads under Experiment and Run. If so, click on the Accession number under Run .) Find the column called Fastq files (galaxy) . Click on File 1 . This file will download to your Galaxy history, and will return you to the Galaxy page. Repeat the above steps for Get Data EBI SRA and download File 2 . The files should now be in your Galaxy history. Click on the pencil icon next to File 1. Re-name it ERR019289_1.fastq.gz . Save Change the datatype to fastqsanger (note: not fastqCsanger). Save Repeat for File 2 (name it ERR019289_2.fastq.gz ). Evaluate quality We will run FastQC on the pair of fastq files. In the Galaxy tools panel, go to NGS Analysis: NGS QC and manipulation: FastQC . Choose the Multiple datasets icon and then select both fastq files. Your Galaxy window should look like this: Click Execute The output (4 files) will appear at the top of your Galaxy history. Click on the eye icon next to FastQC on data 1: Web page Scroll through the results. Take note of the maximum read length ( e.g. 54 bp). Trim In this step we will remove adapters and trim low-quality sequence from the reads. In the Galaxy tools panel, go to NGS Analysis: NGS QC and manipulation: Trimmomatic Leave settings as they are except for: Input FASTQ file R1 - check this is File 1 Input FASTQ file R2 - check this is File 2 Under Perform initial ILLUMINACLIP step choose Yes Under Adapter sequences to use choose Nextera(paired-ended) This trims particular adapters from the sequences. Under Trimmomatic Operation leave the settings as they are. We will use the average quality across a 4-base sliding window to identify and delete bad sequence (and the flanking bases to the start or end of the sequences - whichever is nearest to the patch of poor quality sequence) Your tool interface should look like this: Click Execute There are four output files. Because trimmomatic might have trimmed some reads to zero, there are now some reads reads with no pair. These are in the unpaired output files. These can be deleted (with the cross button). Re-name the other two output files, e.g. as ERRxxxxx_T1.fastq.gz ERRxxxxx_T2.fastq.gz . These properly paired fastq files will be the input for the Spades assembly. Assemble We will assemble the trimmed reads. In the left hand tools panel, go to NGS Analysis: NGS Assembly: spades . Leave the parameters as their defaults except: Careful correction? No Kmers to use, separated by commas: 21,33,51 chosen kmers must be shorter than the maximum read length (see the FastQC output: sequence length) Coverage Cutoff: Off using a coverage cutoff might cause a problem if there are high-copy-number plasmids Forward reads: ERR019289_T1.fastq.gz Reverse reads: ERR019289_T2.fastq.gz Your tool interface should look like this: Click Execute There are five output files. SPAdes contigs (fasta) SPAdes scaffolds (fasta) : The draft genome assembly. (These should be identical with the conditions used here.) SPAdes contig stats SPAdes scaffold stats : A list of all the contigs and sizes in each of these files. SPAdes log : A summary of the assembly run. Rename SPAdes contigs (fasta) to something like ERR019289.fasta . Check the size of your draft genome sequence If you only have a few contigs, you can estimate the size from the SPAdes contig stats file by adding together the contig sizes. Alternatively, go to NGS Common Toolsets: Fasta Statistics and input the SPAdes contigs (fasta) file. Click Execute . The output will show the draft genome size next to num_bp . Compare your assembly size to others of the same species Go to the NCBI website: Genome Next to Genome , enter the name of your species; e.g. Vibrio cholerae . Click on Genome ASsembly and Annotation report View the table. Click on the Size column to sort by size. (Check for additional pages at the bottom right.) Is your assembly size similar? Find antibiotic resistance genes Now that we have our draft genome sequence, we can search for particular genes. We will use the tool called ABRicate to find antibiotic resistance genes in the genome. ABRicate uses a database of these genes called ResFinder . In the tools panel, go to NGS Analysis: NGS Annotation: ABRicate . For Select fasta file choose SPAdes contigs (fasta) or whatever you renamed it (e.g. ERR019289.fasta). Click Execute . There is one output file. Click on the eye icon to view. This shows a table with one line for each antibiotic resistance gene found, in which contig, at which position, and the % coverage. Find the sequence type (MLST) Bacterial samples (isolates) are often assigned a sequence type . This is a number that defines the particular combination of alleles in that isolate, e.g. ST248. Because several genes (loci) are used, this is termed Multi-Locus Sequence Typing (MLST). There are different MLST schemes for different groups of bacteria. In the tools panel, go to NGS Analysis: NGS Annotation: MLST Under input_file choose choose SPAdes contigs (fasta) or whatever you renamed it (e.g. ERR019289.fasta). Note: a specific MLST scheme can be specified if you wish, but by default all schemes are searched Click Execute . There is one output file. Click on the eye icon to view. There is a one line output. Some symbols are used to describe missing or inexact matches to alleles: n: Exact intact allele ~n : Novel allele similar to n n,m : Multiple alleles - : Allele missing Annotate We have found a list of resistance genes in the draft sequence, but we can also annotate the whole genome to find all the genes present. In the tools panel, go to Tools NGS Analysis NGS: Annotation Prokka Set the following parameters (leave everything else unchanged): Contigs to annotate : SPAdes contigs (fasta) (or equivalent) Locus tag prefix ( locustag) : P Force GenBank/ENA/DDJB compliance ( compliant) : No Sequencing Centre ID ( centre) : V Click Execute There are several output files: gff : the master annotation in GFF format, containing both sequences and annotations gbk : a standard GenBank file derived from the master .gff. If the input to prokka was a multi-FASTA, then this will be a multi-GenBank, with one record for each sequence fna : nucleotide FASTA file of the input contig sequences faa : protein FASTA file of the translated CDS sequences ffn : nucleotide FASTA file of all the annotated sequences, not just CDS sqn : an ASN1 format Sequin file for submission to GenBank. It needs to be edited to set the correct taxonomy, authors, related publication, etc. fsa : nucleotide FASTA file of the input contig sequences, used by tbl2asn to create the .sqn file. It is mostly the same as the .fna file, but with extra Sequin tags in the sequence description lines tbl : Feature Table file, used by tbl2asn to create the .sqn file err : unacceptable annotations - the NCBI discrepancy report log : contains all the output that Prokka produced during its run txt : statistics relating to the annotated features found Tabulate If you are working on this tutorial as part of a class workshop: Go to the table of isolates and add information about genome size, GC content, and number of contigs. Next View the annotated genome in Artemis or JBrowse.","title":"Home"},{"location":"modules/access_data/#public-data-assembly-annotation-mlst","text":"","title":"Public data &rarr; assembly, annotation, MLST"},{"location":"modules/access_data/#overview","text":"Download a readset from a public database Check the quality of the data and filter Assemble the reads into a draft genome Find antibiotic resistance genes Annotate the genome Find the sequence type (the MLST)","title":"Overview"},{"location":"modules/access_data/#background","text":"Sequencing reads (readsets) for more than 100,000 isolates are available on public molecular sequence databases (GenBank/ENA/DDJB): Most of these have been produced using the Illumina sequencing platform. Most of these have no corresponding draft assembly. Not all readsets are of high quality: There may be insufficient reads (usually ~x20 is the minimum read coverage needed). The reads could be from a mixed colony. The classification could be incorrect (both genus and species). It is VERY important to check that what you find in the readset makes sense!","title":"Background"},{"location":"modules/access_data/#import-data","text":"Go to your Galaxy instance. Set up a new History for this Activity. In the History panel, click on the cog icon, select Create New . A new empty history should appear; click on Unnamed history and re-name it (e.g. ENA Activity). Choose an accession number. If you are working on this tutorial in a workshop: assign yourself a readset from the table of isolates provided. Put your name in Column B. The accession number for the readset that relates to each isolate is located in Column A. ERR019289 will be used in this demonstration. Alternatively, use accession number ERR019289. This is Vibrio cholerae . In Galaxy, go to the Tools panel on the left, select Get Data EBI SRA . This causes the ENA website to open. Enter the accession number in the ENA search bar. (The search may find reads under Experiment and Run. If so, click on the Accession number under Run .) Find the column called Fastq files (galaxy) . Click on File 1 . This file will download to your Galaxy history, and will return you to the Galaxy page. Repeat the above steps for Get Data EBI SRA and download File 2 . The files should now be in your Galaxy history. Click on the pencil icon next to File 1. Re-name it ERR019289_1.fastq.gz . Save Change the datatype to fastqsanger (note: not fastqCsanger). Save Repeat for File 2 (name it ERR019289_2.fastq.gz ).","title":"Import data"},{"location":"modules/access_data/#evaluate-quality","text":"We will run FastQC on the pair of fastq files. In the Galaxy tools panel, go to NGS Analysis: NGS QC and manipulation: FastQC . Choose the Multiple datasets icon and then select both fastq files. Your Galaxy window should look like this: Click Execute The output (4 files) will appear at the top of your Galaxy history. Click on the eye icon next to FastQC on data 1: Web page Scroll through the results. Take note of the maximum read length ( e.g. 54 bp).","title":"Evaluate quality"},{"location":"modules/access_data/#trim","text":"In this step we will remove adapters and trim low-quality sequence from the reads. In the Galaxy tools panel, go to NGS Analysis: NGS QC and manipulation: Trimmomatic Leave settings as they are except for: Input FASTQ file R1 - check this is File 1 Input FASTQ file R2 - check this is File 2 Under Perform initial ILLUMINACLIP step choose Yes Under Adapter sequences to use choose Nextera(paired-ended) This trims particular adapters from the sequences. Under Trimmomatic Operation leave the settings as they are. We will use the average quality across a 4-base sliding window to identify and delete bad sequence (and the flanking bases to the start or end of the sequences - whichever is nearest to the patch of poor quality sequence) Your tool interface should look like this: Click Execute There are four output files. Because trimmomatic might have trimmed some reads to zero, there are now some reads reads with no pair. These are in the unpaired output files. These can be deleted (with the cross button). Re-name the other two output files, e.g. as ERRxxxxx_T1.fastq.gz ERRxxxxx_T2.fastq.gz . These properly paired fastq files will be the input for the Spades assembly.","title":"Trim"},{"location":"modules/access_data/#assemble","text":"We will assemble the trimmed reads. In the left hand tools panel, go to NGS Analysis: NGS Assembly: spades . Leave the parameters as their defaults except: Careful correction? No Kmers to use, separated by commas: 21,33,51 chosen kmers must be shorter than the maximum read length (see the FastQC output: sequence length) Coverage Cutoff: Off using a coverage cutoff might cause a problem if there are high-copy-number plasmids Forward reads: ERR019289_T1.fastq.gz Reverse reads: ERR019289_T2.fastq.gz Your tool interface should look like this: Click Execute There are five output files. SPAdes contigs (fasta) SPAdes scaffolds (fasta) : The draft genome assembly. (These should be identical with the conditions used here.) SPAdes contig stats SPAdes scaffold stats : A list of all the contigs and sizes in each of these files. SPAdes log : A summary of the assembly run. Rename SPAdes contigs (fasta) to something like ERR019289.fasta . Check the size of your draft genome sequence If you only have a few contigs, you can estimate the size from the SPAdes contig stats file by adding together the contig sizes. Alternatively, go to NGS Common Toolsets: Fasta Statistics and input the SPAdes contigs (fasta) file. Click Execute . The output will show the draft genome size next to num_bp . Compare your assembly size to others of the same species Go to the NCBI website: Genome Next to Genome , enter the name of your species; e.g. Vibrio cholerae . Click on Genome ASsembly and Annotation report View the table. Click on the Size column to sort by size. (Check for additional pages at the bottom right.) Is your assembly size similar?","title":"Assemble"},{"location":"modules/access_data/#find-antibiotic-resistance-genes","text":"Now that we have our draft genome sequence, we can search for particular genes. We will use the tool called ABRicate to find antibiotic resistance genes in the genome. ABRicate uses a database of these genes called ResFinder . In the tools panel, go to NGS Analysis: NGS Annotation: ABRicate . For Select fasta file choose SPAdes contigs (fasta) or whatever you renamed it (e.g. ERR019289.fasta). Click Execute . There is one output file. Click on the eye icon to view. This shows a table with one line for each antibiotic resistance gene found, in which contig, at which position, and the % coverage.","title":"Find antibiotic resistance genes"},{"location":"modules/access_data/#find-the-sequence-type-mlst","text":"Bacterial samples (isolates) are often assigned a sequence type . This is a number that defines the particular combination of alleles in that isolate, e.g. ST248. Because several genes (loci) are used, this is termed Multi-Locus Sequence Typing (MLST). There are different MLST schemes for different groups of bacteria. In the tools panel, go to NGS Analysis: NGS Annotation: MLST Under input_file choose choose SPAdes contigs (fasta) or whatever you renamed it (e.g. ERR019289.fasta). Note: a specific MLST scheme can be specified if you wish, but by default all schemes are searched Click Execute . There is one output file. Click on the eye icon to view. There is a one line output. Some symbols are used to describe missing or inexact matches to alleles: n: Exact intact allele ~n : Novel allele similar to n n,m : Multiple alleles - : Allele missing","title":"Find the sequence type (MLST)"},{"location":"modules/access_data/#annotate","text":"We have found a list of resistance genes in the draft sequence, but we can also annotate the whole genome to find all the genes present. In the tools panel, go to Tools NGS Analysis NGS: Annotation Prokka Set the following parameters (leave everything else unchanged): Contigs to annotate : SPAdes contigs (fasta) (or equivalent) Locus tag prefix ( locustag) : P Force GenBank/ENA/DDJB compliance ( compliant) : No Sequencing Centre ID ( centre) : V Click Execute There are several output files: gff : the master annotation in GFF format, containing both sequences and annotations gbk : a standard GenBank file derived from the master .gff. If the input to prokka was a multi-FASTA, then this will be a multi-GenBank, with one record for each sequence fna : nucleotide FASTA file of the input contig sequences faa : protein FASTA file of the translated CDS sequences ffn : nucleotide FASTA file of all the annotated sequences, not just CDS sqn : an ASN1 format Sequin file for submission to GenBank. It needs to be edited to set the correct taxonomy, authors, related publication, etc. fsa : nucleotide FASTA file of the input contig sequences, used by tbl2asn to create the .sqn file. It is mostly the same as the .fna file, but with extra Sequin tags in the sequence description lines tbl : Feature Table file, used by tbl2asn to create the .sqn file err : unacceptable annotations - the NCBI discrepancy report log : contains all the output that Prokka produced during its run txt : statistics relating to the annotated features found Tabulate If you are working on this tutorial as part of a class workshop: Go to the table of isolates and add information about genome size, GC content, and number of contigs.","title":"Annotate"},{"location":"modules/access_data/#next","text":"View the annotated genome in Artemis or JBrowse.","title":"Next"},{"location":"modules/artemis/","text":"Introduction to Artemis Background Artemis is a genome browser and annotation tool. Artemis is written in Java and can be used on Mac, Linux or Windows. While it can be used to view any genomic data, it is specifically designed for viewing bacterial or viral genomes. A variety of file types can be loaded and viewed using Artemis; these include FASTA, EMBL, GenBank and GFF3 format files. Learning objectives At the end of this tutorial, you should be able to: install Artemis on your laptop download a bacterial genome sequence, and open the genome sequence in Artemis and look at the sequence features. Install Artemis Artemis is available at no cost from the Sanger Institute Website. Instructions for downloading and installing Artemis can be found at: http://www.sanger.ac.uk/science/tools/artemis Download and run. Not working? The most common problem: Java is either not installed or too old. Artemis needs a recent version of Java v1.6 or higher. Go to http://java.com and update. Still not working? Seek help! Download a bacterial genome We will download the sequence of Leptospira borgpetersenii serovar Hardjobovis Chromosome II from the NCBI website in GenBank format. Go to http://www.ncbi.nlm.nih.gov Select the \u201cTaxonomy\u201d database from the list on the left hand side. Enter \u201cLeptospira borgpetersenii\u201d in the search box and click Search . Click on the species name: Click on the species name again (at the top of the list): Next to Genome , click on the far right number 1 . Click on Genome Assembly and Annotation report : Then, for the first species listed, scroll to the far right of the table, see chromosome 2, and click the link to the annotated assembly number NC_008509.1 (not CP0003): Then, click Send choose Complete Record , Destination File , Format GenBank (full) click Create File . Note the download location (e.g. Downloads folder). Open the GenBank file in Artemis Open Artemis Menu File Open Select file from dialog box, and OPEN The Artemis interface There are three main panes: The overview The DNA view Text summary of features Overview: 6-frame annotation annotated genome features are highlighted black lines are stop codons DNA view: 6-frame translation DNA sequence in the middle amino acid translations above and below Text summary: text summary of features Navigation: Go to the overview pane click on one annotated feature (highlighted in blue) it will be summarized on the top line ( selected feature ) the corresponding sequence will be highlighted in the DNA view pane the corresponding feature will be higlighted in the text summary pane now double click on the same annotated feature all three panes will be centred for this feature to move left or right, use the horizontal scroll bars under each pane to zoom, use the vertical scroll bars on the right What next Links to more information: the Artemis manual. Assemble a bacterial genome using Spades.","title":"Introduction to Artemis"},{"location":"modules/artemis/#introduction-to-artemis","text":"","title":"Introduction to Artemis"},{"location":"modules/artemis/#background","text":"Artemis is a genome browser and annotation tool. Artemis is written in Java and can be used on Mac, Linux or Windows. While it can be used to view any genomic data, it is specifically designed for viewing bacterial or viral genomes. A variety of file types can be loaded and viewed using Artemis; these include FASTA, EMBL, GenBank and GFF3 format files.","title":"Background"},{"location":"modules/artemis/#learning-objectives","text":"At the end of this tutorial, you should be able to: install Artemis on your laptop download a bacterial genome sequence, and open the genome sequence in Artemis and look at the sequence features.","title":"Learning objectives"},{"location":"modules/artemis/#install-artemis","text":"Artemis is available at no cost from the Sanger Institute Website. Instructions for downloading and installing Artemis can be found at: http://www.sanger.ac.uk/science/tools/artemis Download and run. Not working? The most common problem: Java is either not installed or too old. Artemis needs a recent version of Java v1.6 or higher. Go to http://java.com and update. Still not working? Seek help!","title":"Install Artemis"},{"location":"modules/artemis/#download-a-bacterial-genome","text":"We will download the sequence of Leptospira borgpetersenii serovar Hardjobovis Chromosome II from the NCBI website in GenBank format. Go to http://www.ncbi.nlm.nih.gov Select the \u201cTaxonomy\u201d database from the list on the left hand side. Enter \u201cLeptospira borgpetersenii\u201d in the search box and click Search . Click on the species name: Click on the species name again (at the top of the list): Next to Genome , click on the far right number 1 . Click on Genome Assembly and Annotation report : Then, for the first species listed, scroll to the far right of the table, see chromosome 2, and click the link to the annotated assembly number NC_008509.1 (not CP0003): Then, click Send choose Complete Record , Destination File , Format GenBank (full) click Create File . Note the download location (e.g. Downloads folder).","title":"Download a bacterial genome"},{"location":"modules/artemis/#open-the-genbank-file-in-artemis","text":"Open Artemis Menu File Open Select file from dialog box, and OPEN","title":"Open the GenBank file in Artemis"},{"location":"modules/artemis/#the-artemis-interface","text":"There are three main panes: The overview The DNA view Text summary of features Overview: 6-frame annotation annotated genome features are highlighted black lines are stop codons DNA view: 6-frame translation DNA sequence in the middle amino acid translations above and below Text summary: text summary of features Navigation: Go to the overview pane click on one annotated feature (highlighted in blue) it will be summarized on the top line ( selected feature ) the corresponding sequence will be highlighted in the DNA view pane the corresponding feature will be higlighted in the text summary pane now double click on the same annotated feature all three panes will be centred for this feature to move left or right, use the horizontal scroll bars under each pane to zoom, use the vertical scroll bars on the right","title":"The Artemis interface"},{"location":"modules/artemis/#what-next","text":"Links to more information: the Artemis manual. Assemble a bacterial genome using Spades.","title":"What next"},{"location":"modules/breseq/","text":"Breseq Overview Breseq is a tool to find variants, by comparing sequencing reads against a reference. http://barricklab.org/twiki/bin/view/Lab/ToolsBacterialGenomeResequencing https://www.ncbi.nlm.nih.gov/pubmed?Db=pubmed Cmd=ShowDetailView TermToSearch=24838886 Get data ref sequence in .gbk R1.fq R2.fq Run breseq -j 16 -r ref.gbk R1.fq R2.fq -o output -j number of cores -o output dir Results Navigate to output directory. use readlink command go into own mac terminal rsync -av (output) then open in browser address bar","title":"Breseq"},{"location":"modules/breseq/#breseq","text":"","title":"Breseq"},{"location":"modules/breseq/#overview","text":"Breseq is a tool to find variants, by comparing sequencing reads against a reference. http://barricklab.org/twiki/bin/view/Lab/ToolsBacterialGenomeResequencing https://www.ncbi.nlm.nih.gov/pubmed?Db=pubmed Cmd=ShowDetailView TermToSearch=24838886","title":"Overview"},{"location":"modules/breseq/#get-data","text":"ref sequence in .gbk R1.fq R2.fq","title":"Get data"},{"location":"modules/breseq/#run","text":"breseq -j 16 -r ref.gbk R1.fq R2.fq -o output -j number of cores -o output dir","title":"Run"},{"location":"modules/breseq/#results","text":"Navigate to output directory. use readlink command go into own mac terminal rsync -av (output) then open in browser address bar","title":"Results"},{"location":"modules/cmdline_assembly/","text":"Pacbio reads: assembly with command line tools Keywords: de novo assembly, PacBio, PacificBiosciences, Illumina, command line, Canu, Circlator, BWA, Spades, Pilon, Microbial Genomics Virtual Laboratory This tutorial demonstrates how to use long Pacbio sequence reads to assemble a bacterial genome, including correcting the assembly with short Illumina reads. Note: this tutorial uses canu v1.5. Resources Tools (and versions) used in this tutorial include: canu 1.5 infoseq and sizeseq (part of EMBOSS) 6.6.0.0 circlator 1.5.1 [recently updated] bwa 0.7.15 samtools 1.3.1 spades 3.10.1 makeblastdb and blastn (part of blast) 2.4.0+ pilon 1.20 Learning objectives At the end of this tutorial, be able to: Assemble and circularise a bacterial genome from PacBio sequence data. Recover small plasmids missed by long read sequencing, using Illumina data Explore the effect of polishing assembled sequences with a different data set. Overview Simplified version of workflow: Get data The files we need are: pacbio.fastq.gz : the PacBio reads illumina_R1.fastq.gz : the Illumina forward reads illumina_R2.fastq.gz : the Illumina reverse reads If you already have the files, skip forward to next section, Assemble . Otherwise, this section has information about how to find and move the files: PacBio files Open the command line. Navigate to or create the directory in which you want to work. If the files are already on your server, you can symlink by using ln -s real_file_path [e.g. data/sample_name/pacbio1.fastq.gz] chosen_symlink_name [e.g. pacbio1.fastq.gz] Alternatively, obtain the input files from elsewhere, e.g. from the BPA portal. (You will need a password.) Pacbio files are often stored in the format: Sample_name/Cell_name/Analysis_Results/long_file_name_1.fastq.gz We will use the longfilename.subreads.fastq.gz files. The reads are usually split into three separate files because they are so large. Right click on the first subreads.fastq.gz file and copy link address . In the command line, type: wget --user username --password password [paste link URL for file] Repeat for the other two subreads.fastq.gz files. Join the files: cat pacbio*.fastq.gz pacbio.fastq.gz If the files are not gzipped, type: cat pacbio*.fastq | gzip pacbio.fastq.gz Illumina files We will also use 2 x Illumina (Miseq) fastq.gz files. These are the R1.fastq.gz and R2.fastq.gz files. Symlink or wget these files as described above for PacBio files. Shorten the name of each of these files: mv longfilename_R1.fastq.gz illumina_R1.fastq.gz mv longfilename_R2.fastq.gz illumina_R2.fastq.gz Sample information The sample used in this tutorial is a gram-positive bacteria called Staphylococcus aureus (sample number 25747). This particular sample is from a strain that is resistant to the antibiotic methicillin (a type of penicillin). It is also called MRSA: methicillin-resistant Staphylococcus aureus . It was isolated from (human) blood and caused bacteraemia, an infection of the bloodstream. Assemble We will use the assembly software called Canu . Run Canu with these commands: canu -p canu -d canu_outdir genomeSize=2.8m -pacbio-raw pacbio.fastq.gz the first canu tells the program to run -p canu names prefix for output files ( canu ) -d canu_outdir names output directory ( canu_outdir ) genomeSize only has to be approximate. e.g. Staphylococcus aureus , 2.8m e.g. Streptococcus pyogenes , 1.8m Canu will correct, trim and assemble the reads. Various output will be displayed on the screen. Check the output Move into canu_outdir and ls to see the output files. The canu.contigs.fasta are the assembled sequences. The canu.unassembled.fasta are the reads that could not be assembled. The canu.correctedReads.fasta.gz are the corrected Pacbio reads that were used in the assembly. The canu.file.gfa is the graph of the assembly. Display summary information about the contigs: ( infoseq is a tool from EMBOSS ) infoseq canu.contigs.fasta This will show the contigs found by Canu. e.g., - tig00000001 2851805 This looks like a chromosome of approximately 2.8 million bases. This matches what we would expect for this sample. For other data, Canu may not be able to join all the reads into one contig, so there may be several contigs in the output. Also, the sample may contain some plasmids and these may be found full or partially by Canu as additional contigs. Change Canu parameters if required If the assembly is poor with many contigs, re-run Canu with extra sensitivity parameters; e.g. canu -p prefix -d outdir corMhapSensitivity=high corMinCoverage=0 genomeSize=2.8m -pacbio-raw pacbio.fastq.gz Questions Question How do long- and short-read assembly methods differ? Answer short reads: De Bruijn graphs; long reads: a move back towards simpler overlap-layout-consensus methods. Question Where can we find out the what the approximate genome size should be for the species being assembled? Answer NCBI Genomes - enter species name - click on Genome Assembly and Annotation report - sort table by clicking on the column header Size (Mb) - look at range of sizes in this column. Question In the assembly output, what are the unassembled reads? Why are they there? Answer Question What are the corrected reads? How did canu correct the reads? Answer Question Where could you view the output .gfa and what would it show? Answer Trim and circularise Run Circlator Circlator identifies and trims overhangs (on chromosomes and plasmids) and orients the start position at an appropriate gene (e.g. dnaA). It takes in the assembled contigs from Canu, as well as the corrected reads prepared by Canu. Overhangs are shown in blue: Adapted from Figure 1. Hunt et al. Genome Biology 2015 Move back into your main analysis folder. Run Circlator: circlator all --threads 8 --verbose canu_outdir/canu.contigs.fasta canu_outdir/canu.correctedReads.fasta.gz circlator_outdir --threads is the number of cores: change this to an appropriate number --verbose prints progress information to the screen canu_outdir/canu.contigs.fasta is the file path to the input Canu assembly canu_outdir/canu.correctedReads.fasta.gz is the file path to the corrected Pacbio reads - note, fastA not fastQ circlator_outdir is the name of the output directory. Some output will print to screen. When finished, it should say Circularized x of x contig(s) . Check the output Move into the circlator_outdir directory and ls to list files. Were the contigs circularised? : less 04.merge.circularise.log Yes, the contig was circularised (last column). Type q to exit. Where were the contigs oriented (which gene)? : less 06.fixstart.log Look in the gene_name column. The contig has been oriented at tr|A0A090N2A8|A0A090N2A8_STAAU, which is another name for dnaA. This is typically used as the start of bacterial chromosome sequences. What are the trimmed contig sizes? : infoseq 06.fixstart.fasta tig00000001 2823331 (28564 bases trimmed) This trimmed part is the overlap. Re-name the contigs file : The trimmed contigs are in the file called 06.fixstart.fasta . Re-name it contig1.fasta : mv 06.fixstart.fasta contig1.fasta Open this file in a text editor (e.g. nano: nano contig1.fasta ) and change the header to chromosome . Move the file back into the main folder ( mv contig1.fasta ../ ). Options If all the contigs have not circularised with Circlator, an option is to change the --b2r_length_cutoff setting to approximately 2X the average read depth. Questions Question Were all the contigs circularised? Why/why not? Answer Question Circlator can set the start of the sequence at a particular gene. Which gene does it use? Is this appropriate for all contigs? Answer Uses dnaA for the chromosomal contig. For other contigs, uses a centrally-located gene. However, ideally, plasmids would be oriented on a gene such as repA. It is possible to provide a file to Circlator to do this. Find smaller plasmids Pacbio reads are long, and may have been longer than small plasmids. We will look for any small plasmids using the Illumina reads. This section involves several steps: Use the Canu+Circlator output of a trimmed assembly contig. Map all the Illumina reads against this Pacbio-assembled contig. Extract any reads that didn t map and assemble them together: this could be a plasmid, or part of a plasmid. Look for overhang: if found, trim. Align Illumina reads to the PacBio contig Index the contigs file: bwa index contig1.fasta Align Illumina reads using using bwa mem: bwa mem -t 8 contig1.fasta illumina_R1.fastq.gz illumina_R2.fastq.gz | samtools sort aln.bam bwa mem is the alignment tool -t 8 is the number of cores: choose an appropriate number contig1.fasta is the input assembly file illumina_R1.fastq.gz illumina_R2.fastq.gz are the Illumina reads | samtools sort pipes the output to samtools to sort aln.bam sends the alignment to the file aln.bam Extract unmapped Illumina reads Index the alignment file: samtools index aln.bam Extract the fastq files from the bam alignment - those reads that were unmapped to the Pacbio alignment - and save them in various unmapped files: samtools fastq -f 4 -1 unmapped.R1.fastq -2 unmapped.R2.fastq -s unmapped.RS.fastq aln.bam fastq is a command that coverts a .bam file into fastq format -f 4 : only output unmapped reads -1 : put R1 reads into a file called unmapped.R1.fastq -2 : put R2 reads into a file called unmapped.R2.fastq -s : put singleton reads into a file called unmapped.RS.fastq aln.bam : input alignment file We now have three files of the unampped reads: unmapped.R1.fastq , unmapped.R2.fastq , unmapped.RS.fastq . Assemble the unmapped reads Assemble with Spades: spades.py -1 unmapped.R1.fastq -2 unmapped.R2.fastq -s unmapped.RS.fastq --careful --cov-cutoff auto -o spades_assembly -1 is input file forward -2 is input file reverse -s is unpaired --careful minimizes mismatches and short indels --cov-cutoff auto computes the coverage threshold (rather than the default setting, off ) -o is the output directory Move into the output directory ( spades_assembly ) and look at the contigs: infoseq contigs.fasta 78 contigs were assembled, with the max length of 2250 (the first contig). All other nodes are 650kb so we will disregard as they are unlikely to be plasmids. Type q to exit. We will extract the first sequence (NODE_1): samtools faidx contigs.fasta samtools faidx contigs.fasta NODE_1_length_2550_cov_496.613 contig2.fasta This is now saved as contig2.fasta Open in nano and change header to plasmid . Trim the plasmid To trim any overhang on this plasmid, we will blast the start of contig2 against itself. Take the start of the contig: head -n 10 contig2.fasta contig2.fa.head We want to see if it matches the end (overhang). Format the assembly file for blast: makeblastdb -in contig2.fasta -dbtype nucl Blast the start of the assembly (.head file) against all of the assembly: blastn -query contig2.fa.head -db contig2.fasta -evalue 1e-3 -dust no -out contig2.bls Look at contig2.bls to see hits: less contig2.bls The first hit is at start, as expected. The second hit is at 2474 all the way to the end - 2550. This is the overhang. Trim to position 2473. Index the plasmid.fa file: samtools faidx contig2.fasta Trim: samtools faidx contig2.fasta plasmid:1-2473 plasmid.fa.trimmed plasmid is the name of the contig, and we want the sequence from 1-2473. Open this file in nano ( nano plasmid.fa.trimmed ) and change the header to plasmid , save. We now have a trimmed plasmid. Move file back into main folder: cp plasmid.fa.trimmed ../ Move into the main folder. Plasmid contig orientation The bacterial chromosome was oriented at the gene dnaA. Plasmids are often oriented at the replication gene, but this is highly variable and there is no established convention. Here we will orient the plasmid at a gene found by Prodigal, in Circlator: circlator fixstart plasmid.fa.trimmed plasmid_fixstart fixstart is an option in Circlator just to orient a sequence. plasmid.fa.trimmed is our small plasmid. plasmid_fixstart is the prefix for the output files. View the output: less plasmid_fixstart.log The plasmid has been oriented at a gene predicted by Prodigal, and the break-point is at position 1200. Change the file name: cp plasmid_fixstart.fasta contig2.fasta Collect contigs cat contig1.fasta contig2.fasta genome.fasta See the contigs and sizes: infoseq genome.fasta chromosome: 2823331 plasmid: 2473 Questions Question Why is this section so complicated? Answer Finding small plasmids is difficult for many reasons! This paper has a nice summary: On the (im)possibility to reconstruct plasmids from whole genome short-read sequencing data. doi: https://doi.org/10.1101/086744 Question Why can PacBio sequencing miss small plasmids? Answer Library prep size selection Question We extract unmapped Illumina reads and assemble these to find small plasmids. What could they be missing? Answer Repeats that have mapped to the PacBio assembly. Question How do you find a plasmid in a Bandage graph? Answer It is probably circular, matches the size of a known plasmid, and has a rep gene. Question Are there easier ways to find plasmids? Answer Possibly. One option is the program called Unicycler which may automate many of these steps. https://github.com/rrwick/Unicycler Correct We will correct the Pacbio assembly with Illumina reads. Make an alignment file Align the Illumina reads (R1 and R2) to the draft PacBio assembly, e.g. genome.fasta : bwa index genome.fasta bwa mem -t 32 genome.fasta illumina_R1.fastq.gz illumina_R2.fastq.gz | samtools sort aln.bam -t is the number of cores: set this to an appropriate number. (To find out how many you have, grep -c processor /proc/cpuinfo ). Index the files: samtools index aln.bam samtools faidx genome.fasta Now we have an alignment file to use in Pilon: aln.bam Run Pilon Run: pilon --genome genome.fasta --frags aln.bam --output pilon1 --fix all --mindepth 0.5 --changes --verbose --threads 32 --genome is the name of the input assembly to be corrected --frags is the alignment of the reads against the assembly --output is the name of the output prefix --fix is an option for types of corrections --mindepth gives a minimum read depth to use --changes produces an output file of the changes made --verbose prints information to the screen during the run --threads : set this to an appropriate number Look at the changes file: less pilon1.changes Example: Look at the details of the fasta file: infoseq pilon1.fasta chromosome - 2823340 (net +9 bases) plasmid - 2473 (no change) Option: If there are many changes, run Pilon again, using the pilon1.fasta file as the input assembly, and the Illumina reads to correct. Genome output Change the file name: cp pilon1.fasta assembly.fasta We now have the corrected genome assembly of Staphylococcus aureus in .fasta format, containing a chromosome and a small plasmid. Questions Q: Question Why don t we correct earlier in the assembly process? Answer We need to circularise the contigs and trim overhangs first. Question Why can we use some reads (Illumina) to correct other reads (PacBio) ? Answer Illumina reads have higher accuracy. Question Could we just use PacBio reads to assemble the genome? Answer Yes, if accuracy adequate. Advanced analysis This example shows a more complex analysis where many more steps are involved in the finding the small plasmid. The sample used is Staphylococcus aureus (sample number 25745). Assemble canu -p canu -d canu_outdir genomeSize=2.8m -pacbio-raw pacbio.fastq.gz Output: 2 contigs, likely to be the chromosome (2748030) and a large plasmid (49397). Trim and circularise circlator all --threads 16 --verbose canu_outdir/canu.contigs.fasta canu_outdir/canu.correctedReads.fasta.gz circlator_outdir Look at the information about circularisation, orientation, and trimmed sizes. Re-name the file contigs_1_2.fasta and move it into the main folder. Find smaller plasmids Align Illumina reads to the PacBio assembly: bwa index contigs_1_2.fasta bwa mem -t 8 contigs_1_2.fasta illumina_R1.fastq.gz illumina_R2.fastq.gz | samtools sort aln.bam samtools index aln.bam samtools fastq -f 4 -1 unmapped.R1.fastq -2 unmapped.R2.fastq -s unmapped.RS.fastq aln.bam spades.py -1 unmapped.R1.fastq -2 unmapped.R2.fastq -s unmapped.RS.fastq --careful --cov-cutoff auto -o spades_assembly Look at the output: cd spades_assembly infoseq contigs.fasta Extract the first node: samtools faidx contigs.fasta samtools faidx contigs.fasta NODE_1_length_2229_cov_610.298 contig3.fasta Open in Nano and change header to plasmid . Look for overhang by blasting start of plamsid against itself: head -n 10 contig3.fasta contig3.fa.head makeblastdb -in contig3.fasta -dbtype nucl blastn -query contig3.fa.head -db contig3.fasta -evalue 1e-3 -dust no -out contig3.bls less contig3.bls There is only one hit, to the start of the plasmid. No overhang is found. Search Genbank for any matching proteins: Copy the sequence Go to NCBI: https://blast.ncbi.nlm.nih.gov/Blast.cgi ; choose blastx Paste the sequence from contig3.fasta Choose genetic code = 11 Blast This hits a replication (plasmid) protein. Hypothesise that this is a small plasmid; search for the entire sequence within the assembly of all the Illumina reads (next step). Copy contig3.fasta into the main folder. Assemble all the Illumina reads and produce an assembly graph. spades.py -1 illumina_R1.fastq.gz -2 illumina_R2.fastq.gz --careful --cov-cutoff auto -o spades_assembly_all_illumina Navigate to the output and find the assembly_graph.fastg . Transfer this file to your local computer (e.g. using the file transfer program Cyberduck ). Examine the assembly in the program Bandage . File: Load graph: assembly_graph.fastg In the left hand panel, click Draw graph Your assembly graph may look like this: Blast the small plasmid sequence in this assembly In the left hand panel: Blast: create/view BLAST search Build blast database Paste in the sequence of contig3.fasta Run Blast search There are two hits around a node (in this case, node 249). Go to the main Bandage window In the right hand panel, enter the node number. Click Find nodes This node is a circular contig in the graph, and is slightly longer (2329) than our contig3 (2229): this could be the plasmid. Extract this node in fasta format: In the top panel, go to Output: Save selected node sequences; save as contig3b.fasta Move this file back to the analysis folder. Open this file in nano and change the header to contig3b , save. Take the start of the sequence and see if it matches the end: head -n 10 contig3b.fasta contig3b.fa.head makeblastdb -in contig3b.fasta -dbtype nucl blastn -query contig3b.fa.head -db contig3b.fasta -evalue 1e-3 -dust no -out contig3b.bls less contig3b.bls The first hit is against the start of the chromosome, as expected. The last hit starts at position 2253; we will trim the plasmid to position 2252 Index and trim the contig3b.fa file: samtools faidx contig3b.fasta samtools faidx contig3b.fasta contig3b:1-2252 contig3b.fa.trimmed Open this file in nano and change the header to contig3b , save. We now have a trimmed contig3b. Join all contigs: cat contigs_1_2.fasta contig3b.fa.trimmed genome.fasta Correct bwa index genome.fasta bwa mem -t 32 genome.fasta illumina_R1.fastq.gz illumina_R2.fastq.gz | samtools sort aln.bam samtools index aln.bam samtools faidx genome.fasta pilon --genome genome.fasta --frags aln.bam --output pilon1 --fix all --mindepth 0.5 --changes --verbose --threads 32 Look at the pilon1.changes file. Change the file name. cp pilon1.fasta assembly.fasta Look at the final assembly: infoseq assembly.fasta Assembly details: Chromosome: 2725222 Large plasmid: 25012 Small plasmid: 2252 Next Further analyses: Annotate with Prokka. Comparative genomics, e.g. with Roary. Links: Details of bas.h5 files Canu manual and gitub repository Circlator article and github repository Pilon article and github repository Notes on finishing and evaluating assemblies.","title":"Home"},{"location":"modules/cmdline_assembly/#pacbio-reads-assembly-with-command-line-tools","text":"Keywords: de novo assembly, PacBio, PacificBiosciences, Illumina, command line, Canu, Circlator, BWA, Spades, Pilon, Microbial Genomics Virtual Laboratory This tutorial demonstrates how to use long Pacbio sequence reads to assemble a bacterial genome, including correcting the assembly with short Illumina reads. Note: this tutorial uses canu v1.5.","title":"Pacbio reads: assembly with command line tools"},{"location":"modules/cmdline_assembly/#resources","text":"Tools (and versions) used in this tutorial include: canu 1.5 infoseq and sizeseq (part of EMBOSS) 6.6.0.0 circlator 1.5.1 [recently updated] bwa 0.7.15 samtools 1.3.1 spades 3.10.1 makeblastdb and blastn (part of blast) 2.4.0+ pilon 1.20","title":"Resources"},{"location":"modules/cmdline_assembly/#learning-objectives","text":"At the end of this tutorial, be able to: Assemble and circularise a bacterial genome from PacBio sequence data. Recover small plasmids missed by long read sequencing, using Illumina data Explore the effect of polishing assembled sequences with a different data set.","title":"Learning objectives"},{"location":"modules/cmdline_assembly/#overview","text":"Simplified version of workflow:","title":"Overview"},{"location":"modules/cmdline_assembly/#get-data","text":"The files we need are: pacbio.fastq.gz : the PacBio reads illumina_R1.fastq.gz : the Illumina forward reads illumina_R2.fastq.gz : the Illumina reverse reads If you already have the files, skip forward to next section, Assemble . Otherwise, this section has information about how to find and move the files:","title":"Get data"},{"location":"modules/cmdline_assembly/#pacbio-files","text":"Open the command line. Navigate to or create the directory in which you want to work. If the files are already on your server, you can symlink by using ln -s real_file_path [e.g. data/sample_name/pacbio1.fastq.gz] chosen_symlink_name [e.g. pacbio1.fastq.gz] Alternatively, obtain the input files from elsewhere, e.g. from the BPA portal. (You will need a password.) Pacbio files are often stored in the format: Sample_name/Cell_name/Analysis_Results/long_file_name_1.fastq.gz We will use the longfilename.subreads.fastq.gz files. The reads are usually split into three separate files because they are so large. Right click on the first subreads.fastq.gz file and copy link address . In the command line, type: wget --user username --password password [paste link URL for file] Repeat for the other two subreads.fastq.gz files. Join the files: cat pacbio*.fastq.gz pacbio.fastq.gz If the files are not gzipped, type: cat pacbio*.fastq | gzip pacbio.fastq.gz","title":"PacBio files"},{"location":"modules/cmdline_assembly/#illumina-files","text":"We will also use 2 x Illumina (Miseq) fastq.gz files. These are the R1.fastq.gz and R2.fastq.gz files. Symlink or wget these files as described above for PacBio files. Shorten the name of each of these files: mv longfilename_R1.fastq.gz illumina_R1.fastq.gz mv longfilename_R2.fastq.gz illumina_R2.fastq.gz","title":"Illumina files"},{"location":"modules/cmdline_assembly/#sample-information","text":"The sample used in this tutorial is a gram-positive bacteria called Staphylococcus aureus (sample number 25747). This particular sample is from a strain that is resistant to the antibiotic methicillin (a type of penicillin). It is also called MRSA: methicillin-resistant Staphylococcus aureus . It was isolated from (human) blood and caused bacteraemia, an infection of the bloodstream.","title":"Sample information"},{"location":"modules/cmdline_assembly/#assemble","text":"We will use the assembly software called Canu . Run Canu with these commands: canu -p canu -d canu_outdir genomeSize=2.8m -pacbio-raw pacbio.fastq.gz the first canu tells the program to run -p canu names prefix for output files ( canu ) -d canu_outdir names output directory ( canu_outdir ) genomeSize only has to be approximate. e.g. Staphylococcus aureus , 2.8m e.g. Streptococcus pyogenes , 1.8m Canu will correct, trim and assemble the reads. Various output will be displayed on the screen.","title":"Assemble"},{"location":"modules/cmdline_assembly/#check-the-output","text":"Move into canu_outdir and ls to see the output files. The canu.contigs.fasta are the assembled sequences. The canu.unassembled.fasta are the reads that could not be assembled. The canu.correctedReads.fasta.gz are the corrected Pacbio reads that were used in the assembly. The canu.file.gfa is the graph of the assembly. Display summary information about the contigs: ( infoseq is a tool from EMBOSS ) infoseq canu.contigs.fasta This will show the contigs found by Canu. e.g., - tig00000001 2851805 This looks like a chromosome of approximately 2.8 million bases. This matches what we would expect for this sample. For other data, Canu may not be able to join all the reads into one contig, so there may be several contigs in the output. Also, the sample may contain some plasmids and these may be found full or partially by Canu as additional contigs.","title":"Check the output"},{"location":"modules/cmdline_assembly/#change-canu-parameters-if-required","text":"If the assembly is poor with many contigs, re-run Canu with extra sensitivity parameters; e.g. canu -p prefix -d outdir corMhapSensitivity=high corMinCoverage=0 genomeSize=2.8m -pacbio-raw pacbio.fastq.gz","title":"Change Canu parameters if required"},{"location":"modules/cmdline_assembly/#questions","text":"Question How do long- and short-read assembly methods differ? Answer short reads: De Bruijn graphs; long reads: a move back towards simpler overlap-layout-consensus methods. Question Where can we find out the what the approximate genome size should be for the species being assembled? Answer NCBI Genomes - enter species name - click on Genome Assembly and Annotation report - sort table by clicking on the column header Size (Mb) - look at range of sizes in this column. Question In the assembly output, what are the unassembled reads? Why are they there? Answer Question What are the corrected reads? How did canu correct the reads? Answer Question Where could you view the output .gfa and what would it show? Answer","title":"Questions"},{"location":"modules/cmdline_assembly/#trim-and-circularise","text":"","title":"Trim and circularise"},{"location":"modules/cmdline_assembly/#run-circlator","text":"Circlator identifies and trims overhangs (on chromosomes and plasmids) and orients the start position at an appropriate gene (e.g. dnaA). It takes in the assembled contigs from Canu, as well as the corrected reads prepared by Canu. Overhangs are shown in blue: Adapted from Figure 1. Hunt et al. Genome Biology 2015 Move back into your main analysis folder. Run Circlator: circlator all --threads 8 --verbose canu_outdir/canu.contigs.fasta canu_outdir/canu.correctedReads.fasta.gz circlator_outdir --threads is the number of cores: change this to an appropriate number --verbose prints progress information to the screen canu_outdir/canu.contigs.fasta is the file path to the input Canu assembly canu_outdir/canu.correctedReads.fasta.gz is the file path to the corrected Pacbio reads - note, fastA not fastQ circlator_outdir is the name of the output directory. Some output will print to screen. When finished, it should say Circularized x of x contig(s) .","title":"Run Circlator"},{"location":"modules/cmdline_assembly/#check-the-output_1","text":"Move into the circlator_outdir directory and ls to list files. Were the contigs circularised? : less 04.merge.circularise.log Yes, the contig was circularised (last column). Type q to exit. Where were the contigs oriented (which gene)? : less 06.fixstart.log Look in the gene_name column. The contig has been oriented at tr|A0A090N2A8|A0A090N2A8_STAAU, which is another name for dnaA. This is typically used as the start of bacterial chromosome sequences. What are the trimmed contig sizes? : infoseq 06.fixstart.fasta tig00000001 2823331 (28564 bases trimmed) This trimmed part is the overlap. Re-name the contigs file : The trimmed contigs are in the file called 06.fixstart.fasta . Re-name it contig1.fasta : mv 06.fixstart.fasta contig1.fasta Open this file in a text editor (e.g. nano: nano contig1.fasta ) and change the header to chromosome . Move the file back into the main folder ( mv contig1.fasta ../ ).","title":"Check the output"},{"location":"modules/cmdline_assembly/#options","text":"If all the contigs have not circularised with Circlator, an option is to change the --b2r_length_cutoff setting to approximately 2X the average read depth.","title":"Options"},{"location":"modules/cmdline_assembly/#questions_1","text":"Question Were all the contigs circularised? Why/why not? Answer Question Circlator can set the start of the sequence at a particular gene. Which gene does it use? Is this appropriate for all contigs? Answer Uses dnaA for the chromosomal contig. For other contigs, uses a centrally-located gene. However, ideally, plasmids would be oriented on a gene such as repA. It is possible to provide a file to Circlator to do this.","title":"Questions"},{"location":"modules/cmdline_assembly/#find-smaller-plasmids","text":"Pacbio reads are long, and may have been longer than small plasmids. We will look for any small plasmids using the Illumina reads. This section involves several steps: Use the Canu+Circlator output of a trimmed assembly contig. Map all the Illumina reads against this Pacbio-assembled contig. Extract any reads that didn t map and assemble them together: this could be a plasmid, or part of a plasmid. Look for overhang: if found, trim.","title":"Find smaller plasmids"},{"location":"modules/cmdline_assembly/#align-illumina-reads-to-the-pacbio-contig","text":"Index the contigs file: bwa index contig1.fasta Align Illumina reads using using bwa mem: bwa mem -t 8 contig1.fasta illumina_R1.fastq.gz illumina_R2.fastq.gz | samtools sort aln.bam bwa mem is the alignment tool -t 8 is the number of cores: choose an appropriate number contig1.fasta is the input assembly file illumina_R1.fastq.gz illumina_R2.fastq.gz are the Illumina reads | samtools sort pipes the output to samtools to sort aln.bam sends the alignment to the file aln.bam","title":"Align Illumina reads to the PacBio contig"},{"location":"modules/cmdline_assembly/#extract-unmapped-illumina-reads","text":"Index the alignment file: samtools index aln.bam Extract the fastq files from the bam alignment - those reads that were unmapped to the Pacbio alignment - and save them in various unmapped files: samtools fastq -f 4 -1 unmapped.R1.fastq -2 unmapped.R2.fastq -s unmapped.RS.fastq aln.bam fastq is a command that coverts a .bam file into fastq format -f 4 : only output unmapped reads -1 : put R1 reads into a file called unmapped.R1.fastq -2 : put R2 reads into a file called unmapped.R2.fastq -s : put singleton reads into a file called unmapped.RS.fastq aln.bam : input alignment file We now have three files of the unampped reads: unmapped.R1.fastq , unmapped.R2.fastq , unmapped.RS.fastq .","title":"Extract unmapped Illumina reads"},{"location":"modules/cmdline_assembly/#assemble-the-unmapped-reads","text":"Assemble with Spades: spades.py -1 unmapped.R1.fastq -2 unmapped.R2.fastq -s unmapped.RS.fastq --careful --cov-cutoff auto -o spades_assembly -1 is input file forward -2 is input file reverse -s is unpaired --careful minimizes mismatches and short indels --cov-cutoff auto computes the coverage threshold (rather than the default setting, off ) -o is the output directory Move into the output directory ( spades_assembly ) and look at the contigs: infoseq contigs.fasta 78 contigs were assembled, with the max length of 2250 (the first contig). All other nodes are 650kb so we will disregard as they are unlikely to be plasmids. Type q to exit. We will extract the first sequence (NODE_1): samtools faidx contigs.fasta samtools faidx contigs.fasta NODE_1_length_2550_cov_496.613 contig2.fasta This is now saved as contig2.fasta Open in nano and change header to plasmid .","title":"Assemble the unmapped reads"},{"location":"modules/cmdline_assembly/#trim-the-plasmid","text":"To trim any overhang on this plasmid, we will blast the start of contig2 against itself. Take the start of the contig: head -n 10 contig2.fasta contig2.fa.head We want to see if it matches the end (overhang). Format the assembly file for blast: makeblastdb -in contig2.fasta -dbtype nucl Blast the start of the assembly (.head file) against all of the assembly: blastn -query contig2.fa.head -db contig2.fasta -evalue 1e-3 -dust no -out contig2.bls Look at contig2.bls to see hits: less contig2.bls The first hit is at start, as expected. The second hit is at 2474 all the way to the end - 2550. This is the overhang. Trim to position 2473. Index the plasmid.fa file: samtools faidx contig2.fasta Trim: samtools faidx contig2.fasta plasmid:1-2473 plasmid.fa.trimmed plasmid is the name of the contig, and we want the sequence from 1-2473. Open this file in nano ( nano plasmid.fa.trimmed ) and change the header to plasmid , save. We now have a trimmed plasmid. Move file back into main folder: cp plasmid.fa.trimmed ../ Move into the main folder.","title":"Trim the plasmid"},{"location":"modules/cmdline_assembly/#plasmid-contig-orientation","text":"The bacterial chromosome was oriented at the gene dnaA. Plasmids are often oriented at the replication gene, but this is highly variable and there is no established convention. Here we will orient the plasmid at a gene found by Prodigal, in Circlator: circlator fixstart plasmid.fa.trimmed plasmid_fixstart fixstart is an option in Circlator just to orient a sequence. plasmid.fa.trimmed is our small plasmid. plasmid_fixstart is the prefix for the output files. View the output: less plasmid_fixstart.log The plasmid has been oriented at a gene predicted by Prodigal, and the break-point is at position 1200. Change the file name: cp plasmid_fixstart.fasta contig2.fasta","title":"Plasmid contig orientation"},{"location":"modules/cmdline_assembly/#collect-contigs","text":"cat contig1.fasta contig2.fasta genome.fasta See the contigs and sizes: infoseq genome.fasta chromosome: 2823331 plasmid: 2473","title":"Collect contigs"},{"location":"modules/cmdline_assembly/#questions_2","text":"Question Why is this section so complicated? Answer Finding small plasmids is difficult for many reasons! This paper has a nice summary: On the (im)possibility to reconstruct plasmids from whole genome short-read sequencing data. doi: https://doi.org/10.1101/086744 Question Why can PacBio sequencing miss small plasmids? Answer Library prep size selection Question We extract unmapped Illumina reads and assemble these to find small plasmids. What could they be missing? Answer Repeats that have mapped to the PacBio assembly. Question How do you find a plasmid in a Bandage graph? Answer It is probably circular, matches the size of a known plasmid, and has a rep gene. Question Are there easier ways to find plasmids? Answer Possibly. One option is the program called Unicycler which may automate many of these steps. https://github.com/rrwick/Unicycler","title":"Questions"},{"location":"modules/cmdline_assembly/#correct","text":"We will correct the Pacbio assembly with Illumina reads.","title":"Correct"},{"location":"modules/cmdline_assembly/#make-an-alignment-file","text":"Align the Illumina reads (R1 and R2) to the draft PacBio assembly, e.g. genome.fasta : bwa index genome.fasta bwa mem -t 32 genome.fasta illumina_R1.fastq.gz illumina_R2.fastq.gz | samtools sort aln.bam -t is the number of cores: set this to an appropriate number. (To find out how many you have, grep -c processor /proc/cpuinfo ). Index the files: samtools index aln.bam samtools faidx genome.fasta Now we have an alignment file to use in Pilon: aln.bam","title":"Make an alignment file"},{"location":"modules/cmdline_assembly/#run-pilon","text":"Run: pilon --genome genome.fasta --frags aln.bam --output pilon1 --fix all --mindepth 0.5 --changes --verbose --threads 32 --genome is the name of the input assembly to be corrected --frags is the alignment of the reads against the assembly --output is the name of the output prefix --fix is an option for types of corrections --mindepth gives a minimum read depth to use --changes produces an output file of the changes made --verbose prints information to the screen during the run --threads : set this to an appropriate number Look at the changes file: less pilon1.changes Example: Look at the details of the fasta file: infoseq pilon1.fasta chromosome - 2823340 (net +9 bases) plasmid - 2473 (no change) Option: If there are many changes, run Pilon again, using the pilon1.fasta file as the input assembly, and the Illumina reads to correct.","title":"Run Pilon"},{"location":"modules/cmdline_assembly/#genome-output","text":"Change the file name: cp pilon1.fasta assembly.fasta We now have the corrected genome assembly of Staphylococcus aureus in .fasta format, containing a chromosome and a small plasmid.","title":"Genome output"},{"location":"modules/cmdline_assembly/#questions_3","text":"Q: Question Why don t we correct earlier in the assembly process? Answer We need to circularise the contigs and trim overhangs first. Question Why can we use some reads (Illumina) to correct other reads (PacBio) ? Answer Illumina reads have higher accuracy. Question Could we just use PacBio reads to assemble the genome? Answer Yes, if accuracy adequate.","title":"Questions"},{"location":"modules/cmdline_assembly/#advanced-analysis","text":"This example shows a more complex analysis where many more steps are involved in the finding the small plasmid. The sample used is Staphylococcus aureus (sample number 25745).","title":"Advanced analysis"},{"location":"modules/cmdline_assembly/#assemble_1","text":"canu -p canu -d canu_outdir genomeSize=2.8m -pacbio-raw pacbio.fastq.gz Output: 2 contigs, likely to be the chromosome (2748030) and a large plasmid (49397).","title":"Assemble"},{"location":"modules/cmdline_assembly/#trim-and-circularise_1","text":"circlator all --threads 16 --verbose canu_outdir/canu.contigs.fasta canu_outdir/canu.correctedReads.fasta.gz circlator_outdir Look at the information about circularisation, orientation, and trimmed sizes. Re-name the file contigs_1_2.fasta and move it into the main folder.","title":"Trim and circularise"},{"location":"modules/cmdline_assembly/#find-smaller-plasmids_1","text":"Align Illumina reads to the PacBio assembly: bwa index contigs_1_2.fasta bwa mem -t 8 contigs_1_2.fasta illumina_R1.fastq.gz illumina_R2.fastq.gz | samtools sort aln.bam samtools index aln.bam samtools fastq -f 4 -1 unmapped.R1.fastq -2 unmapped.R2.fastq -s unmapped.RS.fastq aln.bam spades.py -1 unmapped.R1.fastq -2 unmapped.R2.fastq -s unmapped.RS.fastq --careful --cov-cutoff auto -o spades_assembly Look at the output: cd spades_assembly infoseq contigs.fasta Extract the first node: samtools faidx contigs.fasta samtools faidx contigs.fasta NODE_1_length_2229_cov_610.298 contig3.fasta Open in Nano and change header to plasmid . Look for overhang by blasting start of plamsid against itself: head -n 10 contig3.fasta contig3.fa.head makeblastdb -in contig3.fasta -dbtype nucl blastn -query contig3.fa.head -db contig3.fasta -evalue 1e-3 -dust no -out contig3.bls less contig3.bls There is only one hit, to the start of the plasmid. No overhang is found. Search Genbank for any matching proteins: Copy the sequence Go to NCBI: https://blast.ncbi.nlm.nih.gov/Blast.cgi ; choose blastx Paste the sequence from contig3.fasta Choose genetic code = 11 Blast This hits a replication (plasmid) protein. Hypothesise that this is a small plasmid; search for the entire sequence within the assembly of all the Illumina reads (next step). Copy contig3.fasta into the main folder. Assemble all the Illumina reads and produce an assembly graph. spades.py -1 illumina_R1.fastq.gz -2 illumina_R2.fastq.gz --careful --cov-cutoff auto -o spades_assembly_all_illumina Navigate to the output and find the assembly_graph.fastg . Transfer this file to your local computer (e.g. using the file transfer program Cyberduck ). Examine the assembly in the program Bandage . File: Load graph: assembly_graph.fastg In the left hand panel, click Draw graph Your assembly graph may look like this: Blast the small plasmid sequence in this assembly In the left hand panel: Blast: create/view BLAST search Build blast database Paste in the sequence of contig3.fasta Run Blast search There are two hits around a node (in this case, node 249). Go to the main Bandage window In the right hand panel, enter the node number. Click Find nodes This node is a circular contig in the graph, and is slightly longer (2329) than our contig3 (2229): this could be the plasmid. Extract this node in fasta format: In the top panel, go to Output: Save selected node sequences; save as contig3b.fasta Move this file back to the analysis folder. Open this file in nano and change the header to contig3b , save. Take the start of the sequence and see if it matches the end: head -n 10 contig3b.fasta contig3b.fa.head makeblastdb -in contig3b.fasta -dbtype nucl blastn -query contig3b.fa.head -db contig3b.fasta -evalue 1e-3 -dust no -out contig3b.bls less contig3b.bls The first hit is against the start of the chromosome, as expected. The last hit starts at position 2253; we will trim the plasmid to position 2252 Index and trim the contig3b.fa file: samtools faidx contig3b.fasta samtools faidx contig3b.fasta contig3b:1-2252 contig3b.fa.trimmed Open this file in nano and change the header to contig3b , save. We now have a trimmed contig3b. Join all contigs: cat contigs_1_2.fasta contig3b.fa.trimmed genome.fasta","title":"Find smaller plasmids"},{"location":"modules/cmdline_assembly/#correct_1","text":"bwa index genome.fasta bwa mem -t 32 genome.fasta illumina_R1.fastq.gz illumina_R2.fastq.gz | samtools sort aln.bam samtools index aln.bam samtools faidx genome.fasta pilon --genome genome.fasta --frags aln.bam --output pilon1 --fix all --mindepth 0.5 --changes --verbose --threads 32 Look at the pilon1.changes file. Change the file name. cp pilon1.fasta assembly.fasta Look at the final assembly: infoseq assembly.fasta Assembly details: Chromosome: 2725222 Large plasmid: 25012 Small plasmid: 2252","title":"Correct"},{"location":"modules/cmdline_assembly/#next","text":"Further analyses: Annotate with Prokka. Comparative genomics, e.g. with Roary. Links: Details of bas.h5 files Canu manual and gitub repository Circlator article and github repository Pilon article and github repository Notes on finishing and evaluating assemblies.","title":"Next"},{"location":"modules/cmdline_assembly_v2/","text":"Long read assembly workshop This is a tutorial for a workshop on long-read (PacBio) genome assembly. It demonstrates how to use long PacBio sequencing reads to assemble a bacterial genome, and includes additional steps for circularising, trimming, finding plasmids, and correcting the assembly with short-read Illumina data. Overview Simplified version of workflow: 1. Get started Your workshop trainers will provide you with the address of a virtual machine. Mac users Open the Terminal. Type in ssh researcher@[your virtual machine address] Type in the password provided. Windows users If you are using Windows 10, you might be able to use the Ubuntu Subsystem. Otherwise, install and open Putty. Download putty here . Open. A configuration window will appear. Under Host Name (or IP address) enter in the address of your virtual machine. Under Port type in 22 Under Connection Type select SSH Click Open Under Login as: enter researcher Type in the password provided. Create a new working directory on your remote computer. Because we are starting a new analysis it is always good practice to start in a new empty directory. Therefore, we will create a new directory and change to it for the rest of the workshop. In your terminal: Create a new directory called Workshop mkdir Workshop Change to that directory cd Workshop NOTE: Everytime you open a new terminal or Putty session, you will need to make sure you are in this directory again. The current directory can be obtained with the linux command: pwd 2. Get data The sample used in this tutorial is from a bacteria called Staphylococcus aureus . We have used a small section of its real genome so that the programs can run in the workshop time. The files we need are: pacbio.fq : the PacBio reads R1.fq : the Illumina forward reads R2.fq : the Illumina reverse reads In a new tab, go to https://doi.org/10.5281/zenodo.1009308 . Next to the first file, right-click (or control-click) the Download button, and select Copy link address . Back in your terminal, enter wget [paste file link here] The file should download. Note: paste the link to the file, not to the webpage. Repeat this for the other two files. 3. Assemble We will use the assembly software called Canu , version 1.6. Run Canu with these commands: canu -p canu -d canu_outdir genomeSize=0.03m -pacbio-raw pacbio.fq the first canu tells the program to run -p canu names prefix for output files ( canu ) -d canu_outdir names output directory ( canu_outdir ) genomeSize only has to be approximate. (In this case we are using a partial genome of expected size 30,000 base pairs). Canu will correct, trim and assemble the reads. Various output will be displayed on the screen. Note : Canu could say Finished but may still be running. In this case, type squeue to see if jobs are still running. If you run squeue you will see something like this: JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 6 main canu_can research PD 0:00 1 (Dependency) 5_1 main cormhap_ research R 0:29 1 master You will know if Canu has completely finished when squeue shows no jobs listed under the header row. 4. Check assembly output Move into the canu output folder: cd canu_outdir View the list of files: ls The canu.contigs.fasta are the assembled sequences. The canu.unassembled.fasta are the reads that could not be assembled. The canu.correctedReads.fasta.gz are the corrected Pacbio reads that were used in the assembly. The canu.contigs.gfa is the graph of the assembly. The canu.report file is a summary of all of the steps Canu performed with information about the reads used, how they were handled and a whole lot of summary information about the assembly. Display summary information about the contigs: ( infoseq is a tool from EMBOSS ) infoseq canu.contigs.fasta This will show the contigs found by Canu. e.g., tig00000001 39136 tig00000001 is the name given to the contig 39136 is the number of base pairs in that contig. This matches what we were expecting for this sample (approximately 30,000 base pairs). For other data, Canu may not be able to join all the reads into one contig, so there may be several contigs in the output. We should also look at the canu.report . To do this: less canu.report less is a command to display the file on the screen. Use the up and down arrows to scroll up and down. You will see lots of histograms of read lengths before and after processing, final contig construction, etc. For a description of the outputs that Canu produces, see: http://canu.readthedocs.io/en/latest/tutorial.html#outputs Type q to exit viewing the report. Questions Question How do long- and short-read assembly methods differ? Answer Short reads are usually assembled with De Bruijn graphs. For long reads, there is a move back towards simpler overlap-layout-consensus methods. Question Where can we find out the what the approximate genome size should be for the species being assembled? Answer Go to NCBI Genomes, enter species name, click on Genome Assembly and Annotation report, sort table by clicking on the column header Size (Mb), look at range of sizes in this column. Question In the assembly output, what are the unassembled reads? Why are they there? Answer Reads and low-coverage contigs that were not used in the assembly. Question What are the corrected reads? How did canu correct the reads? Answer Canu builds overlaps between reads. The consensus is used to correct the reads. Question Where could you view the output .gfa and what would it show? Answer A useful program is Bandage . If the assembly has multiple contigs, the assembly graph shows how these are connected. 5. Trim and circularise Bacteria have circular chromosomes. Because of sequencing errors, there may be some overhang in the assembled linear sequence. Our assembly may have some overhang because it is 9000 bases longer than expected. Adapted from Figure 1. Hunt et al. Genome Biology 2015 A tool called Circlator identifies and trims overhangs (on chromosomes and plasmids). It takes in the assembled contigs from Canu, as well as the corrected reads prepared by Canu. Move back into your main analysis folder: cd .. Run Circlator circlator all --threads 4 --verbose canu_outdir/canu.contigs.fasta canu_outdir/canu.correctedReads.fasta.gz circlator_outdir (Click on the dark grey slider bar above and move it to the right, to see all the way to the end of the line.) --threads is the number of cores --verbose prints progress information to the screen canu_outdir/canu.contigs.fasta is the file path to the input Canu assembly canu_outdir/canu.correctedReads.fasta.gz is the file path to the corrected Pacbio reads - note, fastA not fastQ circlator_outdir is the name of the output directory. Some output will print to screen. When finished, it should say Circularized x of x contig(s) . Check the output Move into the Circlator output directory: cd circlator_outdir List the files: ls Circlator has named the output files with numbers as prefixes. Were the contigs circularised? less 04.merge.circularise.log less is a command to display the file on the screen. 04.merge.circularise.log is the name of the file. Yes, the contig was circularised (last column). Type q to exit. What are the trimmed contig sizes? infoseq 06.fixstart.fasta The contig tig00000001 has a length of 30019. This is about 9000 bases shorter than before circularisation. This was the overhang and has now been trimmed. Copy the circularised contigs file to the main analysis directory with a new name: cp 06.fixstart.fasta ../contig1.fasta Move back into the main folder: cd .. Questions Question Were all the contigs circularised? Answer In this example, yes, the contig was circularised. Question Circlator can set the start of the sequence at a particular gene. Which gene does it use? Is this appropriate for all contigs? Answer Circlator uses dnaA (if present) for the chromosomal contig. For other contigs, it uses a centrally-located gene. However, ideally, plasmids would be oriented on a gene such as repA. It is possible to provide a file to Circlator to do this. 6. Find smaller plasmids Pacbio reads are long, and may have been longer than small plasmids. We will look for any small plasmids using the Illumina reads. This section involves several steps: Use the Canu+Circlator output of a trimmed assembly contig. Map all the Illumina reads against this Pacbio-assembled contig. Extract any reads that didn t map and assemble them together: this could be a plasmid, or part of a plasmid. Look for overhang: if found, trim. Align Illumina reads to the PacBio contig Index the contigs file: bwa index contig1.fasta Align Illumina reads using using bwa mem: bwa mem -t 4 contig1.fasta R1.fq R2.fq | samtools sort aln.bam bwa mem is the alignment tool -t 4 is the number of cores contig1.fasta is the input assembly file R1.fq R2.fq are the Illumina reads | samtools sort pipes the output to samtools to sort aln.bam sends the alignment to the file aln.bam Extract unmapped Illumina reads Index the alignment file: samtools index aln.bam Extract the fastq files from the bam alignment - those reads that were unmapped to the Pacbio alignment - and save them in various unmapped files: samtools fastq -f 4 -1 unmapped.R1.fastq -2 unmapped.R2.fastq -s unmapped.RS.fastq aln.bam fastq is a command that coverts a .bam file into fastq format -f 4 : only output unmapped reads -1 : put R1 reads into a file called unmapped.R1.fastq -2 : put R2 reads into a file called unmapped.R2.fastq -s : put singleton reads into a file called unmapped.RS.fastq aln.bam : input alignment file We now have three files of the unampped reads: unmapped.R1.fastq , unmapped.R2.fastq , unmapped.RS.fastq . Assemble the unmapped reads Assemble with Spades: spades.py -1 unmapped.R1.fastq -2 unmapped.R2.fastq -s unmapped.RS.fastq --careful --cov-cutoff auto -o spades_assembly (Click on the dark grey slider bar above and move it to the right, to see all the way to the end of the line.) -1 is input file forward -2 is input file reverse -s is unpaired --careful minimizes mismatches and short indels --cov-cutoff auto computes the coverage threshold (rather than the default setting, off ) -o is the output directory Move into the output directory: cd spades_assembly Look at the contigs: infoseq contigs.fasta 1 contig has been assembled with a length of 2359 bases. Copy it to a new file: cp contigs.fasta contig2.fasta Trim the plasmid To trim any overhang on this plasmid, we will blast the start of contig2 against itself. Take the start of the contig: head -n 10 contig2.fasta contig2.fa.head head -n 10 takes the first ten lines of contig2.fasta sends that output to a new file called contig2.fa.head We want to see if the start of the contig matches the end (overhang). Format the assembly file for blast: makeblastdb -in contig2.fasta -dbtype nucl makeblastdb makes a database for the tool Blast This will generate three new files in the directory with suffixes .nhr, .nin and .nsq -in sets the input file as contig2.fasta -dbtype nucl sets the type to nucleotide (rather than protein) Blast the start of the assembly (.head file) against all of the assembly: blastn -query contig2.fa.head -db contig2.fasta -evalue 1e-3 -dust no -out contig2.bls blastn is the tool Blast, set as blast n to compare sequences of nucleotides to each other -query sets the input sequence as contig2.fa.head -db sets the database as that of the original sequence contig2.fasta . We don t have to specify the other files that were created when we formatted this file, but they need to present in our current directory. -evalue is the number of hits expected by chance, here set as 1e-3 -dust no turns off the masking of low-complexity regions -out sets the output file as contig2.bls Look at the hits (the matches): less contig2.bls The first hit is at the start, as expected. We can see that Query 1 (the start of the contig) is aligned to Sbject 1 (the whole contig), for the first 540 bases. Scroll down with the down arrow. The second hit shows Query 1 (the start of the contig) also matches to Sbject 1 (the whole contig) at position 2253, all the way to the end, position 2359. This is the overhang. Therefore, in the next step, we need to trim the contig to position 2252. Type q to exit. First, change the name of the contig within the file: nano contig2.fasta nano opens up a text editor. Use the arrow keys to navigate. (The mouse won t work.) At the first line, delete the text, which will be something like NODE_1_length_2359_cov_3.320333 Type in contig2 Don t forget the symbol Press Control-X Save modified buffer ? - type Y Press the Enter key Index the file (this will allow samtools to edit the file as it will have an index): samtools faidx contig2.fasta faidx means index the fasta file Trim the contig: samtools faidx contig2.fasta contig2:1-2252 plasmid.fasta this extracts contig2 from position 1-2252 plasmid.fasta sends the extracted section to a new file We now have a trimmed plasmid. Copy the plasmid file into the main folder: cp plasmid.fasta ../ Move file back into main folder: cd .. Collect contigs Collect the chromosome and the plasmid in one fasta file (they will be 2 records in the file): cat contig1.fasta plasmid.fasta genome.fasta See the contigs and sizes: infoseq genome.fasta chromosome: 30019 plasmid: 2252 Questions Question Why is this section so complicated? Answer Finding small plasmids is difficult for many reasons! This paper has a nice summary: On the (im)possibility to reconstruct plasmids from whole genome short-read sequencing data. doi: https://doi.org/10.1101/086744 Question Why can PacBio sequencing miss small plasmids? Answer Library prep size selection. Question We extract unmapped Illumina reads and assemble these to find small plasmids. What could they be missing? Answer Repeats that have mapped to the PacBio assembly. Question How do you find a plasmid in a Bandage graph? Answer It is probably circular, matches the size of a known plasmid, and has a rep gene. Question Are there easier ways to find plasmids? Answer Possibly. One option is the program called Unicycler which may automate many of these steps. https://github.com/rrwick/Unicycler 7. Correct the assembly Sequences from PacBio can have more errors than those from Illumina. Therefore, although it is useful to use the long PacBio reads to assemble the genome, we can also use the shorter and more accurate Illumina reads to correct errors in the PacBio assembly. Make an alignment file Index the fasta file: bwa index genome.fasta Align the Illumina reads: bwa mem -t 4 genome.fasta R1.fq R2.fq | samtools sort pilon_aln.bam Aligns Illumina R1.fq and R2.fq to the PacBio assembly genome.fasta . This produces a .bam file | pipes the output to samtools to sort (required for downstream processing) pilon_aln.bam redirects the sorted bam to this file Index the files: samtools index pilon_aln.bam samtools faidx genome.fasta Now we have an alignment file to use with the tool Pilon : pilon_aln.bam Run Pilon Run: pilon --genome genome.fasta --frags pilon_aln.bam --output pilon1 --fix all --mindepth 0.5 --changes --verbose --threads 4 --genome is the name of the input assembly to be corrected --frags is the alignment of the reads against the assembly --output is the name of the output prefix --fix is an option for types of corrections --mindepth gives a minimum read depth to use --changes produces an output file of the changes made --verbose prints information to the screen during the run --threads : number of cores Look at the changes file: less pilon1.changes Example: We can see lots of cases where a deletion (represented by a dot) has been corrected to a base. Type q to exit. Look at the details of the fasta file: infoseq pilon1.fasta chromosome - 30060 (net +41 bases) plasmid - 2252 (no change) Change the file name: cp pilon1.fasta assembly.fasta We now have the corrected genome assembly of Staphylococcus aureus in .fasta format, containing a chromosome and a small plasmid. Questions Question Why don t we correct earlier in the assembly process? Answer We need to circularise the contigs and trim overhangs first. Question Why can we use some reads (Illumina) to correct other reads (PacBio) ? Answer Illumina reads have higher accuracy. Question Could we just use PacBio reads to assemble the genome? Answer Yes, if accuracy adequate. 8. Comparative Genomics In the workshop so far, we used a partial bacterial genome so that the exercises could run in the time available. As a demonstration, to better see the effect of long and short reads on the assembly, we will examine complete bacterial genome. Assemblies This bacterial genome has been assembled from either long PacBio reads (using Canu) or shorter Illumina reads (using Spades). Assembly graphs: Look at the assembly graph (usually has a suffix .gfa), in the program Bandage . This shows how contigs are related, albeit with ambiguity in some places. The assembly graph from Illumina reads (Spades assembly): The assembly graph from PacBio reads (Canu assembly) - this is missing the small plasmid: Here we can see that the long read data results in a more contiguous assembly - one complete chromosome versus many smaller contigs with ambiguous placement. Question Does it matter that an assembly is in many contigs? Answer Yes and No. Yes: broken genes can lead to missing/incorrect annotations; fragmented assemblies provide less information about the genomic structure ( e.g. the number of plasmids) and the location of genes of interest ( e.g. gene A is located on plasmid X). No: many or all genes may still be annotated correctly. Gene location is useful (e.g. chromosome, plasmid1) but not always essential (e.g. presence/absence of particular resistance genes may be enough information). Annotations Genomic features such as genes can be identified with annotation tools. We have used a tool called Prokka to annotate the two genomes described above. Some of the output data is displayed here: assembly: PacBio Illumina size 2,825,804 2,792,905 contigs 2 123 CDS 2614 2575 tRNA 61 65 rRNA 19 4 Question Why are there more CDS identified in the PacBio assembly? Answer The PacBio assembly may have errors (usually a one base indel) which will cause a frame shift, which can result in three things: a longer CDS, a shorter CDS, or a shorter CDS plus an additional CDS. In addition, the Illumina assembly is about 33 kb smaller than the PacBio assembly. In bacteria, a rule of thumb is that 1 kb is roughly equal to one gene. Thus, we would probably expect about 33 fewer identified genes, which fits with these results. Question Why are there more rRNA identified in the PacBio assembly? Answer There may be multiple copies of the rRNAs and these could have been collapsed as repeats in the Illumina assembly. 9. Summary In this workshop, we used bacterial sequencing data from long and short reads to produce a polished genome. Procedure and tools: Canu to assemble long-read PacBio data Circlator to trim and circularise contigs BWA-MEM to map shorter Illumina reads to the PacBio assembly Spades to assemble any unmapped, leftover Illumina reads (the plasmid) Pilon to correct the PacBio assembly with the more accurate Illumina reads We also looked at comparative genomics: Bandage to examine assembly graphs Prokka to annotate genomes with features such as genes Further research: Align genomes with Mauve: tutorial link Find core and pan genomes with Roary and Phandango: tutorial link Melbourne Bioinformatics tutorials: https://www.melbournebioinformatics.org.au/tutorials/ Additional microbial genomics tutorials: http://sepsis-omics.github.io/tutorials/","title":"Long read assembly workshop"},{"location":"modules/cmdline_assembly_v2/#long-read-assembly-workshop","text":"This is a tutorial for a workshop on long-read (PacBio) genome assembly. It demonstrates how to use long PacBio sequencing reads to assemble a bacterial genome, and includes additional steps for circularising, trimming, finding plasmids, and correcting the assembly with short-read Illumina data.","title":"Long read assembly workshop"},{"location":"modules/cmdline_assembly_v2/#overview","text":"Simplified version of workflow:","title":"Overview"},{"location":"modules/cmdline_assembly_v2/#1-get-started","text":"Your workshop trainers will provide you with the address of a virtual machine.","title":"1. Get started"},{"location":"modules/cmdline_assembly_v2/#mac-users","text":"Open the Terminal. Type in ssh researcher@[your virtual machine address] Type in the password provided.","title":"Mac users"},{"location":"modules/cmdline_assembly_v2/#windows-users","text":"If you are using Windows 10, you might be able to use the Ubuntu Subsystem. Otherwise, install and open Putty. Download putty here . Open. A configuration window will appear. Under Host Name (or IP address) enter in the address of your virtual machine. Under Port type in 22 Under Connection Type select SSH Click Open Under Login as: enter researcher Type in the password provided.","title":"Windows users"},{"location":"modules/cmdline_assembly_v2/#create-a-new-working-directory-on-your-remote-computer","text":"Because we are starting a new analysis it is always good practice to start in a new empty directory. Therefore, we will create a new directory and change to it for the rest of the workshop. In your terminal: Create a new directory called Workshop mkdir Workshop Change to that directory cd Workshop NOTE: Everytime you open a new terminal or Putty session, you will need to make sure you are in this directory again. The current directory can be obtained with the linux command: pwd","title":"Create a new working directory on your remote computer."},{"location":"modules/cmdline_assembly_v2/#2-get-data","text":"The sample used in this tutorial is from a bacteria called Staphylococcus aureus . We have used a small section of its real genome so that the programs can run in the workshop time. The files we need are: pacbio.fq : the PacBio reads R1.fq : the Illumina forward reads R2.fq : the Illumina reverse reads In a new tab, go to https://doi.org/10.5281/zenodo.1009308 . Next to the first file, right-click (or control-click) the Download button, and select Copy link address . Back in your terminal, enter wget [paste file link here] The file should download. Note: paste the link to the file, not to the webpage. Repeat this for the other two files.","title":"2. Get data"},{"location":"modules/cmdline_assembly_v2/#3-assemble","text":"We will use the assembly software called Canu , version 1.6. Run Canu with these commands: canu -p canu -d canu_outdir genomeSize=0.03m -pacbio-raw pacbio.fq the first canu tells the program to run -p canu names prefix for output files ( canu ) -d canu_outdir names output directory ( canu_outdir ) genomeSize only has to be approximate. (In this case we are using a partial genome of expected size 30,000 base pairs). Canu will correct, trim and assemble the reads. Various output will be displayed on the screen. Note : Canu could say Finished but may still be running. In this case, type squeue to see if jobs are still running. If you run squeue you will see something like this: JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 6 main canu_can research PD 0:00 1 (Dependency) 5_1 main cormhap_ research R 0:29 1 master You will know if Canu has completely finished when squeue shows no jobs listed under the header row.","title":"3. Assemble"},{"location":"modules/cmdline_assembly_v2/#4-check-assembly-output","text":"Move into the canu output folder: cd canu_outdir View the list of files: ls The canu.contigs.fasta are the assembled sequences. The canu.unassembled.fasta are the reads that could not be assembled. The canu.correctedReads.fasta.gz are the corrected Pacbio reads that were used in the assembly. The canu.contigs.gfa is the graph of the assembly. The canu.report file is a summary of all of the steps Canu performed with information about the reads used, how they were handled and a whole lot of summary information about the assembly. Display summary information about the contigs: ( infoseq is a tool from EMBOSS ) infoseq canu.contigs.fasta This will show the contigs found by Canu. e.g., tig00000001 39136 tig00000001 is the name given to the contig 39136 is the number of base pairs in that contig. This matches what we were expecting for this sample (approximately 30,000 base pairs). For other data, Canu may not be able to join all the reads into one contig, so there may be several contigs in the output. We should also look at the canu.report . To do this: less canu.report less is a command to display the file on the screen. Use the up and down arrows to scroll up and down. You will see lots of histograms of read lengths before and after processing, final contig construction, etc. For a description of the outputs that Canu produces, see: http://canu.readthedocs.io/en/latest/tutorial.html#outputs Type q to exit viewing the report.","title":"4. Check assembly output"},{"location":"modules/cmdline_assembly_v2/#questions","text":"Question How do long- and short-read assembly methods differ? Answer Short reads are usually assembled with De Bruijn graphs. For long reads, there is a move back towards simpler overlap-layout-consensus methods. Question Where can we find out the what the approximate genome size should be for the species being assembled? Answer Go to NCBI Genomes, enter species name, click on Genome Assembly and Annotation report, sort table by clicking on the column header Size (Mb), look at range of sizes in this column. Question In the assembly output, what are the unassembled reads? Why are they there? Answer Reads and low-coverage contigs that were not used in the assembly. Question What are the corrected reads? How did canu correct the reads? Answer Canu builds overlaps between reads. The consensus is used to correct the reads. Question Where could you view the output .gfa and what would it show? Answer A useful program is Bandage . If the assembly has multiple contigs, the assembly graph shows how these are connected.","title":"Questions"},{"location":"modules/cmdline_assembly_v2/#5-trim-and-circularise","text":"Bacteria have circular chromosomes. Because of sequencing errors, there may be some overhang in the assembled linear sequence. Our assembly may have some overhang because it is 9000 bases longer than expected. Adapted from Figure 1. Hunt et al. Genome Biology 2015 A tool called Circlator identifies and trims overhangs (on chromosomes and plasmids). It takes in the assembled contigs from Canu, as well as the corrected reads prepared by Canu. Move back into your main analysis folder: cd ..","title":"5. Trim and circularise"},{"location":"modules/cmdline_assembly_v2/#run-circlator","text":"circlator all --threads 4 --verbose canu_outdir/canu.contigs.fasta canu_outdir/canu.correctedReads.fasta.gz circlator_outdir (Click on the dark grey slider bar above and move it to the right, to see all the way to the end of the line.) --threads is the number of cores --verbose prints progress information to the screen canu_outdir/canu.contigs.fasta is the file path to the input Canu assembly canu_outdir/canu.correctedReads.fasta.gz is the file path to the corrected Pacbio reads - note, fastA not fastQ circlator_outdir is the name of the output directory. Some output will print to screen. When finished, it should say Circularized x of x contig(s) .","title":"Run Circlator"},{"location":"modules/cmdline_assembly_v2/#check-the-output","text":"Move into the Circlator output directory: cd circlator_outdir List the files: ls Circlator has named the output files with numbers as prefixes. Were the contigs circularised? less 04.merge.circularise.log less is a command to display the file on the screen. 04.merge.circularise.log is the name of the file. Yes, the contig was circularised (last column). Type q to exit. What are the trimmed contig sizes? infoseq 06.fixstart.fasta The contig tig00000001 has a length of 30019. This is about 9000 bases shorter than before circularisation. This was the overhang and has now been trimmed. Copy the circularised contigs file to the main analysis directory with a new name: cp 06.fixstart.fasta ../contig1.fasta Move back into the main folder: cd ..","title":"Check the output"},{"location":"modules/cmdline_assembly_v2/#questions_1","text":"Question Were all the contigs circularised? Answer In this example, yes, the contig was circularised. Question Circlator can set the start of the sequence at a particular gene. Which gene does it use? Is this appropriate for all contigs? Answer Circlator uses dnaA (if present) for the chromosomal contig. For other contigs, it uses a centrally-located gene. However, ideally, plasmids would be oriented on a gene such as repA. It is possible to provide a file to Circlator to do this.","title":"Questions"},{"location":"modules/cmdline_assembly_v2/#6-find-smaller-plasmids","text":"Pacbio reads are long, and may have been longer than small plasmids. We will look for any small plasmids using the Illumina reads. This section involves several steps: Use the Canu+Circlator output of a trimmed assembly contig. Map all the Illumina reads against this Pacbio-assembled contig. Extract any reads that didn t map and assemble them together: this could be a plasmid, or part of a plasmid. Look for overhang: if found, trim.","title":"6. Find smaller plasmids"},{"location":"modules/cmdline_assembly_v2/#align-illumina-reads-to-the-pacbio-contig","text":"Index the contigs file: bwa index contig1.fasta Align Illumina reads using using bwa mem: bwa mem -t 4 contig1.fasta R1.fq R2.fq | samtools sort aln.bam bwa mem is the alignment tool -t 4 is the number of cores contig1.fasta is the input assembly file R1.fq R2.fq are the Illumina reads | samtools sort pipes the output to samtools to sort aln.bam sends the alignment to the file aln.bam","title":"Align Illumina reads to the PacBio contig"},{"location":"modules/cmdline_assembly_v2/#extract-unmapped-illumina-reads","text":"Index the alignment file: samtools index aln.bam Extract the fastq files from the bam alignment - those reads that were unmapped to the Pacbio alignment - and save them in various unmapped files: samtools fastq -f 4 -1 unmapped.R1.fastq -2 unmapped.R2.fastq -s unmapped.RS.fastq aln.bam fastq is a command that coverts a .bam file into fastq format -f 4 : only output unmapped reads -1 : put R1 reads into a file called unmapped.R1.fastq -2 : put R2 reads into a file called unmapped.R2.fastq -s : put singleton reads into a file called unmapped.RS.fastq aln.bam : input alignment file We now have three files of the unampped reads: unmapped.R1.fastq , unmapped.R2.fastq , unmapped.RS.fastq .","title":"Extract unmapped Illumina reads"},{"location":"modules/cmdline_assembly_v2/#assemble-the-unmapped-reads","text":"Assemble with Spades: spades.py -1 unmapped.R1.fastq -2 unmapped.R2.fastq -s unmapped.RS.fastq --careful --cov-cutoff auto -o spades_assembly (Click on the dark grey slider bar above and move it to the right, to see all the way to the end of the line.) -1 is input file forward -2 is input file reverse -s is unpaired --careful minimizes mismatches and short indels --cov-cutoff auto computes the coverage threshold (rather than the default setting, off ) -o is the output directory Move into the output directory: cd spades_assembly Look at the contigs: infoseq contigs.fasta 1 contig has been assembled with a length of 2359 bases. Copy it to a new file: cp contigs.fasta contig2.fasta","title":"Assemble the unmapped reads"},{"location":"modules/cmdline_assembly_v2/#trim-the-plasmid","text":"To trim any overhang on this plasmid, we will blast the start of contig2 against itself. Take the start of the contig: head -n 10 contig2.fasta contig2.fa.head head -n 10 takes the first ten lines of contig2.fasta sends that output to a new file called contig2.fa.head We want to see if the start of the contig matches the end (overhang). Format the assembly file for blast: makeblastdb -in contig2.fasta -dbtype nucl makeblastdb makes a database for the tool Blast This will generate three new files in the directory with suffixes .nhr, .nin and .nsq -in sets the input file as contig2.fasta -dbtype nucl sets the type to nucleotide (rather than protein) Blast the start of the assembly (.head file) against all of the assembly: blastn -query contig2.fa.head -db contig2.fasta -evalue 1e-3 -dust no -out contig2.bls blastn is the tool Blast, set as blast n to compare sequences of nucleotides to each other -query sets the input sequence as contig2.fa.head -db sets the database as that of the original sequence contig2.fasta . We don t have to specify the other files that were created when we formatted this file, but they need to present in our current directory. -evalue is the number of hits expected by chance, here set as 1e-3 -dust no turns off the masking of low-complexity regions -out sets the output file as contig2.bls Look at the hits (the matches): less contig2.bls The first hit is at the start, as expected. We can see that Query 1 (the start of the contig) is aligned to Sbject 1 (the whole contig), for the first 540 bases. Scroll down with the down arrow. The second hit shows Query 1 (the start of the contig) also matches to Sbject 1 (the whole contig) at position 2253, all the way to the end, position 2359. This is the overhang. Therefore, in the next step, we need to trim the contig to position 2252. Type q to exit. First, change the name of the contig within the file: nano contig2.fasta nano opens up a text editor. Use the arrow keys to navigate. (The mouse won t work.) At the first line, delete the text, which will be something like NODE_1_length_2359_cov_3.320333 Type in contig2 Don t forget the symbol Press Control-X Save modified buffer ? - type Y Press the Enter key Index the file (this will allow samtools to edit the file as it will have an index): samtools faidx contig2.fasta faidx means index the fasta file Trim the contig: samtools faidx contig2.fasta contig2:1-2252 plasmid.fasta this extracts contig2 from position 1-2252 plasmid.fasta sends the extracted section to a new file We now have a trimmed plasmid. Copy the plasmid file into the main folder: cp plasmid.fasta ../ Move file back into main folder: cd ..","title":"Trim the plasmid"},{"location":"modules/cmdline_assembly_v2/#collect-contigs","text":"Collect the chromosome and the plasmid in one fasta file (they will be 2 records in the file): cat contig1.fasta plasmid.fasta genome.fasta See the contigs and sizes: infoseq genome.fasta chromosome: 30019 plasmid: 2252","title":"Collect contigs"},{"location":"modules/cmdline_assembly_v2/#questions_2","text":"Question Why is this section so complicated? Answer Finding small plasmids is difficult for many reasons! This paper has a nice summary: On the (im)possibility to reconstruct plasmids from whole genome short-read sequencing data. doi: https://doi.org/10.1101/086744 Question Why can PacBio sequencing miss small plasmids? Answer Library prep size selection. Question We extract unmapped Illumina reads and assemble these to find small plasmids. What could they be missing? Answer Repeats that have mapped to the PacBio assembly. Question How do you find a plasmid in a Bandage graph? Answer It is probably circular, matches the size of a known plasmid, and has a rep gene. Question Are there easier ways to find plasmids? Answer Possibly. One option is the program called Unicycler which may automate many of these steps. https://github.com/rrwick/Unicycler","title":"Questions"},{"location":"modules/cmdline_assembly_v2/#7-correct-the-assembly","text":"Sequences from PacBio can have more errors than those from Illumina. Therefore, although it is useful to use the long PacBio reads to assemble the genome, we can also use the shorter and more accurate Illumina reads to correct errors in the PacBio assembly.","title":"7. Correct the assembly"},{"location":"modules/cmdline_assembly_v2/#make-an-alignment-file","text":"Index the fasta file: bwa index genome.fasta Align the Illumina reads: bwa mem -t 4 genome.fasta R1.fq R2.fq | samtools sort pilon_aln.bam Aligns Illumina R1.fq and R2.fq to the PacBio assembly genome.fasta . This produces a .bam file | pipes the output to samtools to sort (required for downstream processing) pilon_aln.bam redirects the sorted bam to this file Index the files: samtools index pilon_aln.bam samtools faidx genome.fasta Now we have an alignment file to use with the tool Pilon : pilon_aln.bam","title":"Make an alignment file"},{"location":"modules/cmdline_assembly_v2/#run-pilon","text":"Run: pilon --genome genome.fasta --frags pilon_aln.bam --output pilon1 --fix all --mindepth 0.5 --changes --verbose --threads 4 --genome is the name of the input assembly to be corrected --frags is the alignment of the reads against the assembly --output is the name of the output prefix --fix is an option for types of corrections --mindepth gives a minimum read depth to use --changes produces an output file of the changes made --verbose prints information to the screen during the run --threads : number of cores Look at the changes file: less pilon1.changes Example: We can see lots of cases where a deletion (represented by a dot) has been corrected to a base. Type q to exit. Look at the details of the fasta file: infoseq pilon1.fasta chromosome - 30060 (net +41 bases) plasmid - 2252 (no change) Change the file name: cp pilon1.fasta assembly.fasta We now have the corrected genome assembly of Staphylococcus aureus in .fasta format, containing a chromosome and a small plasmid.","title":"Run Pilon"},{"location":"modules/cmdline_assembly_v2/#questions_3","text":"Question Why don t we correct earlier in the assembly process? Answer We need to circularise the contigs and trim overhangs first. Question Why can we use some reads (Illumina) to correct other reads (PacBio) ? Answer Illumina reads have higher accuracy. Question Could we just use PacBio reads to assemble the genome? Answer Yes, if accuracy adequate.","title":"Questions"},{"location":"modules/cmdline_assembly_v2/#8-comparative-genomics","text":"In the workshop so far, we used a partial bacterial genome so that the exercises could run in the time available. As a demonstration, to better see the effect of long and short reads on the assembly, we will examine complete bacterial genome.","title":"8. Comparative Genomics"},{"location":"modules/cmdline_assembly_v2/#assemblies","text":"This bacterial genome has been assembled from either long PacBio reads (using Canu) or shorter Illumina reads (using Spades). Assembly graphs: Look at the assembly graph (usually has a suffix .gfa), in the program Bandage . This shows how contigs are related, albeit with ambiguity in some places. The assembly graph from Illumina reads (Spades assembly): The assembly graph from PacBio reads (Canu assembly) - this is missing the small plasmid: Here we can see that the long read data results in a more contiguous assembly - one complete chromosome versus many smaller contigs with ambiguous placement. Question Does it matter that an assembly is in many contigs? Answer Yes and No. Yes: broken genes can lead to missing/incorrect annotations; fragmented assemblies provide less information about the genomic structure ( e.g. the number of plasmids) and the location of genes of interest ( e.g. gene A is located on plasmid X). No: many or all genes may still be annotated correctly. Gene location is useful (e.g. chromosome, plasmid1) but not always essential (e.g. presence/absence of particular resistance genes may be enough information).","title":"Assemblies"},{"location":"modules/cmdline_assembly_v2/#annotations","text":"Genomic features such as genes can be identified with annotation tools. We have used a tool called Prokka to annotate the two genomes described above. Some of the output data is displayed here: assembly: PacBio Illumina size 2,825,804 2,792,905 contigs 2 123 CDS 2614 2575 tRNA 61 65 rRNA 19 4 Question Why are there more CDS identified in the PacBio assembly? Answer The PacBio assembly may have errors (usually a one base indel) which will cause a frame shift, which can result in three things: a longer CDS, a shorter CDS, or a shorter CDS plus an additional CDS. In addition, the Illumina assembly is about 33 kb smaller than the PacBio assembly. In bacteria, a rule of thumb is that 1 kb is roughly equal to one gene. Thus, we would probably expect about 33 fewer identified genes, which fits with these results. Question Why are there more rRNA identified in the PacBio assembly? Answer There may be multiple copies of the rRNAs and these could have been collapsed as repeats in the Illumina assembly.","title":"Annotations"},{"location":"modules/cmdline_assembly_v2/#9-summary","text":"In this workshop, we used bacterial sequencing data from long and short reads to produce a polished genome. Procedure and tools: Canu to assemble long-read PacBio data Circlator to trim and circularise contigs BWA-MEM to map shorter Illumina reads to the PacBio assembly Spades to assemble any unmapped, leftover Illumina reads (the plasmid) Pilon to correct the PacBio assembly with the more accurate Illumina reads We also looked at comparative genomics: Bandage to examine assembly graphs Prokka to annotate genomes with features such as genes Further research: Align genomes with Mauve: tutorial link Find core and pan genomes with Roary and Phandango: tutorial link Melbourne Bioinformatics tutorials: https://www.melbournebioinformatics.org.au/tutorials/ Additional microbial genomics tutorials: http://sepsis-omics.github.io/tutorials/","title":"9. Summary"},{"location":"modules/cmdline_prokka/","text":"Prokka - commandline Background In this section we will use a software tool called Prokka to annotate a bacterial genome assembly. Prokka is a \u201cwrapper\u201d; it collects together several pieces of software (from various authors), and so avoids \u201cre-inventing the wheel\u201d. Prokka finds and annotates features (both protein coding regions and RNA genes, i.e. tRNA, rRNA) present on on a sequence. Note, Prokka uses a two-step process for the annotation of protein coding regions: first, protein coding regions on the genome are identified using Prodigal ; second, the function of the encoded protein is predicted by similarity to proteins in one of many protein or protein domain databases. Prokka is a software tool that can be used to annotate bacterial, archaeal and viral genomes quickly, generating standard output files in GenBank, EMBL and gff formats. More information about Prokka can be found here . Learning objectives At the end of this tutorial you should be able to: load a genome assembly into Prokka annotate the assembly using Prokka examine the annotated genome using Artemis Input data Prokka requires assembled contigs. Download the assembled contigs from [tba] Open the GVL command line go to GVL dashboard - go to SSH - find name of virtual machine on local machine, open terminal (or equivalent) ssh to your GVL prokka --outdir mydir --prefix staph contigs.fa note: add fast output: - 10 files prokka will save the output in mydir move there: cd mydir look at the text file: cat file . txt [ check name ] look at [screenshot] look at the annotations in artemis: art mydir/staph.gff look at [screenshot]","title":"Prokka - commandline"},{"location":"modules/cmdline_prokka/#prokka-commandline","text":"","title":"Prokka - commandline"},{"location":"modules/cmdline_prokka/#background","text":"In this section we will use a software tool called Prokka to annotate a bacterial genome assembly. Prokka is a \u201cwrapper\u201d; it collects together several pieces of software (from various authors), and so avoids \u201cre-inventing the wheel\u201d. Prokka finds and annotates features (both protein coding regions and RNA genes, i.e. tRNA, rRNA) present on on a sequence. Note, Prokka uses a two-step process for the annotation of protein coding regions: first, protein coding regions on the genome are identified using Prodigal ; second, the function of the encoded protein is predicted by similarity to proteins in one of many protein or protein domain databases. Prokka is a software tool that can be used to annotate bacterial, archaeal and viral genomes quickly, generating standard output files in GenBank, EMBL and gff formats. More information about Prokka can be found here .","title":"Background"},{"location":"modules/cmdline_prokka/#learning-objectives","text":"At the end of this tutorial you should be able to: load a genome assembly into Prokka annotate the assembly using Prokka examine the annotated genome using Artemis","title":"Learning objectives"},{"location":"modules/cmdline_prokka/#input-data","text":"Prokka requires assembled contigs. Download the assembled contigs from [tba]","title":"Input data"},{"location":"modules/cmdline_prokka/#open-the-gvl-command-line","text":"go to GVL dashboard - go to SSH - find name of virtual machine on local machine, open terminal (or equivalent) ssh to your GVL prokka --outdir mydir --prefix staph contigs.fa note: add fast output: - 10 files prokka will save the output in mydir move there: cd mydir look at the text file: cat file . txt [ check name ] look at [screenshot] look at the annotations in artemis: art mydir/staph.gff look at [screenshot]","title":"Open the GVL command line"},{"location":"modules/data-dna/","text":"Dataset Import a Galaxy history Copy this link: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Microbial_tutorials/Galaxy_history_input_files.tar.gz Go to your Galaxy instance. Make sure you are registered and logged in. Refresh the page. Click on the History cog Select Import from File In the box called Archived History URL , paste in the link address to the Galaxy history (that you copied above). Click Submit Wait a few seconds. Click on the view all histories button See if the Galaxy history has been imported: it will be called imported from archive: Data Above that pane, click on the Switch to button. Then click Done (in the top left corner). You should now have a list of five files in your current history. We will use these for the Genomics Workshop; or see below for additional files. Additional Galaxy histories If you are using only part of the Genomics Workshop, you can upload any required histories listed here. Follow the instructions above. Galaxy history: FastQC https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Microbial_tutorials/FastQChistory.tar.gz Galaxy history: Spades https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Microbial_tutorials/Spadeshistory.tar.gz Galaxy history: Prokka https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Microbial_tutorials/Prokkahistory.tar.gz Galaxy history: Snippy https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Microbial_tutorials/Snippyhistory.tar.gz Galaxy history: Workflows https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Microbial_tutorials/Galaxy-History-Workflows-history.tar.gz Additional files If you need individual files, you can upload any of the files listed here. The instructions are listed below. Wildtype reference wildtype.fna https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Microbial_tutorials/wildtype.fna wildtype.gbk https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Microbial_tutorials/wildtype.gbk wildtype.gff https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Microbial_tutorials/wildtype.gff Mutant Illumina sequence mutant_R1.fastq.gz https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Microbial_tutorials/mutant_R1.fastq.gz mutant_R2.fastq.gz https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Microbial_tutorials/mutant_R2.fastq.gz Assembled contigs SPAdes_contigs.fasta https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Microbial_tutorials/SPAdes_contigs.fasta Upload to Galaxy There are two ways to upload these files to Galaxy. You can either download to your local computer and upload to Galaxy, or you can tell Galaxy to directly upload the file from an external source. Download and upload: Download required file(s) to your computer. From the Galaxy tool panel, click on Get Data Upload File Click the Choose local file button Find and select the file you downloaded and click Open Set the Type correctly. Click the Start button. Once the progress bar reaches 100%, click the Close button The file will now upload to your current history. Or, tell Galaxy to find the file from an external source: From the Galaxy tool panel, click on Get Data Upload File Click the Paste/Fetch data button Paste the URL into the box. Click the Start button. Once the progress bar reaches 100%, click the Close button The file will now upload to your current history.","title":"Home"},{"location":"modules/data-dna/#dataset","text":"","title":"Dataset"},{"location":"modules/data-dna/#import-a-galaxy-history","text":"Copy this link: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Microbial_tutorials/Galaxy_history_input_files.tar.gz Go to your Galaxy instance. Make sure you are registered and logged in. Refresh the page. Click on the History cog Select Import from File In the box called Archived History URL , paste in the link address to the Galaxy history (that you copied above). Click Submit Wait a few seconds. Click on the view all histories button See if the Galaxy history has been imported: it will be called imported from archive: Data Above that pane, click on the Switch to button. Then click Done (in the top left corner). You should now have a list of five files in your current history. We will use these for the Genomics Workshop; or see below for additional files.","title":"Import a Galaxy history"},{"location":"modules/data-dna/#additional-galaxy-histories","text":"If you are using only part of the Genomics Workshop, you can upload any required histories listed here. Follow the instructions above.","title":"Additional Galaxy histories"},{"location":"modules/data-dna/#galaxy-history-fastqc","text":"https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Microbial_tutorials/FastQChistory.tar.gz","title":"Galaxy history: FastQC"},{"location":"modules/data-dna/#galaxy-history-spades","text":"https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Microbial_tutorials/Spadeshistory.tar.gz","title":"Galaxy history: Spades"},{"location":"modules/data-dna/#galaxy-history-prokka","text":"https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Microbial_tutorials/Prokkahistory.tar.gz","title":"Galaxy history: Prokka"},{"location":"modules/data-dna/#galaxy-history-snippy","text":"https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Microbial_tutorials/Snippyhistory.tar.gz","title":"Galaxy history: Snippy"},{"location":"modules/data-dna/#galaxy-history-workflows","text":"https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Microbial_tutorials/Galaxy-History-Workflows-history.tar.gz","title":"Galaxy history: Workflows"},{"location":"modules/data-dna/#additional-files","text":"If you need individual files, you can upload any of the files listed here. The instructions are listed below.","title":"Additional files"},{"location":"modules/data-dna/#wildtype-reference","text":"wildtype.fna https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Microbial_tutorials/wildtype.fna wildtype.gbk https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Microbial_tutorials/wildtype.gbk wildtype.gff https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Microbial_tutorials/wildtype.gff","title":"Wildtype reference"},{"location":"modules/data-dna/#mutant-illumina-sequence","text":"mutant_R1.fastq.gz https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Microbial_tutorials/mutant_R1.fastq.gz mutant_R2.fastq.gz https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Microbial_tutorials/mutant_R2.fastq.gz","title":"Mutant Illumina sequence"},{"location":"modules/data-dna/#assembled-contigs","text":"SPAdes_contigs.fasta https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Microbial_tutorials/SPAdes_contigs.fasta","title":"Assembled contigs"},{"location":"modules/data-dna/#upload-to-galaxy","text":"There are two ways to upload these files to Galaxy. You can either download to your local computer and upload to Galaxy, or you can tell Galaxy to directly upload the file from an external source. Download and upload: Download required file(s) to your computer. From the Galaxy tool panel, click on Get Data Upload File Click the Choose local file button Find and select the file you downloaded and click Open Set the Type correctly. Click the Start button. Once the progress bar reaches 100%, click the Close button The file will now upload to your current history. Or, tell Galaxy to find the file from an external source: From the Galaxy tool panel, click on Get Data Upload File Click the Paste/Fetch data button Paste the URL into the box. Click the Start button. Once the progress bar reaches 100%, click the Close button The file will now upload to your current history.","title":"Upload to Galaxy"},{"location":"modules/dge/","text":"RNA-Seq: Bacteria This tutorial is about using RNA-seq data to investigate differential gene expression in bacteria, using Galaxy tools and Degust (a tool on the web). New to Galaxy? First try the introduction and then learn some key tasks Background Differential Gene Expression (DGE) is the process of determining whether any genes were expressed at a different level between two conditions. For example, the conditions could be wildtype versus mutant, or two growth conditions. Usually multiple biological replicates are done for each condition - these are needed to separate variation within the condition from that between the conditions. Input data: reads and reference RNA-Seq reads A typical experiment will have 2 conditions each with 3 replicates, for a total of 6 samples. Our RNA-seq reads are from 6 samples in FASTQ format. We have single-end reads; so one file per sample. Data could also be paired-end reads, and there would be two files per sample. These have been reduced to 1% of their original size for this tutorial. The experiment used the bacteria E. coli grown in two conditions. Files labelled LB are the wildtype Files labelled MG have been exposed to 0.5% MG - alpha methyglucoside (a sugar solution). Reference genome The reference genomes is in FASTA format and the gene annotations are in GTF format. The FASTA file contains the DNA sequence(s) that make up the genome; e.g. the chromosome and any plasmids. The GTF file lists the coordinates (position) of each feature. Commonly-annotated features are genes, transcripts and protein-coding sequences. Upload files to Galaxy Log in to your Galaxy instance (for example, Galaxy Australia, usegalaxy.org.au ). Use shared data If you are using Galaxy Australia, you can import the data from a shared data library. In the top menu bar, go to Shared Data . Click on Data Libraries . Click on Galaxy Australia Training Material: RNA-Seq: Microbial RNA-Seq . Tick the boxes next to the nine files. Click the To History button, select As Datasets . Name a new history and click Import . In the top menu bar, click Analyze Data . [Optional] Next to each file, click on the pencil icon and change (shorten) its name. You should now have eight files in your current history ready for the analysis, plus an additional JBrowse file that we will use later. Or, import from the web *Only follow this step if unable to load the data files from shared data, as described above. Click here to expand.* * In a new browser tab, go to this webpage: * [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1311269.svg)](https://doi.org/10.5281/zenodo.1311269) * For each file, right click on file name: select \"copy link address\" * In Galaxy, go to Get Data and then Upload File * Click Paste/Fetch data * A box will appear: paste in link address * Click Start * Click Close * The file will now appear in the top of your history panel. * Repeat for all files in Zenodo. * Change (shorten) the file names with the pencil icon. Convert the GTF file In the tool panel, search for GTF . Click on GTF-to-GFF converter select the Ecoli_k12.gtf file Click Execute This will produce a convereted reference genome file that we need in a different format for downstream analyses. Re-name the output file with the pencil icon, e.g. to Ecoli_k12.gff . Align reads to reference The RNA-Seq reads are fragmented and are not complete transcripts. To determine the transcripts from which the reads originated (and therefore, to which gene they correspond) we can map them to a reference genome. In Galaxy: In the Tools panel, search for Map with BWA-MEM and click on it. Under Will you select a reference genome from your history or use a built-in index? : Use a genome from history and build index Use the following dataset as the reference sequence : Ecoli_k12.fasta Single or Paired-end reads : single Select fastq dataset : Click on the Multiple Datasets icon in centre Select all 6 FASTQ files (they turn blue; use side-scroll bar to check all have been selected) This will map each set of reads to the reference genome Your tool interface should look like this: Click Execute Click Refresh in the history pane to see if the analysis has finished. Output: 6 bam files of reads mapped to the reference genome. Re-name the output files: These are called Map with BWA-MEM on data x and data x . The x will refer to the numbered file that Galaxy used in the analysis. Click on the pencil icon next to each of these and re-name them as their sample name, e.g. LB1.bam , LB2.bam etc. View the mapped reads In the tool panel, search for the tool JBrowse and click on JBrowse Genome Browser . Under Reference genome to display choose Use a genome from history . Under Select the reference genome choose Ecoli_k12.fasta . For Produce a Standalone Instance select Yes . For Genetic Code choose 11: The Bacterial, Archaeal and Plant Plastid Code . We will now set up several different tracks - these are datasets displayed underneath the reference sequence (which is displayed as nucleotides in FASTA format). We will choose to display the mapped reads from one of each of the experimental conditions, and the annotated reference genome. Track 1 - mapped RNA-seq reads Click Insert Track Group For Track Cateogry name it RNA-seq reads Click Insert Annotation Track For Track Type choose BAM Pileups For BAM Track Data select LB1.bam and MG1.bam (Your files may be named differently) For Autogenerate SNP Track select Yes Track 2 - annotated reference Click Insert Track Group again For Track Category name it annotated reference Click Insert Annotation Track For Track Type choose GFF/GFF3/BED/GBK Features For Track Data select Ecoli_k12.gff Note - select the GFF not the GTF file Click Execute A new file will be created, called JBrowse on data XX and data XX - Complete . This file may take some time to be generated. For this workshop, we downloaded a completed JBrowse file at the start, which you could look at instead (it should be the same). This is the JBrowse on data XX and data XX - Complete file; the numbers are not important. Click on the eye icon next to the file name. The JBrowse window will appear in the centre Galaxy panel. On the left, tick boxes to display the tracks Use the plus and minus buttons to zoom in and out. This visualization is for us to check that the mapping of the reads worked as expected. You may be able to see some places where more reads have mapped to a gene from one of the conditions. This suggests that the gene in that condition was more highly expressed. However, this needs to be verified with statistical testing, which will be covered in the next parts of this tutorial. Count reads per gene We now need to count how many reads overlap with particular genes. The information about gene names is from the annotations in the GTF file. In Galaxy: In the Tools panel, search for count matrix and click on SAM/BAM to count matrix Note: Don t select the tool called htseq-count . The SAM/BAM to count matrix also uses that tool but allows an input of multiple bam files, which is what we want. For Gene model (GFF) file to count reads over from your current history , select the GTF file. For Reads are stranded select Yes (box turns dark grey) Leave the next two settings as default For GTF feature type for counting reads select transcript . For bam/sam file from your history choose the 6 bam files. Your tool interface should look like this: Click Execute Click Refresh in the history pane to see if the analysis has finished. Output There is one output file: bams to DGE count matrix . Click on the file name to expand the information in the History pane. Click on the file icon underneath to download it to your computer for use later on in this tutorial. Click on the eye icon to see this file. Each row is a gene (or feature) and each column is a sample, with counts against each gene. Have a look at how the counts vary between samples, per gene. (These are quite low as we are using a cut-down data set). We can t just compare the counts directly; they need to be normalized before comparison, and this will be done as part of the DGE analysis in the next step. Test for differential expression There are various tools available to test for differential gene expression. In today s tutorial, we will use the tool Voom (link to the paper here ). In the Tools panel, search for Differential_Count models (don t forget the underscore) and click on it. This has options to use edgeR, DESeq, or Voom. Here we will use Voom. For Select an input matrix choose the count matrix file generated in the previous step. For Title for job outputs enter DGE using voom . For Select columns containing treatment tick boxes for the MG samples. For Select columns containing control tick boxes for the LB samples. Under Run this model using edgeR choose Do not run edgeR . Under Run the same model with DESeq2 and compare findings choose Do not run DESeq2 . Under Run the same model with Voom/limma and compare findings choose Run VOOM . Your tool interface should look like this: Click Execute . Output There are two output files. We will look at the file called DEGusingvoom_topTable_VOOM.xls . The Contig column shows the genes that had transcripts mapped (genes with low counts may be filtered out). The adj.P.Val is the statistical significance (p value) adjusted for multiple testing. The table is sorted by this column (most significant to least significant). The logFC is the log2 fold change, which is the change in gene expression between the treatment and control group. We can see that the most statistically signifcant result is that the ptsG gene was expressed differently between the two conditions. Its expression in the treatment group was lower. The log base 2 change in expression is approximately -4, meaning the difference in expression is approximately 16x. To check, let s look at the ptsG gene in the original table of counts, the bams to DGE count matrix . Click on the eye icon to view. Search for the ptsG gene (Cmd-F to search on a Mac). Do the read counts look different between the control samples and the treatment samples? DGE in Degust Degust is a tool on the web that can analyse the counts files produced in the step above, to test for differential gene expression. (Degust can also display the results from DGE analyses performed elsewhere.) Upload counts file Go to the Degust web page . Click Upload your counts file . Click on Choose File . Select the htseq output file. tabular (that you previously downloaded to your computer from Galaxy) and click Open . This file may have a different name, for example, it may have Galaxy in the title. Click Upload . A Configuation page will appear. For Name type DGE in E coli For Info columns select Contig For Min gene read count put 10 . Click Add condition Add a condition called Control and select the LB columns. Add a condition called Treament and select the MG columns. Save changes View - this brings up the Degust viewing window. Overview of Degust sections Left: Conditions: Control and Treatment. Left: Method selection for DGE. Top centre: Plots, with options at right. When an expression plot is selected, a heatmap appears below. A table of genes (or features); expression in treatment relative to control (Treatment column); and significance (FDR column). The FDR is the False Discovery Rate, also known as the adjusted p value. Analyze gene expression Under Method , make sure that Voom/Limma is selected. Click Apply . This runs Voom/Limma on the uploaded counts. MDS plot First, look at the MDS plot. This is a multidimensional scaling plot which represents the variation between samples. Ideally: All the LB samples would be close to each other All the MG samples would be close to each other The LB and MG groups would be far apart The x-axis is the dimension with the highest magnitude. The control/treatment samples should be split along this axis. Our LB samples are on the left and the MG samples are on the right, which means they are well separated on their major MDS dimension, which looks correct. Expression - MA plot Each dot shows the change in expression in one gene. The average expression (over both condition and treatment samples) is represented on the x-axis. Plot points should be symmetrical around the x-axis. We can see that many genes are expressed at a low level, and some are highly expressed. The fold change is represented on the y axis. If expression is significantly different between treatment and control, the dots are red. If not, they are blue. (In Degust, significant means FDR 0.05). At low levels of gene expression (low values of the x axis), fold changes are less likely to be significant. Click on the dot to see the gene name. Expression - Volcano plot Another way to view expression levels is with the volcano plot. As with the MA plot, each dot is a gene. This time, the logFC axis is horizontal (in the MA plot, it was vertical). The vertical axis is a measure of statistical significance (-log10 FDR). If expression is significantly different between treatment and control, the dots are red. If not, they are blue. (In Degust, significant means FDR 0.05). This is a quick way to visualize those genes with large changes in expression (the left and right sides of the graph). Expression - Parallel Coordinates and heatmap Each line shows the change in expression in one gene, between control and treatment. Go to Options at the right. For FDR cut-off set at 0.001. This is a significance level (an adjusted p value). We will set it quite low in this example, to ensure we only examine key differences. Look at the Parallel Coordinates plot. There are two axes: Left: Control : Gene expression in the control samples. All values are set at zero. Right: Treatment Gene expression in the treatment samples, relative to expression in the control. The blocks of blue and red underneath the plot are called a heatmap. Each block is a gene. Click on a block to see its line in the plot above. Look at the row for the Treatment. Relative to the control, genes expressed more are red; genes expressed less are blue. Note: for an experiment with multiple treatments, the various treatment axes can be dragged to rearrange. There is no natural order (such as a time series). Table of genes Contig : names of genes. Note that gene names are sometimes specific to a species, or they may be only named as a locus ID (a chromosomal location specified in the genome annotation). FDR : False Discovery Rate. This is an adjusted p value to show the significance of the difference in gene expression between two conditions. Click on column headings to sort. By default, this table is sorted by FDR. Control and Treatment : log2(Fold Change) of gene expression. The default display is of fold change in the treatment relative to the control. Therefore, values in the Control column are zero. This can be changed in the Options panel at the top right. In some cases, a large fold change will be meaningful but in others, even a small fold change can be important biologically. Table of genes and expression: Investigate differentially-expressed genes To learn more about the differentially-expressed genes: Go to the NCBI website. Under All Databases , click on Gene Enter the gene name in the search bar; e.g. ptsG Click on the first result that matches the species (e.g. in this case, E. coli ). This provides information about the gene, and may also show further references (e.g. in this case, a link to the EcoGene resource). Some of the most (statistically) significant differentially-expressed genes in this experiment are: ptsG : a glucose-specific transporter. setA : a sugar efflux transporter; is induced by glucose-phosphate stress. sucD : the alpha subunit of the the gene for succinyl-CoA synthetase; involved in ATP production. sucB : a component of the 2-oxoglutarate dehydrogenase complex; catalyzes a step in the Krebs cycle. deoC : 2-deoxyribose-5-phosphate aldolase; binds selenium; may be involved in selenium transport. See this history in Galaxy If you want to see this Galaxy history without performing the steps above: Log in to Galaxy Australia: https://usegalaxy.org.au/ Go to Shared Data Click Histories Click Published-RNA-seq-bacteria Click Import (at the top right corner) The analysis should now be showing as your current history. More information Here are some references covering more information about RNA-seq. A clear, up-to-date introduction to RNA-seq and testing for differential expression (pre-print): Berge, K Hembach, K Soneson, C Tiberi, S Clement, L Love, M Patro, R Robinson, M. RNA sequencing data: hitchhiker\u2019s guide to expression analysis. PeerJPreprints. Available from: http://dx.doi.org/10.7287/peerj.preprints.27283 Transcriptome assembly: Martin JA, Wang Z. Next-generation transcriptome assembly. Nat Rev Genet. 2011 Sep 7;12(10):671\u201382. SuperTranscripts: Davidson NM, Hawkins ADK, Oshlack A. SuperTranscripts: a data driven reference for analysis and visualisation of transcriptomes. Genome Biol. 2017 Aug 4;18(1):148. Splicing: Alamancos GP, Agirre E, Eyras E. Methods to study splicing from high-throughput RNA sequencing data. Methods Mol Biol. 2014;1126:357\u201397. Single-cell RNA-seq: McCarthy DJ, Campbell KR, Lun ATL, Wills QF. Scater: pre-processing, quality control, normalization and visualization of single-cell RNA-seq data in R. Bioinformatics. 2017 Apr 15;33(8):1179\u201386. Pathway analysis: Khatri P, Sirota M, Butte AJ. Ten years of pathway analysis: current approaches and outstanding challenges. PLoS Comput Biol. 2012 Feb 23;8(2):e1002375. What s next? To use the tutorials on this website: see the list in the left hand panel or, click the menu button (three horizontal bars) in the top left of the page You can find more tutorials at the Galaxy Training Network: http://galaxyproject.github.io/training-material/","title":"RNA-seq - bacteria"},{"location":"modules/dge/#rna-seq-bacteria","text":"This tutorial is about using RNA-seq data to investigate differential gene expression in bacteria, using Galaxy tools and Degust (a tool on the web). New to Galaxy? First try the introduction and then learn some key tasks","title":"RNA-Seq: Bacteria"},{"location":"modules/dge/#background","text":"Differential Gene Expression (DGE) is the process of determining whether any genes were expressed at a different level between two conditions. For example, the conditions could be wildtype versus mutant, or two growth conditions. Usually multiple biological replicates are done for each condition - these are needed to separate variation within the condition from that between the conditions.","title":"Background"},{"location":"modules/dge/#input-data-reads-and-reference","text":"RNA-Seq reads A typical experiment will have 2 conditions each with 3 replicates, for a total of 6 samples. Our RNA-seq reads are from 6 samples in FASTQ format. We have single-end reads; so one file per sample. Data could also be paired-end reads, and there would be two files per sample. These have been reduced to 1% of their original size for this tutorial. The experiment used the bacteria E. coli grown in two conditions. Files labelled LB are the wildtype Files labelled MG have been exposed to 0.5% MG - alpha methyglucoside (a sugar solution). Reference genome The reference genomes is in FASTA format and the gene annotations are in GTF format. The FASTA file contains the DNA sequence(s) that make up the genome; e.g. the chromosome and any plasmids. The GTF file lists the coordinates (position) of each feature. Commonly-annotated features are genes, transcripts and protein-coding sequences.","title":"Input data: reads and reference"},{"location":"modules/dge/#upload-files-to-galaxy","text":"Log in to your Galaxy instance (for example, Galaxy Australia, usegalaxy.org.au ).","title":"Upload files to Galaxy"},{"location":"modules/dge/#use-shared-data","text":"If you are using Galaxy Australia, you can import the data from a shared data library. In the top menu bar, go to Shared Data . Click on Data Libraries . Click on Galaxy Australia Training Material: RNA-Seq: Microbial RNA-Seq . Tick the boxes next to the nine files. Click the To History button, select As Datasets . Name a new history and click Import . In the top menu bar, click Analyze Data . [Optional] Next to each file, click on the pencil icon and change (shorten) its name. You should now have eight files in your current history ready for the analysis, plus an additional JBrowse file that we will use later.","title":"Use shared data"},{"location":"modules/dge/#or-import-from-the-web","text":"*Only follow this step if unable to load the data files from shared data, as described above. Click here to expand.* * In a new browser tab, go to this webpage: * [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1311269.svg)](https://doi.org/10.5281/zenodo.1311269) * For each file, right click on file name: select \"copy link address\" * In Galaxy, go to Get Data and then Upload File * Click Paste/Fetch data * A box will appear: paste in link address * Click Start * Click Close * The file will now appear in the top of your history panel. * Repeat for all files in Zenodo. * Change (shorten) the file names with the pencil icon.","title":"Or, import from the web"},{"location":"modules/dge/#convert-the-gtf-file","text":"In the tool panel, search for GTF . Click on GTF-to-GFF converter select the Ecoli_k12.gtf file Click Execute This will produce a convereted reference genome file that we need in a different format for downstream analyses. Re-name the output file with the pencil icon, e.g. to Ecoli_k12.gff .","title":"Convert the GTF file"},{"location":"modules/dge/#align-reads-to-reference","text":"The RNA-Seq reads are fragmented and are not complete transcripts. To determine the transcripts from which the reads originated (and therefore, to which gene they correspond) we can map them to a reference genome. In Galaxy: In the Tools panel, search for Map with BWA-MEM and click on it. Under Will you select a reference genome from your history or use a built-in index? : Use a genome from history and build index Use the following dataset as the reference sequence : Ecoli_k12.fasta Single or Paired-end reads : single Select fastq dataset : Click on the Multiple Datasets icon in centre Select all 6 FASTQ files (they turn blue; use side-scroll bar to check all have been selected) This will map each set of reads to the reference genome Your tool interface should look like this: Click Execute Click Refresh in the history pane to see if the analysis has finished. Output: 6 bam files of reads mapped to the reference genome. Re-name the output files: These are called Map with BWA-MEM on data x and data x . The x will refer to the numbered file that Galaxy used in the analysis. Click on the pencil icon next to each of these and re-name them as their sample name, e.g. LB1.bam , LB2.bam etc.","title":"Align reads to reference"},{"location":"modules/dge/#view-the-mapped-reads","text":"In the tool panel, search for the tool JBrowse and click on JBrowse Genome Browser . Under Reference genome to display choose Use a genome from history . Under Select the reference genome choose Ecoli_k12.fasta . For Produce a Standalone Instance select Yes . For Genetic Code choose 11: The Bacterial, Archaeal and Plant Plastid Code . We will now set up several different tracks - these are datasets displayed underneath the reference sequence (which is displayed as nucleotides in FASTA format). We will choose to display the mapped reads from one of each of the experimental conditions, and the annotated reference genome. Track 1 - mapped RNA-seq reads Click Insert Track Group For Track Cateogry name it RNA-seq reads Click Insert Annotation Track For Track Type choose BAM Pileups For BAM Track Data select LB1.bam and MG1.bam (Your files may be named differently) For Autogenerate SNP Track select Yes Track 2 - annotated reference Click Insert Track Group again For Track Category name it annotated reference Click Insert Annotation Track For Track Type choose GFF/GFF3/BED/GBK Features For Track Data select Ecoli_k12.gff Note - select the GFF not the GTF file Click Execute A new file will be created, called JBrowse on data XX and data XX - Complete . This file may take some time to be generated. For this workshop, we downloaded a completed JBrowse file at the start, which you could look at instead (it should be the same). This is the JBrowse on data XX and data XX - Complete file; the numbers are not important. Click on the eye icon next to the file name. The JBrowse window will appear in the centre Galaxy panel. On the left, tick boxes to display the tracks Use the plus and minus buttons to zoom in and out. This visualization is for us to check that the mapping of the reads worked as expected. You may be able to see some places where more reads have mapped to a gene from one of the conditions. This suggests that the gene in that condition was more highly expressed. However, this needs to be verified with statistical testing, which will be covered in the next parts of this tutorial.","title":"View the mapped reads"},{"location":"modules/dge/#count-reads-per-gene","text":"We now need to count how many reads overlap with particular genes. The information about gene names is from the annotations in the GTF file. In Galaxy: In the Tools panel, search for count matrix and click on SAM/BAM to count matrix Note: Don t select the tool called htseq-count . The SAM/BAM to count matrix also uses that tool but allows an input of multiple bam files, which is what we want. For Gene model (GFF) file to count reads over from your current history , select the GTF file. For Reads are stranded select Yes (box turns dark grey) Leave the next two settings as default For GTF feature type for counting reads select transcript . For bam/sam file from your history choose the 6 bam files. Your tool interface should look like this: Click Execute Click Refresh in the history pane to see if the analysis has finished.","title":"Count reads per gene"},{"location":"modules/dge/#output","text":"There is one output file: bams to DGE count matrix . Click on the file name to expand the information in the History pane. Click on the file icon underneath to download it to your computer for use later on in this tutorial. Click on the eye icon to see this file. Each row is a gene (or feature) and each column is a sample, with counts against each gene. Have a look at how the counts vary between samples, per gene. (These are quite low as we are using a cut-down data set). We can t just compare the counts directly; they need to be normalized before comparison, and this will be done as part of the DGE analysis in the next step.","title":"Output"},{"location":"modules/dge/#test-for-differential-expression","text":"There are various tools available to test for differential gene expression. In today s tutorial, we will use the tool Voom (link to the paper here ). In the Tools panel, search for Differential_Count models (don t forget the underscore) and click on it. This has options to use edgeR, DESeq, or Voom. Here we will use Voom. For Select an input matrix choose the count matrix file generated in the previous step. For Title for job outputs enter DGE using voom . For Select columns containing treatment tick boxes for the MG samples. For Select columns containing control tick boxes for the LB samples. Under Run this model using edgeR choose Do not run edgeR . Under Run the same model with DESeq2 and compare findings choose Do not run DESeq2 . Under Run the same model with Voom/limma and compare findings choose Run VOOM . Your tool interface should look like this: Click Execute .","title":"Test for differential expression"},{"location":"modules/dge/#output_1","text":"There are two output files. We will look at the file called DEGusingvoom_topTable_VOOM.xls . The Contig column shows the genes that had transcripts mapped (genes with low counts may be filtered out). The adj.P.Val is the statistical significance (p value) adjusted for multiple testing. The table is sorted by this column (most significant to least significant). The logFC is the log2 fold change, which is the change in gene expression between the treatment and control group. We can see that the most statistically signifcant result is that the ptsG gene was expressed differently between the two conditions. Its expression in the treatment group was lower. The log base 2 change in expression is approximately -4, meaning the difference in expression is approximately 16x. To check, let s look at the ptsG gene in the original table of counts, the bams to DGE count matrix . Click on the eye icon to view. Search for the ptsG gene (Cmd-F to search on a Mac). Do the read counts look different between the control samples and the treatment samples?","title":"Output"},{"location":"modules/dge/#dge-in-degust","text":"Degust is a tool on the web that can analyse the counts files produced in the step above, to test for differential gene expression. (Degust can also display the results from DGE analyses performed elsewhere.)","title":"DGE in Degust"},{"location":"modules/dge/#upload-counts-file","text":"Go to the Degust web page . Click Upload your counts file . Click on Choose File . Select the htseq output file. tabular (that you previously downloaded to your computer from Galaxy) and click Open . This file may have a different name, for example, it may have Galaxy in the title. Click Upload . A Configuation page will appear. For Name type DGE in E coli For Info columns select Contig For Min gene read count put 10 . Click Add condition Add a condition called Control and select the LB columns. Add a condition called Treament and select the MG columns. Save changes View - this brings up the Degust viewing window.","title":"Upload counts file"},{"location":"modules/dge/#overview-of-degust-sections","text":"Left: Conditions: Control and Treatment. Left: Method selection for DGE. Top centre: Plots, with options at right. When an expression plot is selected, a heatmap appears below. A table of genes (or features); expression in treatment relative to control (Treatment column); and significance (FDR column). The FDR is the False Discovery Rate, also known as the adjusted p value.","title":"Overview of Degust sections"},{"location":"modules/dge/#analyze-gene-expression","text":"Under Method , make sure that Voom/Limma is selected. Click Apply . This runs Voom/Limma on the uploaded counts.","title":"Analyze gene expression"},{"location":"modules/dge/#mds-plot","text":"First, look at the MDS plot. This is a multidimensional scaling plot which represents the variation between samples. Ideally: All the LB samples would be close to each other All the MG samples would be close to each other The LB and MG groups would be far apart The x-axis is the dimension with the highest magnitude. The control/treatment samples should be split along this axis. Our LB samples are on the left and the MG samples are on the right, which means they are well separated on their major MDS dimension, which looks correct.","title":"MDS plot"},{"location":"modules/dge/#expression-ma-plot","text":"Each dot shows the change in expression in one gene. The average expression (over both condition and treatment samples) is represented on the x-axis. Plot points should be symmetrical around the x-axis. We can see that many genes are expressed at a low level, and some are highly expressed. The fold change is represented on the y axis. If expression is significantly different between treatment and control, the dots are red. If not, they are blue. (In Degust, significant means FDR 0.05). At low levels of gene expression (low values of the x axis), fold changes are less likely to be significant. Click on the dot to see the gene name.","title":"Expression - MA plot"},{"location":"modules/dge/#expression-volcano-plot","text":"Another way to view expression levels is with the volcano plot. As with the MA plot, each dot is a gene. This time, the logFC axis is horizontal (in the MA plot, it was vertical). The vertical axis is a measure of statistical significance (-log10 FDR). If expression is significantly different between treatment and control, the dots are red. If not, they are blue. (In Degust, significant means FDR 0.05). This is a quick way to visualize those genes with large changes in expression (the left and right sides of the graph).","title":"Expression - Volcano plot"},{"location":"modules/dge/#expression-parallel-coordinates-and-heatmap","text":"Each line shows the change in expression in one gene, between control and treatment. Go to Options at the right. For FDR cut-off set at 0.001. This is a significance level (an adjusted p value). We will set it quite low in this example, to ensure we only examine key differences. Look at the Parallel Coordinates plot. There are two axes: Left: Control : Gene expression in the control samples. All values are set at zero. Right: Treatment Gene expression in the treatment samples, relative to expression in the control. The blocks of blue and red underneath the plot are called a heatmap. Each block is a gene. Click on a block to see its line in the plot above. Look at the row for the Treatment. Relative to the control, genes expressed more are red; genes expressed less are blue. Note: for an experiment with multiple treatments, the various treatment axes can be dragged to rearrange. There is no natural order (such as a time series).","title":"Expression - Parallel Coordinates and heatmap"},{"location":"modules/dge/#table-of-genes","text":"Contig : names of genes. Note that gene names are sometimes specific to a species, or they may be only named as a locus ID (a chromosomal location specified in the genome annotation). FDR : False Discovery Rate. This is an adjusted p value to show the significance of the difference in gene expression between two conditions. Click on column headings to sort. By default, this table is sorted by FDR. Control and Treatment : log2(Fold Change) of gene expression. The default display is of fold change in the treatment relative to the control. Therefore, values in the Control column are zero. This can be changed in the Options panel at the top right. In some cases, a large fold change will be meaningful but in others, even a small fold change can be important biologically. Table of genes and expression:","title":"Table of genes"},{"location":"modules/dge/#investigate-differentially-expressed-genes","text":"To learn more about the differentially-expressed genes: Go to the NCBI website. Under All Databases , click on Gene Enter the gene name in the search bar; e.g. ptsG Click on the first result that matches the species (e.g. in this case, E. coli ). This provides information about the gene, and may also show further references (e.g. in this case, a link to the EcoGene resource). Some of the most (statistically) significant differentially-expressed genes in this experiment are: ptsG : a glucose-specific transporter. setA : a sugar efflux transporter; is induced by glucose-phosphate stress. sucD : the alpha subunit of the the gene for succinyl-CoA synthetase; involved in ATP production. sucB : a component of the 2-oxoglutarate dehydrogenase complex; catalyzes a step in the Krebs cycle. deoC : 2-deoxyribose-5-phosphate aldolase; binds selenium; may be involved in selenium transport.","title":"Investigate differentially-expressed genes"},{"location":"modules/dge/#see-this-history-in-galaxy","text":"If you want to see this Galaxy history without performing the steps above: Log in to Galaxy Australia: https://usegalaxy.org.au/ Go to Shared Data Click Histories Click Published-RNA-seq-bacteria Click Import (at the top right corner) The analysis should now be showing as your current history.","title":"See this history in Galaxy"},{"location":"modules/dge/#more-information","text":"Here are some references covering more information about RNA-seq. A clear, up-to-date introduction to RNA-seq and testing for differential expression (pre-print): Berge, K Hembach, K Soneson, C Tiberi, S Clement, L Love, M Patro, R Robinson, M. RNA sequencing data: hitchhiker\u2019s guide to expression analysis. PeerJPreprints. Available from: http://dx.doi.org/10.7287/peerj.preprints.27283 Transcriptome assembly: Martin JA, Wang Z. Next-generation transcriptome assembly. Nat Rev Genet. 2011 Sep 7;12(10):671\u201382. SuperTranscripts: Davidson NM, Hawkins ADK, Oshlack A. SuperTranscripts: a data driven reference for analysis and visualisation of transcriptomes. Genome Biol. 2017 Aug 4;18(1):148. Splicing: Alamancos GP, Agirre E, Eyras E. Methods to study splicing from high-throughput RNA sequencing data. Methods Mol Biol. 2014;1126:357\u201397. Single-cell RNA-seq: McCarthy DJ, Campbell KR, Lun ATL, Wills QF. Scater: pre-processing, quality control, normalization and visualization of single-cell RNA-seq data in R. Bioinformatics. 2017 Apr 15;33(8):1179\u201386. Pathway analysis: Khatri P, Sirota M, Butte AJ. Ten years of pathway analysis: current approaches and outstanding challenges. PLoS Comput Biol. 2012 Feb 23;8(2):e1002375.","title":"More information"},{"location":"modules/dge/#whats-next","text":"To use the tutorials on this website: see the list in the left hand panel or, click the menu button (three horizontal bars) in the top left of the page You can find more tutorials at the Galaxy Training Network: http://galaxyproject.github.io/training-material/","title":"What's next?"},{"location":"modules/fastqc/","text":"FastQC in Galaxy After sequencing, the reads should be checked for their quality. This tutorial demonstrates how to use the tool called FastQC to examine bacterial paired-end Illumina sequence reads. The FastQC website is here. New to Galaxy? First try the introduction and then learn some key tasks Import the data Log in to your Galaxy instance (for example, Galaxy Australia, usegalaxy.org.au ). Create a new history for this analysis. In a new browser tab, go to this webpage: Find the file called mutant_R1.fastq Right click on file name: select copy link address In Galaxy, go to Get Data and then Upload File Click Paste/Fetch data A box will appear: paste in link address Click Start Click Close The file will now appear in the top of your history panel. The file name is quite long: let s change it: Click on the pencil icon next to the file name. In the centre Galaxy panel, click in the box under Name Shorten the file name to mutant_R1.fastq Then click Save FASTQ is a file format for sequence reads that displays quality scores for each of the sequenced nucleotides. For more information about FASTQ format see this link . We will evaluate the mutant_R1.fastq reads using the FastQC tool. Run FastQC In the Tool panel search box, search for FastQC ; then click on the tool FastQC . The tool interface will appear in the centre Galaxy panel. for Short read data from your current history : mutant_R1.fastq Click Execute In the History pane, click on the refresh icon to see if the analysis has finished. Examine output files Once finished, examine the output called FastQC on data1:webpage (Hint: click the eye icon). It has a summary at the top of the page and a number of graphs. Look at: Basic Statistics Sequence length : will be important in setting maximum k-mer size value for assembly. Encoding : The quality encoding type is important for quality trimming software. % GC : high GC organisms don\u2019t tend to assemble well and may have an uneven read coverage distribution. Total sequences : Total number of reads: gives you an idea of coverage. Per base sequence quality : Dips in quality near the beginning, middle or end of the reads: determines possible trimming/cleanup methods and parameters and may indicate technical problems with the sequencing process/machine run. In this case, all the reads are of relatively high quality across their length (150 bp). Per base N content : Presence of large numbers of Ns in reads may point to a poor quality sequencing run. You would need to trim these reads to remove Ns. General questions you might ask about your input reads include: How good is my read set? Do I need to ask for a new sequencing run? Is it suitable for the analysis I need to do? For a fuller discussion of FastQC outputs and warnings, see: the FastQC website link , including the section on each of the output reports , and examples of good and bad Illumina data. For a more general introduction to quality control, see: this collection of articles about common sequencing problems . What s next? To use the tutorials on this website: see the list in the left hand panel or, click the menu button (three horizontal bars) in the top left of the page You can find more tutorials at the Galaxy Training Network: http://galaxyproject.github.io/training-material/","title":"Quality control"},{"location":"modules/fastqc/#fastqc-in-galaxy","text":"After sequencing, the reads should be checked for their quality. This tutorial demonstrates how to use the tool called FastQC to examine bacterial paired-end Illumina sequence reads. The FastQC website is here. New to Galaxy? First try the introduction and then learn some key tasks","title":"FastQC in Galaxy"},{"location":"modules/fastqc/#import-the-data","text":"Log in to your Galaxy instance (for example, Galaxy Australia, usegalaxy.org.au ). Create a new history for this analysis. In a new browser tab, go to this webpage: Find the file called mutant_R1.fastq Right click on file name: select copy link address In Galaxy, go to Get Data and then Upload File Click Paste/Fetch data A box will appear: paste in link address Click Start Click Close The file will now appear in the top of your history panel. The file name is quite long: let s change it: Click on the pencil icon next to the file name. In the centre Galaxy panel, click in the box under Name Shorten the file name to mutant_R1.fastq Then click Save FASTQ is a file format for sequence reads that displays quality scores for each of the sequenced nucleotides. For more information about FASTQ format see this link . We will evaluate the mutant_R1.fastq reads using the FastQC tool.","title":"Import the data"},{"location":"modules/fastqc/#run-fastqc","text":"In the Tool panel search box, search for FastQC ; then click on the tool FastQC . The tool interface will appear in the centre Galaxy panel. for Short read data from your current history : mutant_R1.fastq Click Execute In the History pane, click on the refresh icon to see if the analysis has finished.","title":"Run FastQC"},{"location":"modules/fastqc/#examine-output-files","text":"Once finished, examine the output called FastQC on data1:webpage (Hint: click the eye icon). It has a summary at the top of the page and a number of graphs. Look at: Basic Statistics Sequence length : will be important in setting maximum k-mer size value for assembly. Encoding : The quality encoding type is important for quality trimming software. % GC : high GC organisms don\u2019t tend to assemble well and may have an uneven read coverage distribution. Total sequences : Total number of reads: gives you an idea of coverage. Per base sequence quality : Dips in quality near the beginning, middle or end of the reads: determines possible trimming/cleanup methods and parameters and may indicate technical problems with the sequencing process/machine run. In this case, all the reads are of relatively high quality across their length (150 bp). Per base N content : Presence of large numbers of Ns in reads may point to a poor quality sequencing run. You would need to trim these reads to remove Ns. General questions you might ask about your input reads include: How good is my read set? Do I need to ask for a new sequencing run? Is it suitable for the analysis I need to do? For a fuller discussion of FastQC outputs and warnings, see: the FastQC website link , including the section on each of the output reports , and examples of good and bad Illumina data. For a more general introduction to quality control, see: this collection of articles about common sequencing problems .","title":"Examine output files"},{"location":"modules/fastqc/#whats-next","text":"To use the tutorials on this website: see the list in the left hand panel or, click the menu button (three horizontal bars) in the top left of the page You can find more tutorials at the Galaxy Training Network: http://galaxyproject.github.io/training-material/","title":"What's next?"},{"location":"modules/freebayes/","text":"Comparative Genomics Background In this activity we will identify \u2018micro\u2019 differences between genome sequences using the BWA short read mapper and Freebayes variant caller. After investigating the \u2018micro\u2019 differences (SNPs/INDELS), we will attempt to detect larger \u2018macro\u2019 differences using Mauve. Learning objectives At the end of this tutorial you should be able to: map sequence reads to a reference genome view the mapped reads identify variants using Freebayes, and identify larger structural variants using Mauve. Input data The genome sequences being compared are those of the \u2018wildtype\u2019 and \u2018mutant\u2019 strains. The relevant files should already be available on Galaxy (from the previous Assembly with Spades tutorial). Just for a recap: We have a closed, annotated genome sequence for the wildtype strain. This file has two formats (one with the sequence, and one with the features). Files: wildtype.fna and wildtype.gff For the mutant strain we have whole genome shotgun short sequence reads from an Illumina DNA sequencing instrument. Files: mutant_R1.fastq and mutant_R2.fastq (fastq format) The reads are paired-end Each read is 150 bases The reads coverage depth is estimated at 19x. Map reads to reference Map the reads on to the reference sequence Several programs could be used for this but we will use BWA-MEM. Go to Tools NGS Analysis NGS: Mapping Map with BWA-MEM . Set the following parameters: Will you select a reference genome from your history or use a built-in index? : Use a genome from history and build index Use the following dataset as the reference sequence : wildtype.fna Select first set of reads : mutant_R1.fastq Select second set of reads : mutant_R2.fastq Click Execute . Examine the mapped reads To do this, we will look at the contents of the BAM file. Hint The BAM file is a Binary Compressed Datafile and cannot be viewed directly. If you attempt to view it using the view data button (the eye icon) it will be downloaded to your local computer. Instead, we must convert it to a non-compressed text format (SAM) first. Go to Tools NGS Common Toolsets NGS: SAM tools BAM-to-SAM . BAM File to Convert : your BAM file Execute . View the resultant SAM file by clicking on the eye icon. Have a look at the fields in the file (the column headings). The demonstrator will now point out what all the fields are. (FIXME: or add some info here?) View the BAM file using Artemis In this section we will use Artemis to view the BAM file we produced above. Download the BAM file to your local computer. Click on the name of the BAM file that you created in Section 1. Click on the download button ; you need to download both the BAM file and the bam_index. (FIXME: why?) Also, download the annotated reference sequence; wildtype.gff . Start Artemis and load the wildtype.gff From the Artemis menu, Click File Open Load wildtype.gff . You should now have the wildtype\u2019s annotated sequence loaded into the Artemis genome browser. Load the BAM file into Artemis Click File Read BAM/VCF Select : Galaxy \u2026 .bam Click Ok FIXME: but don t load index? bai file? You should see something like this: Can you find a SNP? Demonstration of the ways that the view of the BAM file can be enhanced! Imagine finding each SNP manually - luckily this can be automated using a tool available on Galaxy. Variant Calling We will now search through our alignment file (BAM) for statistically-valid SNPs using the Freebayes variant calling tool. Run Freebayes Go to Tools NGS Analysis NGS: Variant Analysis FreeBayes Set the following parameters: Load reference genome from : History Sample BAM file : Map with BWA-MEM on data \u2026 BAM format Use the following dataset as the reference sequence : wildtype.fna Click Execute Examine the Freebayes output Freebayes will create a VCF file. This stands for Variant Calling Format. Click on its View Data button (eye icon) and have a look at the file. There is a lot of header information; the variants appear lower down. Can you spot a SNP? What about an insertion? A deletion? Investigation of Variants What is the impact of the differences we have observed? In this section we will use some simple strategies to predict the impact of the variant on the function of the gene and perhaps even the strain itself. Artemis - the annotated draft genome sequence of the mutant strain - what is the impact the protein coding region? what is the predicted function? blastp - http://blast.ncbi.nlm.nih.gov/Blast.cgi the protein domain display - are any major protein domains truncated by the difference? LipoP/SignalP/TmHMM - http://www.cbs.dtu.dk/services/ membrane location prediction - has the change had an impact on the membrane location of the protein? Literature? Can you suggest a type of nucleotide sequence that might have no impact on the function of the encoded protein? In this section we will investigate a few variants together as a demonstration perhaps a few individually too?? Detection of \u2018macro\u2019 INDELS and rearrangement using Mauve We will now examine our earlier assembly and compare it with the reference on a genome wide basis using Mauve. Download and install Mauve. More information on Mauve and its use can be found here . You will then need to load both the reference wildtype.gff file and the mutant.gff file that you downloaded earlier. FIXME: add more","title":"Comparative Genomics"},{"location":"modules/freebayes/#comparative-genomics","text":"","title":"Comparative Genomics"},{"location":"modules/freebayes/#background","text":"In this activity we will identify \u2018micro\u2019 differences between genome sequences using the BWA short read mapper and Freebayes variant caller. After investigating the \u2018micro\u2019 differences (SNPs/INDELS), we will attempt to detect larger \u2018macro\u2019 differences using Mauve.","title":"Background"},{"location":"modules/freebayes/#learning-objectives","text":"At the end of this tutorial you should be able to: map sequence reads to a reference genome view the mapped reads identify variants using Freebayes, and identify larger structural variants using Mauve.","title":"Learning objectives"},{"location":"modules/freebayes/#input-data","text":"The genome sequences being compared are those of the \u2018wildtype\u2019 and \u2018mutant\u2019 strains. The relevant files should already be available on Galaxy (from the previous Assembly with Spades tutorial). Just for a recap: We have a closed, annotated genome sequence for the wildtype strain. This file has two formats (one with the sequence, and one with the features). Files: wildtype.fna and wildtype.gff For the mutant strain we have whole genome shotgun short sequence reads from an Illumina DNA sequencing instrument. Files: mutant_R1.fastq and mutant_R2.fastq (fastq format) The reads are paired-end Each read is 150 bases The reads coverage depth is estimated at 19x.","title":"Input data"},{"location":"modules/freebayes/#map-reads-to-reference","text":"","title":"Map reads to reference"},{"location":"modules/freebayes/#map-the-reads-on-to-the-reference-sequence","text":"Several programs could be used for this but we will use BWA-MEM. Go to Tools NGS Analysis NGS: Mapping Map with BWA-MEM . Set the following parameters: Will you select a reference genome from your history or use a built-in index? : Use a genome from history and build index Use the following dataset as the reference sequence : wildtype.fna Select first set of reads : mutant_R1.fastq Select second set of reads : mutant_R2.fastq Click Execute .","title":"Map the reads on to the reference sequence"},{"location":"modules/freebayes/#examine-the-mapped-reads","text":"To do this, we will look at the contents of the BAM file. Hint The BAM file is a Binary Compressed Datafile and cannot be viewed directly. If you attempt to view it using the view data button (the eye icon) it will be downloaded to your local computer. Instead, we must convert it to a non-compressed text format (SAM) first. Go to Tools NGS Common Toolsets NGS: SAM tools BAM-to-SAM . BAM File to Convert : your BAM file Execute . View the resultant SAM file by clicking on the eye icon. Have a look at the fields in the file (the column headings). The demonstrator will now point out what all the fields are. (FIXME: or add some info here?)","title":"Examine the mapped reads"},{"location":"modules/freebayes/#view-the-bam-file-using-artemis","text":"In this section we will use Artemis to view the BAM file we produced above.","title":"View the BAM file using Artemis"},{"location":"modules/freebayes/#download-the-bam-file-to-your-local-computer","text":"Click on the name of the BAM file that you created in Section 1. Click on the download button ; you need to download both the BAM file and the bam_index. (FIXME: why?) Also, download the annotated reference sequence; wildtype.gff .","title":"Download the BAM file to your local computer."},{"location":"modules/freebayes/#start-artemis-and-load-the-wildtypegff","text":"From the Artemis menu, Click File Open Load wildtype.gff . You should now have the wildtype\u2019s annotated sequence loaded into the Artemis genome browser.","title":"Start Artemis and load the wildtype.gff"},{"location":"modules/freebayes/#load-the-bam-file-into-artemis","text":"Click File Read BAM/VCF Select : Galaxy \u2026 .bam Click Ok FIXME: but don t load index? bai file? You should see something like this: Can you find a SNP? Demonstration of the ways that the view of the BAM file can be enhanced! Imagine finding each SNP manually - luckily this can be automated using a tool available on Galaxy.","title":"Load the BAM file into Artemis"},{"location":"modules/freebayes/#variant-calling","text":"We will now search through our alignment file (BAM) for statistically-valid SNPs using the Freebayes variant calling tool.","title":"Variant Calling"},{"location":"modules/freebayes/#run-freebayes","text":"Go to Tools NGS Analysis NGS: Variant Analysis FreeBayes Set the following parameters: Load reference genome from : History Sample BAM file : Map with BWA-MEM on data \u2026 BAM format Use the following dataset as the reference sequence : wildtype.fna Click Execute","title":"Run Freebayes"},{"location":"modules/freebayes/#examine-the-freebayes-output","text":"Freebayes will create a VCF file. This stands for Variant Calling Format. Click on its View Data button (eye icon) and have a look at the file. There is a lot of header information; the variants appear lower down. Can you spot a SNP? What about an insertion? A deletion?","title":"Examine the Freebayes output"},{"location":"modules/freebayes/#investigation-of-variants","text":"What is the impact of the differences we have observed? In this section we will use some simple strategies to predict the impact of the variant on the function of the gene and perhaps even the strain itself. Artemis - the annotated draft genome sequence of the mutant strain - what is the impact the protein coding region? what is the predicted function? blastp - http://blast.ncbi.nlm.nih.gov/Blast.cgi the protein domain display - are any major protein domains truncated by the difference? LipoP/SignalP/TmHMM - http://www.cbs.dtu.dk/services/ membrane location prediction - has the change had an impact on the membrane location of the protein? Literature? Can you suggest a type of nucleotide sequence that might have no impact on the function of the encoded protein? In this section we will investigate a few variants together as a demonstration perhaps a few individually too??","title":"Investigation of Variants"},{"location":"modules/freebayes/#detection-of-macro-indels-and-rearrangement-using-mauve","text":"We will now examine our earlier assembly and compare it with the reference on a genome wide basis using Mauve. Download and install Mauve. More information on Mauve and its use can be found here . You will then need to load both the reference wildtype.gff file and the mutant.gff file that you downloaded earlier. FIXME: add more","title":"Detection of \u2018macro\u2019 INDELS and rearrangement using Mauve"},{"location":"modules/frogs/","text":"Metagenomics Metagenomics aims to compare microbial communities from different environments by using information from the metagenome. Typically, 16S rRNA is used when classifying taxa, and whole-genome sequencing when aiming to identify gene functions and pathways. This tutorial covers the tool called FROGS (in Galaxy): Find Rapidly OTU with Galaxy Solution . Get data The data: paired-end Illumina reads from two environmental samples. In Galaxy, in the history panel, click on the cog item, and select import from file . In the Archived History URL: box paste: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Microbial_tutorials/Galaxy-History-16S_metagenomics_dataset.tar.gz You should now have four files in your Galaxy history. In the Galaxy tools panel, in the top search bar, type FROGS. This will bring up the various tools available. We will be using 6 of these tools. 1. Pre-process This is the first step in the Frogs analysis. It takes in the forward and reverse sequencing reads (R1 and R2) from multiple samples (e.g. microbiome A and microbiome B) and performs the following steps: If the data is not in contigs, read1 and read2 will be overlapped (allowing some mismatch in overlapping region). Contigs that are too big or too small will be filtered out. If using the Illumina standard protocol: will look for those primers, filter out contigs without primers, and cut the primer sequences. Sequences that are too small or of poor quality will be filtered out. Sequences will be de-replicated: duplicates will be removed but the number of duplicates will be recorded. Run Go to FROGS Pre-process and select the following: Sequencer : Illumina Input type : Files by samples Reads alread contiged? : No Samples : Name : microbiomeA note: no spaces Reads1 : microbiomeA_R1.fq Reads2 : microbiomeA_R2.fq Insert Samples - click plus sign Name : microbiomeB Reads1 : microbiomeB_R1.fq Reads2 : microbiomeB_R2.fq Reads 1 size : 250 Reads 2 size : 250 Expected amplicon size : 420 Minimum amplicon size : 380 Maximum amplicon size : 460 Sequencing protocol : Illumina standard 5 primer : GGCGVACGGGTGAGTAA 3 primer : GTGCCAGCNGCNGCGG note: needs to be in 5 to 3 orientation Execute Output There are three output files. Click on the eye icon for each to see their contents. Report: The report.html shows how samples were filtered. For example, in the bar chart called Filtering summary , the number of reads kept at each filtering stage is displayed. We would expect some decrease in some of these categories, but if many reads have been filtered at a particular stage it could indicate poor quality data. Below this chart, a table shows the statistics for each sample. Check the boxes next to microbiomeA and microbiomeB and then click Display amplicon lengths - shows amplicon distribution before filtering. Click Display preprocessed amplicon lengths - shows the distribution has been narrowed after filtering. Check if both samples are similar. Counts: The counts.tsv file is a list of sequences and their counts in the two samples. Fasta file: The dereplicated.fasta contains the sequences, without any duplicates. 2. Clustering swarm In this step, sequences are clustered into groups using Swarm (more information about Swarm ). This takes the pre-processed fasta and counts files and does the following: Sorts reads by abundance. Clusters the reads into pre-clusters using Swarm and distance parameter of 1. Sorts these pre-clusters by abundance. Cluster the pre-clusters using Swarm and a user-specified distance. Go to FROGS Clustering swarm and select the following: Sequences files : dereplicated.fasta Counts file : count.tsv Aggregation distance : 3 Perform deionising clustering step? : Yes Execute Note: Galaxy may say that the job submission has failed. Click Close, then click refresh in the top of the history panel. The job should be running. Output There are three output files. The abundance.biom shows the abundance of each cluster. The seed_sequences.fasta contains the the cluster (OTU) representative sequences. The swarms.composition.tsv shows what is in each cluster. 3. Remove chimera Closely-related sequences may form chimeras (mixed sequences) during PCR (libray prep). This step removes these sequences by the following method: Splits input data into samples Uses vsearch to find chimeras in each sample Removes chimeras Go to FROGS Remove chimera and select the following: Sequences file : seed_sequences.fasta Abundance type : BIOM file Abundance file : abundance.biom Execute Note: Galaxy may say that the job submission has failed. Click Close, then click refresh in the top of the history panel. The job should be running. Output There are three output files. The non_chimera.fasta is a filtered file containing no chimeras. The non_chimera_abundance.biom is a filtered abundance file containing no chimeras. The summary report.html . In this case, although almost 70% of the clusters were removed, more than 70% of the actual reads were kept. 4. Filters The OTUs (Operational Taxonomic Units) have now been clustered. In this step, we will filter out some of the OTUs that are either not in both samples, and/or contain at least 0.005% of all the sequences. Go to FROGS Filters and select the following: Sequences file : non_chimera.fasta Abundance file : non_chimera_abundance.biom *** THE FILTERS ON OTUS IN SAMPLES, OTUS SIZE and SEQUENCE PERCENTAGE : Apply filters Minimum number of samples : 2 Minimum proportion/number of sequences to keep OTU : 0.00005 N biggest OTU : leave blank *** THE FILTERS ON RDP : No filters *** THE FILTERS ON BLAST : No filters *** THE FILTERS ON CONTAMINATIONS : No filters Execute ] Note: Galaxy may say that the job submission has failed. Click Close, then click refresh in the top of the history panel. The job should be running. Output There are four output files. sequences.fasta : the retained sequences. abudance.biom : the abundance file. excluded.tsv : discarded OTUs report.html : this shows the proportion of OTUs and the proportion of sequences kept. In this example, although most OTUs have been filtered out, most sequences have been retained. 5. Affiliation OTU An OTU is a cluster of sequences. This step adds the taxonomy to the abundance file. It uses the SILVA database for rRNA. Go to FROGS Affiliation OTU and select: Using reference database : silva123 Also perform RDP assignation : No OTU seed sequence : sequences.fasta from step 4 Abundance file : abundance.biom from step 4 Execute Note: Galaxy may say that the job submission has failed. Click Close, then click refresh in the top of the history panel. The job should be running. Output There are two output files. affiliation.biom : the abundance file with affiliation. Note: this .biom file is not human-readable. If you wish, you can convert it with the FROGS biom to tsv tool. report.html : the report shows the proportion of OTUs and sequences affiliated to sequences in the database. Here we can see that almost all OTUs and sequences have been taxonomically assigned, but almost 10% of OTUs could be assigned to multiple species. 6. Affiliations stat This step computes some statistics from the analysis and generates a report of the OTUs/taxonomy found. Go to FROGS Affiliations stat and select: Abundance file : affiliation.biom from step 5 Rarefaction ranks : Class Order Family Genus Species Affiliation processed : FROGS blast Execute Output There is one output file. Click on the eye icon next to summary.html : click on Display global distribution : this shows the taxonomy of the sample. Start at the centre; major groups each have a segment by colour; as you go outwards, taxonomic identification becomes more specific. Click on a segment to display a table of statisics below. Click on the cross in the top right corner to exit. Next, we will look at the rarefaction curve, which is a measure of samples vs diversity. Click on the boxes next to each of the samples (both, or one at a time) in the table (microbiomeA and microbiomeB). Click Display rarefaction . Links FROGS slides by Yvan Le Bras https://f1000research.com/slides/5-1832 The SILVA database: https://www.arb-silva.de/ Other metagenomics software: QIIME: http://qiime.org/ UPARSE: http://www.drive5.com/uparse/ MOTHUR: https://www.mothur.org/ MG-RAST: http://metagenomics.anl.gov/","title":"Home"},{"location":"modules/frogs/#metagenomics","text":"Metagenomics aims to compare microbial communities from different environments by using information from the metagenome. Typically, 16S rRNA is used when classifying taxa, and whole-genome sequencing when aiming to identify gene functions and pathways. This tutorial covers the tool called FROGS (in Galaxy): Find Rapidly OTU with Galaxy Solution .","title":"Metagenomics"},{"location":"modules/frogs/#get-data","text":"The data: paired-end Illumina reads from two environmental samples. In Galaxy, in the history panel, click on the cog item, and select import from file . In the Archived History URL: box paste: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Microbial_tutorials/Galaxy-History-16S_metagenomics_dataset.tar.gz You should now have four files in your Galaxy history. In the Galaxy tools panel, in the top search bar, type FROGS. This will bring up the various tools available. We will be using 6 of these tools.","title":"Get data"},{"location":"modules/frogs/#1-pre-process","text":"This is the first step in the Frogs analysis. It takes in the forward and reverse sequencing reads (R1 and R2) from multiple samples (e.g. microbiome A and microbiome B) and performs the following steps: If the data is not in contigs, read1 and read2 will be overlapped (allowing some mismatch in overlapping region). Contigs that are too big or too small will be filtered out. If using the Illumina standard protocol: will look for those primers, filter out contigs without primers, and cut the primer sequences. Sequences that are too small or of poor quality will be filtered out. Sequences will be de-replicated: duplicates will be removed but the number of duplicates will be recorded.","title":"1. Pre-process"},{"location":"modules/frogs/#run","text":"Go to FROGS Pre-process and select the following: Sequencer : Illumina Input type : Files by samples Reads alread contiged? : No Samples : Name : microbiomeA note: no spaces Reads1 : microbiomeA_R1.fq Reads2 : microbiomeA_R2.fq Insert Samples - click plus sign Name : microbiomeB Reads1 : microbiomeB_R1.fq Reads2 : microbiomeB_R2.fq Reads 1 size : 250 Reads 2 size : 250 Expected amplicon size : 420 Minimum amplicon size : 380 Maximum amplicon size : 460 Sequencing protocol : Illumina standard 5 primer : GGCGVACGGGTGAGTAA 3 primer : GTGCCAGCNGCNGCGG note: needs to be in 5 to 3 orientation Execute","title":"Run"},{"location":"modules/frogs/#output","text":"There are three output files. Click on the eye icon for each to see their contents. Report: The report.html shows how samples were filtered. For example, in the bar chart called Filtering summary , the number of reads kept at each filtering stage is displayed. We would expect some decrease in some of these categories, but if many reads have been filtered at a particular stage it could indicate poor quality data. Below this chart, a table shows the statistics for each sample. Check the boxes next to microbiomeA and microbiomeB and then click Display amplicon lengths - shows amplicon distribution before filtering. Click Display preprocessed amplicon lengths - shows the distribution has been narrowed after filtering. Check if both samples are similar. Counts: The counts.tsv file is a list of sequences and their counts in the two samples. Fasta file: The dereplicated.fasta contains the sequences, without any duplicates.","title":"Output"},{"location":"modules/frogs/#2-clustering-swarm","text":"In this step, sequences are clustered into groups using Swarm (more information about Swarm ). This takes the pre-processed fasta and counts files and does the following: Sorts reads by abundance. Clusters the reads into pre-clusters using Swarm and distance parameter of 1. Sorts these pre-clusters by abundance. Cluster the pre-clusters using Swarm and a user-specified distance. Go to FROGS Clustering swarm and select the following: Sequences files : dereplicated.fasta Counts file : count.tsv Aggregation distance : 3 Perform deionising clustering step? : Yes Execute Note: Galaxy may say that the job submission has failed. Click Close, then click refresh in the top of the history panel. The job should be running.","title":"2. Clustering swarm"},{"location":"modules/frogs/#output_1","text":"There are three output files. The abundance.biom shows the abundance of each cluster. The seed_sequences.fasta contains the the cluster (OTU) representative sequences. The swarms.composition.tsv shows what is in each cluster.","title":"Output"},{"location":"modules/frogs/#3-remove-chimera","text":"Closely-related sequences may form chimeras (mixed sequences) during PCR (libray prep). This step removes these sequences by the following method: Splits input data into samples Uses vsearch to find chimeras in each sample Removes chimeras Go to FROGS Remove chimera and select the following: Sequences file : seed_sequences.fasta Abundance type : BIOM file Abundance file : abundance.biom Execute Note: Galaxy may say that the job submission has failed. Click Close, then click refresh in the top of the history panel. The job should be running.","title":"3. Remove chimera"},{"location":"modules/frogs/#output_2","text":"There are three output files. The non_chimera.fasta is a filtered file containing no chimeras. The non_chimera_abundance.biom is a filtered abundance file containing no chimeras. The summary report.html . In this case, although almost 70% of the clusters were removed, more than 70% of the actual reads were kept.","title":"Output"},{"location":"modules/frogs/#4-filters","text":"The OTUs (Operational Taxonomic Units) have now been clustered. In this step, we will filter out some of the OTUs that are either not in both samples, and/or contain at least 0.005% of all the sequences. Go to FROGS Filters and select the following: Sequences file : non_chimera.fasta Abundance file : non_chimera_abundance.biom *** THE FILTERS ON OTUS IN SAMPLES, OTUS SIZE and SEQUENCE PERCENTAGE : Apply filters Minimum number of samples : 2 Minimum proportion/number of sequences to keep OTU : 0.00005 N biggest OTU : leave blank *** THE FILTERS ON RDP : No filters *** THE FILTERS ON BLAST : No filters *** THE FILTERS ON CONTAMINATIONS : No filters Execute ] Note: Galaxy may say that the job submission has failed. Click Close, then click refresh in the top of the history panel. The job should be running.","title":"4. Filters"},{"location":"modules/frogs/#output_3","text":"There are four output files. sequences.fasta : the retained sequences. abudance.biom : the abundance file. excluded.tsv : discarded OTUs report.html : this shows the proportion of OTUs and the proportion of sequences kept. In this example, although most OTUs have been filtered out, most sequences have been retained.","title":"Output"},{"location":"modules/frogs/#5-affiliation-otu","text":"An OTU is a cluster of sequences. This step adds the taxonomy to the abundance file. It uses the SILVA database for rRNA. Go to FROGS Affiliation OTU and select: Using reference database : silva123 Also perform RDP assignation : No OTU seed sequence : sequences.fasta from step 4 Abundance file : abundance.biom from step 4 Execute Note: Galaxy may say that the job submission has failed. Click Close, then click refresh in the top of the history panel. The job should be running.","title":"5. Affiliation OTU"},{"location":"modules/frogs/#output_4","text":"There are two output files. affiliation.biom : the abundance file with affiliation. Note: this .biom file is not human-readable. If you wish, you can convert it with the FROGS biom to tsv tool. report.html : the report shows the proportion of OTUs and sequences affiliated to sequences in the database. Here we can see that almost all OTUs and sequences have been taxonomically assigned, but almost 10% of OTUs could be assigned to multiple species.","title":"Output"},{"location":"modules/frogs/#6-affiliations-stat","text":"This step computes some statistics from the analysis and generates a report of the OTUs/taxonomy found. Go to FROGS Affiliations stat and select: Abundance file : affiliation.biom from step 5 Rarefaction ranks : Class Order Family Genus Species Affiliation processed : FROGS blast Execute","title":"6. Affiliations stat"},{"location":"modules/frogs/#output_5","text":"There is one output file. Click on the eye icon next to summary.html : click on Display global distribution : this shows the taxonomy of the sample. Start at the centre; major groups each have a segment by colour; as you go outwards, taxonomic identification becomes more specific. Click on a segment to display a table of statisics below. Click on the cross in the top right corner to exit. Next, we will look at the rarefaction curve, which is a measure of samples vs diversity. Click on the boxes next to each of the samples (both, or one at a time) in the table (microbiomeA and microbiomeB). Click Display rarefaction .","title":"Output"},{"location":"modules/frogs/#links","text":"FROGS slides by Yvan Le Bras https://f1000research.com/slides/5-1832 The SILVA database: https://www.arb-silva.de/ Other metagenomics software: QIIME: http://qiime.org/ UPARSE: http://www.drive5.com/uparse/ MOTHUR: https://www.mothur.org/ MG-RAST: http://metagenomics.anl.gov/","title":"Links"},{"location":"modules/galaxy/","text":"What is Galaxy Australia? What is Galaxy? Galaxy is a web-based analysis and workflow platform. Designed for biologists Easily accessible via a web page Free to use You can upload your own data You can access shared data Use common bioinformatics tools Develop workflows What does Galaxy look like? Tools on the left Viewing panel in the middle History of analysis and files on the right Can I use Galaxy? Yes! Galaxy is free to use and available for everyone. Australian researchers may find Galaxy Australia particularly suited to their analysis needs as it has been developed in consultation with scientists across the country. What is Galaxy Australia? Galaxy Australia has been developed for Australian researchers. It replaces the previous servers called Galaxy-Qld and Galaxy-Mel. Where are the other Galaxy servers? A list of Galaxy servers is here . What is on the other Galaxy servers? Some have a set of general tools, and some have been developed for particular analyses. Can I use more than one Galaxy server? Yes, but you will need to register separately for each one, and they don t share data between them. Log in Open your internet browser. Use Safari, Chrome or Firefox. Don t use Internet Explorer. Type in the address of Galaxy Australia: usegalaxy.org.au The first time you use Galaxy Australia, click on the Login or Register tab and select Register . Enter the details required and click Submit . Log in, and refresh the page. Each time you use this Galaxy server, log in to see your data and histories. If you are using a different Galaxy server, you would need to register/log in to that server. Use tools Available tools are in the left hand panel. Find the tool you want or use the search bar at the top of the tools. Click on the tool you want to use. The tool interface will appear in the centre Galaxy panel. Check the settings. Click Execute . When the tool has finished, output file(s) will appear at the top of your Current History in the right hand panel. Click on the eye icon next to a file to view it. To access older histories, use the button at the top right of the History panel. More information More information about the Galaxy project can be found here . What s next? To use the tutorials on this website: see the list in the left hand panel or, click the menu button (three horizontal bars) in the top left of the page You can find more tutorials at the Galaxy Training Network: http://galaxyproject.github.io/training-material/","title":"Get started"},{"location":"modules/galaxy/#what-is-galaxy-australia","text":"","title":"What is Galaxy Australia?"},{"location":"modules/galaxy/#what-is-galaxy","text":"Galaxy is a web-based analysis and workflow platform. Designed for biologists Easily accessible via a web page Free to use You can upload your own data You can access shared data Use common bioinformatics tools Develop workflows","title":"What is Galaxy?"},{"location":"modules/galaxy/#what-does-galaxy-look-like","text":"Tools on the left Viewing panel in the middle History of analysis and files on the right","title":"What does Galaxy look like?"},{"location":"modules/galaxy/#can-i-use-galaxy","text":"Yes! Galaxy is free to use and available for everyone. Australian researchers may find Galaxy Australia particularly suited to their analysis needs as it has been developed in consultation with scientists across the country.","title":"Can I use Galaxy?"},{"location":"modules/galaxy/#what-is-galaxy-australia_1","text":"Galaxy Australia has been developed for Australian researchers. It replaces the previous servers called Galaxy-Qld and Galaxy-Mel. Where are the other Galaxy servers? A list of Galaxy servers is here . What is on the other Galaxy servers? Some have a set of general tools, and some have been developed for particular analyses. Can I use more than one Galaxy server? Yes, but you will need to register separately for each one, and they don t share data between them.","title":"What is Galaxy Australia?"},{"location":"modules/galaxy/#log-in","text":"Open your internet browser. Use Safari, Chrome or Firefox. Don t use Internet Explorer. Type in the address of Galaxy Australia: usegalaxy.org.au The first time you use Galaxy Australia, click on the Login or Register tab and select Register . Enter the details required and click Submit . Log in, and refresh the page. Each time you use this Galaxy server, log in to see your data and histories. If you are using a different Galaxy server, you would need to register/log in to that server.","title":"Log in"},{"location":"modules/galaxy/#use-tools","text":"Available tools are in the left hand panel. Find the tool you want or use the search bar at the top of the tools. Click on the tool you want to use. The tool interface will appear in the centre Galaxy panel. Check the settings. Click Execute . When the tool has finished, output file(s) will appear at the top of your Current History in the right hand panel. Click on the eye icon next to a file to view it. To access older histories, use the button at the top right of the History panel.","title":"Use tools"},{"location":"modules/galaxy/#more-information","text":"More information about the Galaxy project can be found here .","title":"More information"},{"location":"modules/galaxy/#whats-next","text":"To use the tutorials on this website: see the list in the left hand panel or, click the menu button (three horizontal bars) in the top left of the page You can find more tutorials at the Galaxy Training Network: http://galaxyproject.github.io/training-material/","title":"What's next?"},{"location":"modules/galaxy-data/","text":"Data Download and Deletion from Galaxy Australia This tutorial outlines the process to get your data out of Galaxy and to delete it from Galaxy afterwards. This tutorial has been developed to assist users to conform to the Galaxy Australia data retention policy The policy states that Galaxy Australia is primarily an analysis environment and is not designed or maintained for long term data storage. With this in mind, Galaxy Australia has implemented a policy of user disk quotas and data storage times. They are as follows: User Category User Disk Quota User Data Retention Time Registered Australian Research Users* 600 GB 90 days from last activity Other registered users 100 GB 90 days from last activity Non-registered users 5 GB N/A as all data is deleted on browser session expiry * For details please see the Galaxy Australia data retention policy The tutorial is designed to show users how to download their data as: individual datasets from histories collections of datasets or multiple datasets from one history an entire history including it s metadata for import into another Galaxy server It also has instructions and explanation of Galaxy s two step deletion process (delete and then purge) for: individual datasets from histories entire histories At the end of this tutorial, the participate should have a greater knowledge of how Galaxy handles data downloads and deletions and be able to successfully download and delete their own data. This will also allow you to recover your disk quota and allow you to do more and new analyses if you have used it all! Downloading data from Galaxy There are multiple ways of downloading your data from Galaxy, the choice of which method to use depends on the amount of data you wish to download. You can download a single dataset, multiple datasets all the way to entire histories. Part 1: Download a dataset The easiest way to get your data is to download a single dataset from a history. Download a single dataset A history 1. In the History pane, Click on the name of the file you wish to download. This will expand the filename to show some details about the file. 2. Click the Download button . The dataset will now be downloaded to your computer Repeat this for any other data file you want in this or other histories. Part 2: Download multiple datasets Sometimes you may want to download multiple histories at once and doing them one by one can be a pain. Luckily, Galaxy has a mechanism for creating Collections . For more details about collections and how to create them, see this tutorial from the Galaxy Training Network. The concept is very simple however, we are just going to lump together our files of interest and download them all at once. Download multiple datasets Create a collection of the files you want. - Click on the Collections button at the top of the history pane (the little checkbox) - This creates some new history pane items: A checkbox next to each file name and some new menu buttons. - Now just select the files you want by clicking on their checkboxes, you can also select them all or select none of them. - Once you have selected everything you want, click the for all selected button and then click Build Dataset List - A new interface opens showing you the files you have selected. Give the list a name in the appropraite box and click Create - Click on the Collections button again (the little checkbox at the top of the history pane) to close the interface and go back to the normal history view. - You will now have a new item in your history, a collection of other files. Download the collection - Click on the collection name in the history pane to expand it - Click on the Download Collection button to download the collection of files as a compressed archive to your computer. That s it. You can now repeat the process for other files you want to download in other histories. Part 3: Downloading/exporting an entire history The third way to download data from Galaxy is by Exporting a history in it s entirety. This will allow you to download all of the data in a history as well as all of the metadata associated with it such as the tools used, their versions, the parameters chosen etc. It is downloaded as a single file archive. Once downloaded, you can also Import this history into another Galaxy server. Depending on the size of the files and the complexity of the history, it can take quite some time to create the archive before download. For example it can take over 24 hours to create an archive file of a 50GB history. It may be simpler just to download the individual files of interest as outlined above. Exporting a history - On the history pane, click on the history menu (the cog wheel), then select Export History to File - Galaxy will now start archiving the history and making it available for download. In the main Galaxy pane, a message will appear with a download URL. Once the history has been completely archived, the URL can be used to download the history archive file. - Click on the URL to start the download. If it isn t ready yet, it will show you the following message. - You will need to save the URL and try again later. As a guide to how long it can take to produce the archive for downloading: History size Time to archive 1 - 200 MB A few minutes 1 - 2 GB 1 - 2 hours 50 GB multiple days Deleting data from Galaxy Once you have downloaded everything that you want to keep from your Galaxy account, we strongly encourage you to delete your no longer required histories. Deletion in Galaxy is a two stage process. If you delete something, it actually only gets marked as deleted and then is hidden. To actually delete an object you must purge it from disk. The instructions for how to delete datasets and histories follow. Part 1: Deleting a single dataset This section will show you how to delete and then purge a single dataset from a history. You can also delete multiple datasets and then purge them all at once. Delete a single dataset - Click on the Delete button on the dataset you wish to delete - The dataset has now disappeared from the history pane and there is a 1 deleted link at the top. - You can see the dataset by clicking on the 1 deleted link at the top of the history pane. - This will allow you to either: Undelete it OR Permanently remove it from disk by clicking on the appropriate link. Note: Once you click on the permanently remove it from disk link you CANNOT undo it. You can also delete and purge multiple datasets at a time by clicking on the delete button on all the datasets you want to remove and then purge them all at once by going to the history menu (the little cog wheel) and selecting Purge Deleted Datasets Part 2: Deleting an entire history You can delete an entire history and purge it from disk by doing the following: Delete an entire history - Open the Saved Histories display by clicking on the history menu (little cog wheel) and selecting Saved Histories - You will now see a list of all of your histories - You can now delete a history by clicking on the grey button with it s name near or on the little down arrow. - Another little menu will appear. - Select Delete Permanently Note: This will permanently remove the entire history and all of its non shared datasets from disk. It CANNOT be undone. Your history has now been deleted and purged from disk.","title":"Download and Delete Data in Galaxy"},{"location":"modules/galaxy-data/#data-download-and-deletion-from-galaxy-australia","text":"This tutorial outlines the process to get your data out of Galaxy and to delete it from Galaxy afterwards. This tutorial has been developed to assist users to conform to the Galaxy Australia data retention policy The policy states that Galaxy Australia is primarily an analysis environment and is not designed or maintained for long term data storage. With this in mind, Galaxy Australia has implemented a policy of user disk quotas and data storage times. They are as follows: User Category User Disk Quota User Data Retention Time Registered Australian Research Users* 600 GB 90 days from last activity Other registered users 100 GB 90 days from last activity Non-registered users 5 GB N/A as all data is deleted on browser session expiry * For details please see the Galaxy Australia data retention policy The tutorial is designed to show users how to download their data as: individual datasets from histories collections of datasets or multiple datasets from one history an entire history including it s metadata for import into another Galaxy server It also has instructions and explanation of Galaxy s two step deletion process (delete and then purge) for: individual datasets from histories entire histories At the end of this tutorial, the participate should have a greater knowledge of how Galaxy handles data downloads and deletions and be able to successfully download and delete their own data. This will also allow you to recover your disk quota and allow you to do more and new analyses if you have used it all!","title":"Data Download and Deletion from Galaxy Australia"},{"location":"modules/galaxy-data/#downloading-data-from-galaxy","text":"There are multiple ways of downloading your data from Galaxy, the choice of which method to use depends on the amount of data you wish to download. You can download a single dataset, multiple datasets all the way to entire histories.","title":"Downloading data from Galaxy"},{"location":"modules/galaxy-data/#part-1-download-a-dataset","text":"The easiest way to get your data is to download a single dataset from a history. Download a single dataset A history 1. In the History pane, Click on the name of the file you wish to download. This will expand the filename to show some details about the file. 2. Click the Download button . The dataset will now be downloaded to your computer Repeat this for any other data file you want in this or other histories.","title":"Part 1: Download a dataset"},{"location":"modules/galaxy-data/#part-2-download-multiple-datasets","text":"Sometimes you may want to download multiple histories at once and doing them one by one can be a pain. Luckily, Galaxy has a mechanism for creating Collections . For more details about collections and how to create them, see this tutorial from the Galaxy Training Network. The concept is very simple however, we are just going to lump together our files of interest and download them all at once. Download multiple datasets Create a collection of the files you want. - Click on the Collections button at the top of the history pane (the little checkbox) - This creates some new history pane items: A checkbox next to each file name and some new menu buttons. - Now just select the files you want by clicking on their checkboxes, you can also select them all or select none of them. - Once you have selected everything you want, click the for all selected button and then click Build Dataset List - A new interface opens showing you the files you have selected. Give the list a name in the appropraite box and click Create - Click on the Collections button again (the little checkbox at the top of the history pane) to close the interface and go back to the normal history view. - You will now have a new item in your history, a collection of other files. Download the collection - Click on the collection name in the history pane to expand it - Click on the Download Collection button to download the collection of files as a compressed archive to your computer. That s it. You can now repeat the process for other files you want to download in other histories.","title":"Part 2: Download multiple datasets"},{"location":"modules/galaxy-data/#part-3-downloadingexporting-an-entire-history","text":"The third way to download data from Galaxy is by Exporting a history in it s entirety. This will allow you to download all of the data in a history as well as all of the metadata associated with it such as the tools used, their versions, the parameters chosen etc. It is downloaded as a single file archive. Once downloaded, you can also Import this history into another Galaxy server. Depending on the size of the files and the complexity of the history, it can take quite some time to create the archive before download. For example it can take over 24 hours to create an archive file of a 50GB history. It may be simpler just to download the individual files of interest as outlined above. Exporting a history - On the history pane, click on the history menu (the cog wheel), then select Export History to File - Galaxy will now start archiving the history and making it available for download. In the main Galaxy pane, a message will appear with a download URL. Once the history has been completely archived, the URL can be used to download the history archive file. - Click on the URL to start the download. If it isn t ready yet, it will show you the following message. - You will need to save the URL and try again later. As a guide to how long it can take to produce the archive for downloading: History size Time to archive 1 - 200 MB A few minutes 1 - 2 GB 1 - 2 hours 50 GB multiple days","title":"Part 3: Downloading/exporting an entire history"},{"location":"modules/galaxy-data/#deleting-data-from-galaxy","text":"Once you have downloaded everything that you want to keep from your Galaxy account, we strongly encourage you to delete your no longer required histories. Deletion in Galaxy is a two stage process. If you delete something, it actually only gets marked as deleted and then is hidden. To actually delete an object you must purge it from disk. The instructions for how to delete datasets and histories follow.","title":"Deleting data from Galaxy"},{"location":"modules/galaxy-data/#part-1-deleting-a-single-dataset","text":"This section will show you how to delete and then purge a single dataset from a history. You can also delete multiple datasets and then purge them all at once. Delete a single dataset - Click on the Delete button on the dataset you wish to delete - The dataset has now disappeared from the history pane and there is a 1 deleted link at the top. - You can see the dataset by clicking on the 1 deleted link at the top of the history pane. - This will allow you to either: Undelete it OR Permanently remove it from disk by clicking on the appropriate link. Note: Once you click on the permanently remove it from disk link you CANNOT undo it. You can also delete and purge multiple datasets at a time by clicking on the delete button on all the datasets you want to remove and then purge them all at once by going to the history menu (the little cog wheel) and selecting Purge Deleted Datasets","title":"Part 1: Deleting a single dataset"},{"location":"modules/galaxy-data/#part-2-deleting-an-entire-history","text":"You can delete an entire history and purge it from disk by doing the following: Delete an entire history - Open the Saved Histories display by clicking on the history menu (little cog wheel) and selecting Saved Histories - You will now see a list of all of your histories - You can now delete a history by clicking on the grey button with it s name near or on the little down arrow. - Another little menu will appear. - Select Delete Permanently Note: This will permanently remove the entire history and all of its non shared datasets from disk. It CANNOT be undone. Your history has now been deleted and purged from disk.","title":"Part 2: Deleting an entire history"},{"location":"modules/galaxy-long/","text":"Using Galaxy Background Galaxy is a web-based analysis and workflow platform designed for biologists to analyse their own data. It can be used to run a variety of bioinformatics tools. The selection of bioinformatics tools installed on the Galaxy instance we are using today caters for the analysis of bacterial genomics data sets. Bioinformatics tools can be added from the Galaxy \u2018tool shed\u2019 or removed as necessary from your Galaxy instance. Galaxy is an open, web-based platform. Details about the project can be found here . The Galaxy interface is separated into three parts. The Tools list on the left, the Viewing panel in the middle and the analysis and data History on the right. We will be looking at all three parts in this tutorial. This activity will familiarise you with the Galaxy interface. It will cover the following operations: Logging in to the server Putting data onto Galaxy Using some common tools Learning Objectives At the end of this tutorial you should be able to: Register and login to a Galaxy server. Upload data to a Galaxy server from: A file on your local computer. A file on a remote datastore with an accessible URL. Use tools in Galaxy by: Accessing the tool via the tool menu. Using the tool interface to run the particular tool. Viewing/accessing the tool output. Login to Galaxy Open a new tab or window on your web browser. Use Firefox or Chrome - please don\u2019t use Internet Explorer or Safari. In the address bar, type in the address of your galaxy server. Alternatively, you can access galaxy via the dashboard of your mGVL. Click on User button on the right and either register or login. If you haven t yet registered yet: Select: User Register Enter your email, choose a password, and choose a user name. Click Submit If you have already registered: Select: User Login Enter your username password. Click Submit Put data onto Galaxy There are two main ways to put your data onto Galaxy; this section will run through both ways. First, we need to make a new history. Make a new history First, make a new folder to store the work we are about to perform. Click on the history options button at the top of the History panel. Select Create New Click on Unnamed history to rename. Type in a new name. Datatypes What sort of file is being uploaded? We need to tell Galaxy what sort of file is being uploaded. Some common datatypes (file formats) are: text, FASTA, FASTQ, VCF, GFF, GBK, and tabular. Upload a file from your own computer With this method you can get most of the files on your own computer into Galaxy. First, download the following file to your computer: Copy this URL and paste it into the address bar in your web browser: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/galaxy101/Contig_stats.txt.gz This is a file stored on the cloud. Press Enter , and the file will download. Note the download location. Next, upload that file to Galaxy From the Galaxy tool panel, click on Get Data Upload File Click the Choose local file button Find and select the Contig_stats.txt.gz file you downloaded and click Open Set the Type to tabular Click the Start button Once the progress bar reaches 100%, click the Close button The file will now upload to your current history. Upload a file from a URL If a file exists on a web resource somewhere and you know its URL (Unique Resource Location - a web address) you can directly load it into Galaxy. From the tool panel, click on Get Data Upload File Click on the Paste/Fetch Data button Copy and paste the following web address into the URL/Text box: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_1.fastq.gz Set the Type to fastqsanger (CAREFUL: not fastqCsanger) Click Start Once the progress bar has reached 100%, click Close . Note that Galaxy is smart enough to recognize that this is a compressed file and so it will uncompress it as it loads it. Upload another file from a URL Now we are going to upload another file from the remote data source. Repeat the above for: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/MRSA0252.fna Note: this file Type is FASTA . The DNA sequence of Staphylococcus aureus MRSA252 will be loaded into your history as a FASTA file. Your History should now look like this: The data A brief description of each of the three files uploaded to Galaxy: Contigs_stats.txt this file contains a table of summary data from a de novo genome assembly (the process of attempting to recover the full genome of an organism from the short read sequences produced by most DNA sequencing machines). The columns contain a lot of information but the ones we will be using indicate the amount of data (or coverage) that went into making up each piece of the final assembly. bacterial_std_err_1.fastq.gz This file contains sequence reads, in the format produced by Illumina sequencing machines. Read more about the FASTQ format at Wikipedia. MRSA0252.fna This file contains the genome sequence of Staphylococcus aureus MRSA252 . Read more about the FASTA format at Wikipedia. Galaxy tools The purpose of this section is to help you become familiar with the way tools are run on Galaxy. We will see how to: rename files summarize assembly statistics convert file formats, and find features in a DNA sequence. Rename files Two of the files in the History have very long and confusing names. File names can be changed by taking the following steps: Click on the edit icon next to the file in the History called: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_1.fastq In the Name text box, give it a new name. Rename it to: typical.fastq Click the Save button. Repeat the process with another file: Find the file called: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/MRSA0252.fna Rename it to MRSA252.fna Much better. There is other functionality hidden behind that edit icon You can change a file\u2019s datatype, convert its format and many other things. Feel free to play around with them at a later date. Summarize assembly statistics We are going to produce a histogram of contig read-coverage depths and calculate the summary statistics from the Contig_stats.txt file. To do this we need to make some changes to the Contig_stats.txt file: cut out a couple of columns from the file remove a line from the file produce a histogram Cut out and keep two columns Click on the eye icon of the Contig_stats.txt file to have a look at it. Note that there are 18 columns in this file. We want column 1 and column 6. Go to Tools Text Manipulation Cut and set the following: Set Cut Columns to: c1,c6 Delimited by : Tab From : Contig_stats.txt Click Execute Examine the new file by clicking on its eye icon . We now have 2 columns instead of the 18 in the original file. Remove the Header lines of the new file Go to Tools Text Manipulation Remove beginning and set the following: Remove First : 1 from : Cut on data1 click Execute Note the the new file is the same as the previous one without the header line. Make a histogram Go to Tools Graph/Display Data Histogram and set the following: Dataset : Remove beginning on Data 4 Numerical column for X axis Column: 2 Number of breaks : 25 Plot title : Histogram of Contig Coverage Label for X axis : Coverage depth Click Execute Click on the eye icon of the histogram to have a look at it. Note there are a few peaks. Maybe these correspond to single, double and triple copy number of these contigs. Calculate summary statistics for contig coverage depth Go to Tools Statistics and Visualisation Statistics Summary Statisitics and set the following: Summary statistics on : Remove beginning on Data 4 Column or expression : c2 Click Execute You\u2019ll note that the summary statistics tool failed (red background in the History ). There was an error! If you click on the filename, and then the bug symbol , it will tell you what went wrong. (There is a missing python library). At this point, you would normally contact your Galaxy server administrator. Convert file formats This shows how to convert a FASTQ file to a FASTA file. The tool creates a new file with the converted data. Go to Tools Basic Tools Convert Formats FASTQ to FASTA and set the following: FASTQ file to convert : typical.fastq Click Execute The output is a new Fasta file called FASTQ to FASTA on data 2 . Find features This example shows how to use a tool called \u201cbarrnap\u201d to search for rRNAs in a DNA sequence. Find all of the ribosomal RNAs in a sequence Go to Tools NGS Analysis NGS: Annotation barrnap and set the following: Fasta file : MRSA252.fna Click Execute The output is barrnap on data 3 It is a gff3 format file (general feature format version 3). Each line in the file describes a feature in the DNA sequence. Filter the annotations to get the 23S RNAs Make a file with only the 23S rRNA features Go to Tools Basic Tools Filter and Sort Select and set the following: Select lines from : (whatever you called the barrnap gff3 output) the pattern : 23S (this will look for all the lines in the file that contain \u201c23S\u201d) Click Execute Now you have a gff3 file with just the 23S annotations! What next? Remember how we started a new History at the beginning? If you want to see any of your old histories, click on the History options button at the top of the History panel and then select \u201cSaved Histories.\u201d This will give you a list of all the histories you have worked on in this Galaxy server. Upload a saved Galaxy history (a set of files) to use in these tutorials .","title":"Using Galaxy"},{"location":"modules/galaxy-long/#using-galaxy","text":"","title":"Using Galaxy"},{"location":"modules/galaxy-long/#background","text":"Galaxy is a web-based analysis and workflow platform designed for biologists to analyse their own data. It can be used to run a variety of bioinformatics tools. The selection of bioinformatics tools installed on the Galaxy instance we are using today caters for the analysis of bacterial genomics data sets. Bioinformatics tools can be added from the Galaxy \u2018tool shed\u2019 or removed as necessary from your Galaxy instance. Galaxy is an open, web-based platform. Details about the project can be found here . The Galaxy interface is separated into three parts. The Tools list on the left, the Viewing panel in the middle and the analysis and data History on the right. We will be looking at all three parts in this tutorial. This activity will familiarise you with the Galaxy interface. It will cover the following operations: Logging in to the server Putting data onto Galaxy Using some common tools","title":"Background"},{"location":"modules/galaxy-long/#learning-objectives","text":"At the end of this tutorial you should be able to: Register and login to a Galaxy server. Upload data to a Galaxy server from: A file on your local computer. A file on a remote datastore with an accessible URL. Use tools in Galaxy by: Accessing the tool via the tool menu. Using the tool interface to run the particular tool. Viewing/accessing the tool output.","title":"Learning Objectives"},{"location":"modules/galaxy-long/#login-to-galaxy","text":"Open a new tab or window on your web browser. Use Firefox or Chrome - please don\u2019t use Internet Explorer or Safari. In the address bar, type in the address of your galaxy server. Alternatively, you can access galaxy via the dashboard of your mGVL. Click on User button on the right and either register or login. If you haven t yet registered yet: Select: User Register Enter your email, choose a password, and choose a user name. Click Submit If you have already registered: Select: User Login Enter your username password. Click Submit","title":"Login to Galaxy"},{"location":"modules/galaxy-long/#put-data-onto-galaxy","text":"There are two main ways to put your data onto Galaxy; this section will run through both ways. First, we need to make a new history.","title":"Put data onto Galaxy"},{"location":"modules/galaxy-long/#make-a-new-history","text":"First, make a new folder to store the work we are about to perform. Click on the history options button at the top of the History panel. Select Create New Click on Unnamed history to rename. Type in a new name.","title":"Make a new history"},{"location":"modules/galaxy-long/#datatypes","text":"What sort of file is being uploaded? We need to tell Galaxy what sort of file is being uploaded. Some common datatypes (file formats) are: text, FASTA, FASTQ, VCF, GFF, GBK, and tabular.","title":"Datatypes"},{"location":"modules/galaxy-long/#upload-a-file-from-your-own-computer","text":"With this method you can get most of the files on your own computer into Galaxy.","title":"Upload a file from your own computer"},{"location":"modules/galaxy-long/#first-download-the-following-file-to-your-computer","text":"Copy this URL and paste it into the address bar in your web browser: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/galaxy101/Contig_stats.txt.gz This is a file stored on the cloud. Press Enter , and the file will download. Note the download location.","title":"First, download the following file to your computer:"},{"location":"modules/galaxy-long/#next-upload-that-file-to-galaxy","text":"From the Galaxy tool panel, click on Get Data Upload File Click the Choose local file button Find and select the Contig_stats.txt.gz file you downloaded and click Open Set the Type to tabular Click the Start button Once the progress bar reaches 100%, click the Close button The file will now upload to your current history.","title":"Next, upload that file to Galaxy"},{"location":"modules/galaxy-long/#upload-a-file-from-a-url","text":"If a file exists on a web resource somewhere and you know its URL (Unique Resource Location - a web address) you can directly load it into Galaxy. From the tool panel, click on Get Data Upload File Click on the Paste/Fetch Data button Copy and paste the following web address into the URL/Text box: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_1.fastq.gz Set the Type to fastqsanger (CAREFUL: not fastqCsanger) Click Start Once the progress bar has reached 100%, click Close . Note that Galaxy is smart enough to recognize that this is a compressed file and so it will uncompress it as it loads it.","title":"Upload a file from a URL"},{"location":"modules/galaxy-long/#upload-another-file-from-a-url","text":"Now we are going to upload another file from the remote data source. Repeat the above for: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/MRSA0252.fna Note: this file Type is FASTA . The DNA sequence of Staphylococcus aureus MRSA252 will be loaded into your history as a FASTA file. Your History should now look like this:","title":"Upload another file from a URL"},{"location":"modules/galaxy-long/#the-data","text":"A brief description of each of the three files uploaded to Galaxy: Contigs_stats.txt this file contains a table of summary data from a de novo genome assembly (the process of attempting to recover the full genome of an organism from the short read sequences produced by most DNA sequencing machines). The columns contain a lot of information but the ones we will be using indicate the amount of data (or coverage) that went into making up each piece of the final assembly. bacterial_std_err_1.fastq.gz This file contains sequence reads, in the format produced by Illumina sequencing machines. Read more about the FASTQ format at Wikipedia. MRSA0252.fna This file contains the genome sequence of Staphylococcus aureus MRSA252 . Read more about the FASTA format at Wikipedia.","title":"The data"},{"location":"modules/galaxy-long/#galaxy-tools","text":"The purpose of this section is to help you become familiar with the way tools are run on Galaxy. We will see how to: rename files summarize assembly statistics convert file formats, and find features in a DNA sequence.","title":"Galaxy tools"},{"location":"modules/galaxy-long/#rename-files","text":"Two of the files in the History have very long and confusing names. File names can be changed by taking the following steps: Click on the edit icon next to the file in the History called: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_1.fastq In the Name text box, give it a new name. Rename it to: typical.fastq Click the Save button. Repeat the process with another file: Find the file called: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/MRSA0252.fna Rename it to MRSA252.fna Much better. There is other functionality hidden behind that edit icon You can change a file\u2019s datatype, convert its format and many other things. Feel free to play around with them at a later date.","title":"Rename files"},{"location":"modules/galaxy-long/#summarize-assembly-statistics","text":"We are going to produce a histogram of contig read-coverage depths and calculate the summary statistics from the Contig_stats.txt file. To do this we need to make some changes to the Contig_stats.txt file: cut out a couple of columns from the file remove a line from the file produce a histogram","title":"Summarize assembly statistics"},{"location":"modules/galaxy-long/#cut-out-and-keep-two-columns","text":"Click on the eye icon of the Contig_stats.txt file to have a look at it. Note that there are 18 columns in this file. We want column 1 and column 6. Go to Tools Text Manipulation Cut and set the following: Set Cut Columns to: c1,c6 Delimited by : Tab From : Contig_stats.txt Click Execute Examine the new file by clicking on its eye icon . We now have 2 columns instead of the 18 in the original file.","title":"Cut out and keep two columns"},{"location":"modules/galaxy-long/#remove-the-header-lines-of-the-new-file","text":"Go to Tools Text Manipulation Remove beginning and set the following: Remove First : 1 from : Cut on data1 click Execute Note the the new file is the same as the previous one without the header line.","title":"Remove the Header lines of the new file"},{"location":"modules/galaxy-long/#make-a-histogram","text":"Go to Tools Graph/Display Data Histogram and set the following: Dataset : Remove beginning on Data 4 Numerical column for X axis Column: 2 Number of breaks : 25 Plot title : Histogram of Contig Coverage Label for X axis : Coverage depth Click Execute Click on the eye icon of the histogram to have a look at it. Note there are a few peaks. Maybe these correspond to single, double and triple copy number of these contigs.","title":"Make a histogram"},{"location":"modules/galaxy-long/#calculate-summary-statistics-for-contig-coverage-depth","text":"Go to Tools Statistics and Visualisation Statistics Summary Statisitics and set the following: Summary statistics on : Remove beginning on Data 4 Column or expression : c2 Click Execute You\u2019ll note that the summary statistics tool failed (red background in the History ). There was an error! If you click on the filename, and then the bug symbol , it will tell you what went wrong. (There is a missing python library). At this point, you would normally contact your Galaxy server administrator.","title":"Calculate summary statistics for contig coverage depth"},{"location":"modules/galaxy-long/#convert-file-formats","text":"This shows how to convert a FASTQ file to a FASTA file. The tool creates a new file with the converted data. Go to Tools Basic Tools Convert Formats FASTQ to FASTA and set the following: FASTQ file to convert : typical.fastq Click Execute The output is a new Fasta file called FASTQ to FASTA on data 2 .","title":"Convert file formats"},{"location":"modules/galaxy-long/#find-features","text":"This example shows how to use a tool called \u201cbarrnap\u201d to search for rRNAs in a DNA sequence.","title":"Find features"},{"location":"modules/galaxy-long/#find-all-of-the-ribosomal-rnas-in-a-sequence","text":"Go to Tools NGS Analysis NGS: Annotation barrnap and set the following: Fasta file : MRSA252.fna Click Execute The output is barrnap on data 3 It is a gff3 format file (general feature format version 3). Each line in the file describes a feature in the DNA sequence.","title":"Find all of the ribosomal RNAs in a sequence"},{"location":"modules/galaxy-long/#filter-the-annotations-to-get-the-23s-rnas","text":"Make a file with only the 23S rRNA features Go to Tools Basic Tools Filter and Sort Select and set the following: Select lines from : (whatever you called the barrnap gff3 output) the pattern : 23S (this will look for all the lines in the file that contain \u201c23S\u201d) Click Execute Now you have a gff3 file with just the 23S annotations!","title":"Filter the annotations to get the 23S RNAs"},{"location":"modules/galaxy-long/#what-next","text":"Remember how we started a new History at the beginning? If you want to see any of your old histories, click on the History options button at the top of the History panel and then select \u201cSaved Histories.\u201d This will give you a list of all the histories you have worked on in this Galaxy server. Upload a saved Galaxy history (a set of files) to use in these tutorials .","title":"What next?"},{"location":"modules/gubbins/","text":"Gubbins Gubbins takes a set of bacterial genomes, removes recombinant blocks, and infers a phylogeny. https://github.com/sanger-pathogens/gubbins In Galaxy: Input An alignment of whole genome sequences of bacteria Run Gubbins run gubbins. change output file type of .txt tree file to .nhx under the file click Viz, phyloviz","title":"Gubbins"},{"location":"modules/gubbins/#gubbins","text":"Gubbins takes a set of bacterial genomes, removes recombinant blocks, and infers a phylogeny. https://github.com/sanger-pathogens/gubbins In Galaxy:","title":"Gubbins"},{"location":"modules/gubbins/#input","text":"An alignment of whole genome sequences of bacteria","title":"Input"},{"location":"modules/gubbins/#run-gubbins","text":"run gubbins. change output file type of .txt tree file to .nhx under the file click Viz, phyloviz","title":"Run Gubbins"},{"location":"modules/intro/","text":"Learn key tasks This is a short introduction to the Galaxy user interface - the web page that you interact with. We will cover key tasks in Galaxy: uploading files, using tools, and viewing histories. New to Galaxy? First try the introduction What does Galaxy look like? Browse to your Galaxy instance and log in or register. Use Chrome, Safari or Firefox as your browser, not Internet Explorer. This is an image of Galaxy Australia, located at www.usegalaxy.org.au . The particular Galaxy server that you are using may look slightly different and have a different web address. For example, the main Galaxy server is www.usegalaxy.org . The Galaxy page is divided into three panels: Tools on the left Viewing panel in the middle History of analysis and files on the right The first time you use Galaxy, there will be no files in your history panel. Name your current history Your History is in the panel at the right. Go to the History panel Click on the history name ( Unnamed history ) Type in a new name, for example, My-Analysis Press Enter Upload a file Your Tools are in the panel at the left. Go to the Tools panel Click Get Data (at the top of the list) Click Upload File This brings up a box: Click Paste/Fetch data Paste in the address of a file: https://zenodo.org/record/582600/files/mutant_R1.fastq Then click Start . Then click Close . Your uploaded file is now in your current history. When the file has uploaded to Galaxy, it will turn green. Grey means waiting to run ; yellow means in progress . What is this file? Click on the eye icon next to the file name, to look at the file contents. The contents of the file will be displayed in the centre Galaxy panel. This file contains DNA sequencing reads from a bacteria, in FASTQ format: Click here for more information about FASTQ format. Use a tool Let s look at the quality of the reads in this file. In the tools panel search box, type in FastQC . Click on the tool FastQC This brings up a window in the centre of the screen. For Short read data from your current history select the FASTQ file that we uploaded. Leave the other parameters as they are. Click Execute . This tool will run and the two output files will appear at the top of your history panel. How do I choose the correct parameters for my analysis? Under the tool interface panel in Galaxy there will usually be a description of the tool function and options, and a link to the tool s documentation. When trying a tool, you can usually use most of the default settings, as these are typically (generally) appropriate. When doing your own analyses, it is definitely recommended that you read about the tool parameters in more detail in the documentation, and adjust to your data and analysis accordingly. View results We will look at the output file called FastQC on data 1: Webpage . Note that Galaxy has given this file a name according to both the tool (FastQC) and the data file ( data 1 ) that it used. The name data 1 means the data file (our FASTQ file) which was file number 1 in Galaxy s current history. Click on the eye icon next to the output file. The information is displayed in the centre panel. This tool has summarised information about all of the reads in our FASTQ file. What was the length of the reads in the input FASTQ file? Do these reads have higher quality scores in the centre or at the ends? Optional: Run another tool Let s run a tool to filter out lower-quality reads from our FASTQ file. In the tool panel search box, type in Filter by quality . Click on the tool Filter by quality Under Library to filter , Galaxy will probably have found your input FASTQ file. If not, select this file in the drop-down box. Under Quality cut-off value , type in 35. Under Percent of bases in sequence that must have quality equal to / higher than cut-off value , type in 80. Click Execute . After the tool has run, the output file will appear at the top of your History panel. This file will be called Filter by quality on data 1 . Remember that Galaxy has named this file according to the tool it used ( Filter by quality ) and the data file ( data 1 ). The actual numbers in front of the files in the history are not important. What are the results from this filtering tool? We could click on the eye icon to view the contents of this output file, but it will not be very informative - we will just see a list of reads. Instead, let s click on the output file name in the History panel. This expands the information about the file. We can see that 1786 low-quality reads were discarded. Optional: Re-run that tool with changed settings We have now decided that our input reads have to be filtered to an even higher standard. We will change the filter settings and re-run the tool. In the History panel, find the output file from the first time we ran the filter tool. This file is called Filter by quality on data 1 . Click on the icon with two arrows - this means run this tool again . This brings up the tool interface in the centre panel. Change the settings to something even stricter. For example, you might decide you want 80 percent of bases to have a quality of 36 or higher, instead of 35. Click Execute . View the results: Click on the output file name to expand the information. ( Note : not the eye icon.) How many reads were discarded under these new filtering conditions? You can re-run a tool many times with different settings. Each time you re-run the tool, the new output file will appear at the top of your current history. Create a new history Let s create a new history. In the History panel, click on the cog icon. Select Create New . Name your history, e.g. Next-analysis Press Enter This new history does not have any files in it yet. Look at all your histories Where is your first history, called my-analysis ? In the History panel, click on the View all histories icon. All your histories are displayed here. Drag a file into your new history: Click on the FASTQ file in my-analysis history Drag it into the Next-analysis history This makes a copy of the file in the new history Click Done , or, if that button is not showing, click Analyze Data in the menu panel across the top. Your main Galaxy window will now show the current history as Next-analysis , and it will have one file in it. You can go back into the View all histories page and Switch to a different history. Summary What have we learned? The Galaxy interface has tools on the left, viewing pane in the middle, and a history of your data analysis on the right. You can create a new history for each analysis. All your histories are saved. To get data into Galaxy, you can upload a file by pasting in a web address. There are other ways to get data into Galaxy (not covered in this tutorial): you can upload a file from your computer, and you can import an entire history. Choose a tool and change any settings for your analysis. Run the tool. The output files will be saved at the top of your history. View the output files by clicking on the eye icon. View all your histories and move files between them. Switch to a different history. Log out of your Galaxy server. When you log back in (to the same server), your histories will all be there. Conclusion Well done! You have completed the short introduction to Galaxy, where you named the history, uploaded a file, used a tool, and viewed results. Additional tutorials are available for a more in-depth introduction to Galaxy s features. See this history in Galaxy If you want to see this Galaxy history without performing the steps above: Log in to Galaxy Australia: https://usegalaxy.org.au/ Go to Shared Data Click Histories Click Completed-key-tasks-analysis Click Import (at the top right corner) The analysis should now be showing as your current history. What s next? To use the tutorials on this website: see the list in the left hand panel or, click the menu button (three horizontal bars) in the top left of the page You can find more tutorials at the Galaxy Training Network: http://galaxyproject.github.io/training-material/","title":"Learn key tasks"},{"location":"modules/intro/#learn-key-tasks","text":"This is a short introduction to the Galaxy user interface - the web page that you interact with. We will cover key tasks in Galaxy: uploading files, using tools, and viewing histories. New to Galaxy? First try the introduction","title":"Learn key tasks"},{"location":"modules/intro/#what-does-galaxy-look-like","text":"Browse to your Galaxy instance and log in or register. Use Chrome, Safari or Firefox as your browser, not Internet Explorer. This is an image of Galaxy Australia, located at www.usegalaxy.org.au . The particular Galaxy server that you are using may look slightly different and have a different web address. For example, the main Galaxy server is www.usegalaxy.org . The Galaxy page is divided into three panels: Tools on the left Viewing panel in the middle History of analysis and files on the right The first time you use Galaxy, there will be no files in your history panel.","title":"What does Galaxy look like?"},{"location":"modules/intro/#name-your-current-history","text":"Your History is in the panel at the right. Go to the History panel Click on the history name ( Unnamed history ) Type in a new name, for example, My-Analysis Press Enter","title":"Name your current history"},{"location":"modules/intro/#upload-a-file","text":"Your Tools are in the panel at the left. Go to the Tools panel Click Get Data (at the top of the list) Click Upload File This brings up a box: Click Paste/Fetch data Paste in the address of a file: https://zenodo.org/record/582600/files/mutant_R1.fastq Then click Start . Then click Close . Your uploaded file is now in your current history. When the file has uploaded to Galaxy, it will turn green. Grey means waiting to run ; yellow means in progress . What is this file? Click on the eye icon next to the file name, to look at the file contents. The contents of the file will be displayed in the centre Galaxy panel. This file contains DNA sequencing reads from a bacteria, in FASTQ format: Click here for more information about FASTQ format.","title":"Upload a file"},{"location":"modules/intro/#use-a-tool","text":"Let s look at the quality of the reads in this file. In the tools panel search box, type in FastQC . Click on the tool FastQC This brings up a window in the centre of the screen. For Short read data from your current history select the FASTQ file that we uploaded. Leave the other parameters as they are. Click Execute . This tool will run and the two output files will appear at the top of your history panel. How do I choose the correct parameters for my analysis? Under the tool interface panel in Galaxy there will usually be a description of the tool function and options, and a link to the tool s documentation. When trying a tool, you can usually use most of the default settings, as these are typically (generally) appropriate. When doing your own analyses, it is definitely recommended that you read about the tool parameters in more detail in the documentation, and adjust to your data and analysis accordingly.","title":"Use a tool"},{"location":"modules/intro/#view-results","text":"We will look at the output file called FastQC on data 1: Webpage . Note that Galaxy has given this file a name according to both the tool (FastQC) and the data file ( data 1 ) that it used. The name data 1 means the data file (our FASTQ file) which was file number 1 in Galaxy s current history. Click on the eye icon next to the output file. The information is displayed in the centre panel. This tool has summarised information about all of the reads in our FASTQ file. What was the length of the reads in the input FASTQ file? Do these reads have higher quality scores in the centre or at the ends?","title":"View results"},{"location":"modules/intro/#optional-run-another-tool","text":"Let s run a tool to filter out lower-quality reads from our FASTQ file. In the tool panel search box, type in Filter by quality . Click on the tool Filter by quality Under Library to filter , Galaxy will probably have found your input FASTQ file. If not, select this file in the drop-down box. Under Quality cut-off value , type in 35. Under Percent of bases in sequence that must have quality equal to / higher than cut-off value , type in 80. Click Execute . After the tool has run, the output file will appear at the top of your History panel. This file will be called Filter by quality on data 1 . Remember that Galaxy has named this file according to the tool it used ( Filter by quality ) and the data file ( data 1 ). The actual numbers in front of the files in the history are not important. What are the results from this filtering tool? We could click on the eye icon to view the contents of this output file, but it will not be very informative - we will just see a list of reads. Instead, let s click on the output file name in the History panel. This expands the information about the file. We can see that 1786 low-quality reads were discarded.","title":"Optional: Run another tool"},{"location":"modules/intro/#optional-re-run-that-tool-with-changed-settings","text":"We have now decided that our input reads have to be filtered to an even higher standard. We will change the filter settings and re-run the tool. In the History panel, find the output file from the first time we ran the filter tool. This file is called Filter by quality on data 1 . Click on the icon with two arrows - this means run this tool again . This brings up the tool interface in the centre panel. Change the settings to something even stricter. For example, you might decide you want 80 percent of bases to have a quality of 36 or higher, instead of 35. Click Execute . View the results: Click on the output file name to expand the information. ( Note : not the eye icon.) How many reads were discarded under these new filtering conditions? You can re-run a tool many times with different settings. Each time you re-run the tool, the new output file will appear at the top of your current history.","title":"Optional: Re-run that tool with changed settings"},{"location":"modules/intro/#create-a-new-history","text":"Let s create a new history. In the History panel, click on the cog icon. Select Create New . Name your history, e.g. Next-analysis Press Enter This new history does not have any files in it yet.","title":"Create a new history"},{"location":"modules/intro/#look-at-all-your-histories","text":"Where is your first history, called my-analysis ? In the History panel, click on the View all histories icon. All your histories are displayed here. Drag a file into your new history: Click on the FASTQ file in my-analysis history Drag it into the Next-analysis history This makes a copy of the file in the new history Click Done , or, if that button is not showing, click Analyze Data in the menu panel across the top. Your main Galaxy window will now show the current history as Next-analysis , and it will have one file in it. You can go back into the View all histories page and Switch to a different history.","title":"Look at all your histories"},{"location":"modules/intro/#summary","text":"What have we learned? The Galaxy interface has tools on the left, viewing pane in the middle, and a history of your data analysis on the right. You can create a new history for each analysis. All your histories are saved. To get data into Galaxy, you can upload a file by pasting in a web address. There are other ways to get data into Galaxy (not covered in this tutorial): you can upload a file from your computer, and you can import an entire history. Choose a tool and change any settings for your analysis. Run the tool. The output files will be saved at the top of your history. View the output files by clicking on the eye icon. View all your histories and move files between them. Switch to a different history. Log out of your Galaxy server. When you log back in (to the same server), your histories will all be there.","title":"Summary"},{"location":"modules/intro/#conclusion","text":"Well done! You have completed the short introduction to Galaxy, where you named the history, uploaded a file, used a tool, and viewed results. Additional tutorials are available for a more in-depth introduction to Galaxy s features.","title":"Conclusion"},{"location":"modules/intro/#see-this-history-in-galaxy","text":"If you want to see this Galaxy history without performing the steps above: Log in to Galaxy Australia: https://usegalaxy.org.au/ Go to Shared Data Click Histories Click Completed-key-tasks-analysis Click Import (at the top right corner) The analysis should now be showing as your current history.","title":"See this history in Galaxy"},{"location":"modules/intro/#whats-next","text":"To use the tutorials on this website: see the list in the left hand panel or, click the menu button (three horizontal bars) in the top left of the page You can find more tutorials at the Galaxy Training Network: http://galaxyproject.github.io/training-material/","title":"What's next?"},{"location":"modules/jbrowse/","text":"JBrowse Background JBrowse is a genome browser available through the Galaxy interface or as a stand-alone tool. http://jbrowse.org/ http://genome.cshlp.org/content/19/9/1630.full Today we will use JBrowse within Galaxy. A variety of file types can be viewed using JBrowse, including GFF3, BED, FASTA, BAM, VCF. structure: - top: a reference bar showing chromosome position - feature tracks based on genome annotations: - particular annotations: e.g. tRNA, non coding RNA ? - all annotations in a row? other things in jbrowse: - highlight with the highlight button - drop down arrow next to ref seq - can just show some aas, one strand etc - lots of options with bam drop down menu navigation: - dragging - use navigation buttons (left, right, zoom) - type in coordinates or feature name makefile: - transform datatypes go to Graph/Display Data JBrowse choose the ref sequence? only allows the fasta file? will track options then appear? chose wildtype.fna but: python: can t open file jbrowse.py : [Errno 2] No such file or directory Learning objectives At the end of this tutorial, you should be able to: download a bacterial genome sequence, and open the genome sequence in JBrowse and look at the sequence features. Download a bacterial genome We will download the sequence of Leptospira borgpetersenii serovar Hardjobovis Chromosome II from the NCBI website in GenBank format. Go to http://www.ncbi.nlm.nih.gov Select the \u201cTaxonomy\u201d database from the list on the left hand side. Enter \u201cLeptospira borgpetersenii\u201d in the search box and click Search . Click on the species name: Click on the species name again (at the top of the list): Next to Genome , click on the far right number 1 . Click on Genome Assembly and Annotation report : Then, for the first species listed, scroll to the far right of the table, see chromosome 2, and click the link to the annotated assembly number NC_008509.1 (not CP0003): Then, click Send choose Complete Record , Destination File , Format GenBank (full) click Create File . Note the download location (e.g. Downloads folder). Open the GenBank file in JBrowse Overview: 6-frame annotation annotated genome features are highlighted black lines are stop codons DNA view: 6-frame translation DNA sequence in the middle amino acid translations above and below Text summary: text summary of features Navigation: Go to the overview pane click on one annotated feature (highlighted in blue) it will be summarized on the top line ( selected feature ) the corresponding sequence will be highlighted in the DNA view pane the corresponding feature will be higlighted in the text summary pane now double click on the same annotated feature all three panes will be centred for this feature to move left or right, use the horizontal scroll bars under each pane to zoom, use the vertical scroll bars on the right What next Assemble a bacterial genome using Spades.","title":"JBrowse"},{"location":"modules/jbrowse/#jbrowse","text":"","title":"JBrowse"},{"location":"modules/jbrowse/#background","text":"JBrowse is a genome browser available through the Galaxy interface or as a stand-alone tool. http://jbrowse.org/ http://genome.cshlp.org/content/19/9/1630.full Today we will use JBrowse within Galaxy. A variety of file types can be viewed using JBrowse, including GFF3, BED, FASTA, BAM, VCF. structure: - top: a reference bar showing chromosome position - feature tracks based on genome annotations: - particular annotations: e.g. tRNA, non coding RNA ? - all annotations in a row? other things in jbrowse: - highlight with the highlight button - drop down arrow next to ref seq - can just show some aas, one strand etc - lots of options with bam drop down menu navigation: - dragging - use navigation buttons (left, right, zoom) - type in coordinates or feature name makefile: - transform datatypes go to Graph/Display Data JBrowse choose the ref sequence? only allows the fasta file? will track options then appear? chose wildtype.fna but: python: can t open file jbrowse.py : [Errno 2] No such file or directory","title":"Background"},{"location":"modules/jbrowse/#learning-objectives","text":"At the end of this tutorial, you should be able to: download a bacterial genome sequence, and open the genome sequence in JBrowse and look at the sequence features.","title":"Learning objectives"},{"location":"modules/jbrowse/#download-a-bacterial-genome","text":"We will download the sequence of Leptospira borgpetersenii serovar Hardjobovis Chromosome II from the NCBI website in GenBank format. Go to http://www.ncbi.nlm.nih.gov Select the \u201cTaxonomy\u201d database from the list on the left hand side. Enter \u201cLeptospira borgpetersenii\u201d in the search box and click Search . Click on the species name: Click on the species name again (at the top of the list): Next to Genome , click on the far right number 1 . Click on Genome Assembly and Annotation report : Then, for the first species listed, scroll to the far right of the table, see chromosome 2, and click the link to the annotated assembly number NC_008509.1 (not CP0003): Then, click Send choose Complete Record , Destination File , Format GenBank (full) click Create File . Note the download location (e.g. Downloads folder).","title":"Download a bacterial genome"},{"location":"modules/jbrowse/#open-the-genbank-file-in-jbrowse","text":"Overview: 6-frame annotation annotated genome features are highlighted black lines are stop codons DNA view: 6-frame translation DNA sequence in the middle amino acid translations above and below Text summary: text summary of features Navigation: Go to the overview pane click on one annotated feature (highlighted in blue) it will be summarized on the top line ( selected feature ) the corresponding sequence will be highlighted in the DNA view pane the corresponding feature will be higlighted in the text summary pane now double click on the same annotated feature all three panes will be centred for this feature to move left or right, use the horizontal scroll bars under each pane to zoom, use the vertical scroll bars on the right","title":"Open the GenBank file in JBrowse"},{"location":"modules/jbrowse/#what-next","text":"Assemble a bacterial genome using Spades.","title":"What next"},{"location":"modules/kallisto/","text":"DGE using kallisto This tutorial is about differential gene expression in bacteria, using tools on the command-line tools (kallisto) and the web (Degust). Background Differential Gene Expression (DGE) is the process of determining whether any genes were expressed at a different level between two conditions. For example, the conditions could be wildtype versus mutant, or the same sample from two growth conditions. Usually multiple biological replicates are done for each condition - these are needed to separate variation within the condition from that between the conditions. There are several ways to test for DGE. All involve these steps: map/align reads to transcripts count number of reads per transcript see if counts differ between conditions Some tools will combine two of these steps. For example, options include: Map reads to reference genome with BWA-MEM, count reads per transcript with HTSeq-count, examine DGE using voom/limma (within Galaxy or Degust). Pseudo-align reads to a reference transcriptome and count, using kallisto, then examine DGE using voom/limma (within Galaxy or Degust). Learning Objectives At the end of this tutorial you should be able to: (Pseudo-)align RNA-Seq data to a reference transcriptome and count: kallisto Perform statistical analysis to obtain a list of differentially expressed genes: Degust Visualize and interpret the results RNA-Seq reads A typical experiment will have 2 conditions each with 3 replicates, for a total of 6 samples. Our RNA-seq reads are from 6 samples in FASTQ format. We have single-end reads; so one file per sample. Data could also be paired-end reads, and there would be two files per sample. These have been reduced to 1% of their original size for this tutorial. The experiment used the bacteria E. coli grown in two conditions. Files labelled LB are the wildtype Files labelled MG have been exposed to 0.5% MG - alpha methyglucoside (a sugar solution). The files are from Study PRJNA194149 from EBI ENA . We are using 3 FASTQ files from the control set (SRR794833-835) and 3 FASTQ files from the experimental condition set (SRR794848-850). Get data Login to your GVL. e.g. : ssh your_username@gvl.genome.edu.au enter your password Get the files: wget https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Microbial_tutorials/Ecoli_kallisto_files.tar.gz Uncompress and extract the files: tar -zxvf Ecoli_kallisto_files.tar.gz Move into the new directory: cd Ecoli_kallisto_files You should have the following files: 6 x RNA-seq reads in fastq.gz 1 x reference transcriptome in .fasta 1 x table of features in .tsv The reference transcriptome and features table have been produced from a genbank file, using a custom python script . We need to count the number of RNA-seq reads (that exist as fragments) that match different transcripts in the genome, including those for protein-coding sequences (such as genes) and RNA sequences (such as tRNA and mRNA). Therefore, we need a subset of the whole genome - the reference transcriptome. Generate counts Kallisto will count the reads per transcript. Index the transcripts file kallisto index -i transcripts.idx Ecoli_transcripts.fasta transcripts.idx : the name of the output index file transcripts.ffn : the name of the input fasta file file Run kallisto for every read set First, run kallisto for the LB1.fastq.gz reads: kallisto quant -i transcripts.idx -o LB1 --single -l 500 -s 50 LB1.fastq.gz -o LB1: LB1 will be name of the output folder produced from this analysis single : single-end reads -l : estimated length of library fragments -s : estimated standard deviation of library fragments LB1.fastq.gz : input FASTQ file Repeat for every FASTQ file (LB2, LB3, MG1, MG2, MG3). Run as above, but change the name of the output folder, and the file name at the end. We then need to combine all the counts into one table. Extract required columns Each output folder includes an abundance.tsv file. For each of these files, extract the column of counts (column 4) and remove the table heading. cut -f4 -d $ \\t abundance.tsv | tail -n +2 LB1_headless.tsv This cuts column 4, then removes the header, and saves as LB1_headless.tsv . Then add column heading LB1 and saves as LB1.tsv : echo -e LB1 | cat - LB1_headless.tsv LB1.tsv Move LB1.tsv into the main analysis folder. Repeat with all the other abundance.tsv files from the kallisto analyses, giving the tables the correct LBx or MGx header. Paste together with features table paste LB1.tsv LB2.tsv LB3.tsv MG1.tsv MG2.tsv MG3.tsv Ecoli_features.tsv counts.tsv Examine the file: less counts.tsv There should be a column for every set of RNA-Seq reads, and then several columns of information including feature, name and description. Download the counts.tsv file to your local computer. Test for DGE Degust is a tool on the web that can analyse the counts files produced in the step above, to test for differential gene expression. (Degust can also display the results from DGE analyses performed elsewhere.) Upload counts file Go to the Degust web page . Click Get Started . Click on Choose File . Select the counts.tsv and click Open . Click Upload . A Configuation page will appear. For Name type DGE in E coli For Info columns select name For EC Number column select EC For Analyze server side leave box checked. For Min read count put 10 Click Add condition Under Replicates , select LB1, LB2, LB3. Click Add condition again Under Replicates , select MG1, MG2, MG3. Save changes View - this brings up the Degust viewing window. Overview of Degust sections Top black panel with Configure settings at right. Left: Conditions: LB (control) and MG (treatment). Left: Method selection for DGE. Top centre: Plots, with options at right. When either of the expression plots are selected, a heatmap appears below. A table of genes (or features); expression in treatment relative to control (Treatment column); and significance (FDR column). Analyze gene expression Under Method , make sure that Voom/Limma is selected. Click Apply . This runs Voom/Limma on the uploaded counts. MDS plot First, look at the MDS plot. This is a multidimensional scaling plot which represents the variation between samples. Ideally: All the LB samples would be close to each other All the MG samples would be close to each other The LB and MG groups would be far apart The x-axis is the dimension with the highest magnitude. The control/treatment samples should be split along this axis. Our LB samples are on the left and the MG samples are on the right, which means they are well separated on their major MDS dimension, which looks correct. Expression - MA plot Each dot shows the change in expression in one gene. The average expression (over both condition and treatment samples) is represented on the x-axis. Plot points should be symmetrical around the x-axis. We can see that many genes are expressed at a low level, and some are highly expressed. The fold change is represented on the y axis. If expression is significantly different between treatment and control, the dots are red. If not, they are blue. (In Degust, significant means FDR 0.05). At low levels of gene expression (low values of the x axis), fold changes are less likely to be significant. Click on the dot to see the gene name. Expression - Parallel Coordinates and heatmap Each line shows the change in expression in one gene, between control and treatment. Go to Options at the right. For FDR cut-off set at 0.001. This is a significance level (an adjusted p value). We will set it quite low in this example, to ensure we only examine key differences. Look at the Parallel Coordinates plot. There are two axes: Left: LB : Gene expression in the control samples. All values are set at zero. Right: MG Gene expression in the treatment samples, relative to expression in the control. The blocks of blue and red underneath the plot are called a heatmap. Each block is a gene. Click on a block to see its line in the plot above. Look at the row for the Treatment. Relative to the control, genes expressed more are red; genes expressed less are blue. Note: for an experiment with multiple treatments, the various treatment axes can be dragged to rearrange. There is no natural order (such as a time series). Table of genes name : names of genes. Note that gene names are sometimes specific to a species, or they may be only named as a locus ID (a chromosomal location specified in the genome annotation). FDR : False Discovery Rate. This is an adjusted p value to show the significance of the difference in gene expression between two conditions. Click on column headings to sort. By default, this table is sorted by FDR. LB and MG : log2(Fold Change) of gene expression. The default display is of fold change in the treatment (MG) relative to the control (LB). Therefore, values in the LB column are zero. This can be changed in the Options panel at the top right. In some cases, a large fold change will be meaningful but in others, even a small fold change can be important biologically. Kegg Pathway A pathway is a drawn network to show the interaction between molecules, including some or all of genes, proteins, RNAs, chemical reactions. Click on Kegg Pathway, and select Glycolysis . Genes in this pathway will be highlighted as you hover over them elsewhere in Degust (e.g., in the table). What next? To learn more about the differentially-expressed genes: Go to the NCBI website. Under All Databases , click on Gene Enter the gene name in the search bar; e.g. ptsG Click on the first result that matches the species (e.g. in this case, E. coli ). This provides information about the gene, and may also show further references (e.g. in this case, a link to the EcoGene resource). Next steps: Investigate the biochemical pathways involving the genes of interest. Links Kallisto paper Kallisto + sleuth paper . Sleuth can test for differential gene expression. Link to Voom paper","title":"Home"},{"location":"modules/kallisto/#dge-using-kallisto","text":"This tutorial is about differential gene expression in bacteria, using tools on the command-line tools (kallisto) and the web (Degust).","title":"DGE using kallisto"},{"location":"modules/kallisto/#background","text":"Differential Gene Expression (DGE) is the process of determining whether any genes were expressed at a different level between two conditions. For example, the conditions could be wildtype versus mutant, or the same sample from two growth conditions. Usually multiple biological replicates are done for each condition - these are needed to separate variation within the condition from that between the conditions. There are several ways to test for DGE. All involve these steps: map/align reads to transcripts count number of reads per transcript see if counts differ between conditions Some tools will combine two of these steps. For example, options include: Map reads to reference genome with BWA-MEM, count reads per transcript with HTSeq-count, examine DGE using voom/limma (within Galaxy or Degust). Pseudo-align reads to a reference transcriptome and count, using kallisto, then examine DGE using voom/limma (within Galaxy or Degust).","title":"Background"},{"location":"modules/kallisto/#learning-objectives","text":"At the end of this tutorial you should be able to: (Pseudo-)align RNA-Seq data to a reference transcriptome and count: kallisto Perform statistical analysis to obtain a list of differentially expressed genes: Degust Visualize and interpret the results","title":"Learning Objectives"},{"location":"modules/kallisto/#rna-seq-reads","text":"A typical experiment will have 2 conditions each with 3 replicates, for a total of 6 samples. Our RNA-seq reads are from 6 samples in FASTQ format. We have single-end reads; so one file per sample. Data could also be paired-end reads, and there would be two files per sample. These have been reduced to 1% of their original size for this tutorial. The experiment used the bacteria E. coli grown in two conditions. Files labelled LB are the wildtype Files labelled MG have been exposed to 0.5% MG - alpha methyglucoside (a sugar solution). The files are from Study PRJNA194149 from EBI ENA . We are using 3 FASTQ files from the control set (SRR794833-835) and 3 FASTQ files from the experimental condition set (SRR794848-850).","title":"RNA-Seq reads"},{"location":"modules/kallisto/#get-data","text":"Login to your GVL. e.g. : ssh your_username@gvl.genome.edu.au enter your password Get the files: wget https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Microbial_tutorials/Ecoli_kallisto_files.tar.gz Uncompress and extract the files: tar -zxvf Ecoli_kallisto_files.tar.gz Move into the new directory: cd Ecoli_kallisto_files You should have the following files: 6 x RNA-seq reads in fastq.gz 1 x reference transcriptome in .fasta 1 x table of features in .tsv The reference transcriptome and features table have been produced from a genbank file, using a custom python script . We need to count the number of RNA-seq reads (that exist as fragments) that match different transcripts in the genome, including those for protein-coding sequences (such as genes) and RNA sequences (such as tRNA and mRNA). Therefore, we need a subset of the whole genome - the reference transcriptome.","title":"Get data"},{"location":"modules/kallisto/#generate-counts","text":"Kallisto will count the reads per transcript.","title":"Generate counts"},{"location":"modules/kallisto/#index-the-transcripts-file","text":"kallisto index -i transcripts.idx Ecoli_transcripts.fasta transcripts.idx : the name of the output index file transcripts.ffn : the name of the input fasta file file","title":"Index the transcripts file"},{"location":"modules/kallisto/#run-kallisto-for-every-read-set","text":"First, run kallisto for the LB1.fastq.gz reads: kallisto quant -i transcripts.idx -o LB1 --single -l 500 -s 50 LB1.fastq.gz -o LB1: LB1 will be name of the output folder produced from this analysis single : single-end reads -l : estimated length of library fragments -s : estimated standard deviation of library fragments LB1.fastq.gz : input FASTQ file Repeat for every FASTQ file (LB2, LB3, MG1, MG2, MG3). Run as above, but change the name of the output folder, and the file name at the end. We then need to combine all the counts into one table.","title":"Run kallisto for every read set"},{"location":"modules/kallisto/#extract-required-columns","text":"Each output folder includes an abundance.tsv file. For each of these files, extract the column of counts (column 4) and remove the table heading. cut -f4 -d $ \\t abundance.tsv | tail -n +2 LB1_headless.tsv This cuts column 4, then removes the header, and saves as LB1_headless.tsv . Then add column heading LB1 and saves as LB1.tsv : echo -e LB1 | cat - LB1_headless.tsv LB1.tsv Move LB1.tsv into the main analysis folder. Repeat with all the other abundance.tsv files from the kallisto analyses, giving the tables the correct LBx or MGx header.","title":"Extract required columns"},{"location":"modules/kallisto/#paste-together-with-features-table","text":"paste LB1.tsv LB2.tsv LB3.tsv MG1.tsv MG2.tsv MG3.tsv Ecoli_features.tsv counts.tsv Examine the file: less counts.tsv There should be a column for every set of RNA-Seq reads, and then several columns of information including feature, name and description. Download the counts.tsv file to your local computer.","title":"Paste together with features table"},{"location":"modules/kallisto/#test-for-dge","text":"Degust is a tool on the web that can analyse the counts files produced in the step above, to test for differential gene expression. (Degust can also display the results from DGE analyses performed elsewhere.)","title":"Test for DGE"},{"location":"modules/kallisto/#upload-counts-file","text":"Go to the Degust web page . Click Get Started . Click on Choose File . Select the counts.tsv and click Open . Click Upload . A Configuation page will appear. For Name type DGE in E coli For Info columns select name For EC Number column select EC For Analyze server side leave box checked. For Min read count put 10 Click Add condition Under Replicates , select LB1, LB2, LB3. Click Add condition again Under Replicates , select MG1, MG2, MG3. Save changes View - this brings up the Degust viewing window.","title":"Upload counts file"},{"location":"modules/kallisto/#overview-of-degust-sections","text":"Top black panel with Configure settings at right. Left: Conditions: LB (control) and MG (treatment). Left: Method selection for DGE. Top centre: Plots, with options at right. When either of the expression plots are selected, a heatmap appears below. A table of genes (or features); expression in treatment relative to control (Treatment column); and significance (FDR column).","title":"Overview of Degust sections"},{"location":"modules/kallisto/#analyze-gene-expression","text":"Under Method , make sure that Voom/Limma is selected. Click Apply . This runs Voom/Limma on the uploaded counts.","title":"Analyze gene expression"},{"location":"modules/kallisto/#mds-plot","text":"First, look at the MDS plot. This is a multidimensional scaling plot which represents the variation between samples. Ideally: All the LB samples would be close to each other All the MG samples would be close to each other The LB and MG groups would be far apart The x-axis is the dimension with the highest magnitude. The control/treatment samples should be split along this axis. Our LB samples are on the left and the MG samples are on the right, which means they are well separated on their major MDS dimension, which looks correct.","title":"MDS plot"},{"location":"modules/kallisto/#expression-ma-plot","text":"Each dot shows the change in expression in one gene. The average expression (over both condition and treatment samples) is represented on the x-axis. Plot points should be symmetrical around the x-axis. We can see that many genes are expressed at a low level, and some are highly expressed. The fold change is represented on the y axis. If expression is significantly different between treatment and control, the dots are red. If not, they are blue. (In Degust, significant means FDR 0.05). At low levels of gene expression (low values of the x axis), fold changes are less likely to be significant. Click on the dot to see the gene name.","title":"Expression - MA plot"},{"location":"modules/kallisto/#expression-parallel-coordinates-and-heatmap","text":"Each line shows the change in expression in one gene, between control and treatment. Go to Options at the right. For FDR cut-off set at 0.001. This is a significance level (an adjusted p value). We will set it quite low in this example, to ensure we only examine key differences. Look at the Parallel Coordinates plot. There are two axes: Left: LB : Gene expression in the control samples. All values are set at zero. Right: MG Gene expression in the treatment samples, relative to expression in the control. The blocks of blue and red underneath the plot are called a heatmap. Each block is a gene. Click on a block to see its line in the plot above. Look at the row for the Treatment. Relative to the control, genes expressed more are red; genes expressed less are blue. Note: for an experiment with multiple treatments, the various treatment axes can be dragged to rearrange. There is no natural order (such as a time series).","title":"Expression - Parallel Coordinates and heatmap"},{"location":"modules/kallisto/#table-of-genes","text":"name : names of genes. Note that gene names are sometimes specific to a species, or they may be only named as a locus ID (a chromosomal location specified in the genome annotation). FDR : False Discovery Rate. This is an adjusted p value to show the significance of the difference in gene expression between two conditions. Click on column headings to sort. By default, this table is sorted by FDR. LB and MG : log2(Fold Change) of gene expression. The default display is of fold change in the treatment (MG) relative to the control (LB). Therefore, values in the LB column are zero. This can be changed in the Options panel at the top right. In some cases, a large fold change will be meaningful but in others, even a small fold change can be important biologically.","title":"Table of genes"},{"location":"modules/kallisto/#kegg-pathway","text":"A pathway is a drawn network to show the interaction between molecules, including some or all of genes, proteins, RNAs, chemical reactions. Click on Kegg Pathway, and select Glycolysis . Genes in this pathway will be highlighted as you hover over them elsewhere in Degust (e.g., in the table).","title":"Kegg Pathway"},{"location":"modules/kallisto/#what-next","text":"To learn more about the differentially-expressed genes: Go to the NCBI website. Under All Databases , click on Gene Enter the gene name in the search bar; e.g. ptsG Click on the first result that matches the species (e.g. in this case, E. coli ). This provides information about the gene, and may also show further references (e.g. in this case, a link to the EcoGene resource). Next steps: Investigate the biochemical pathways involving the genes of interest.","title":"What next?"},{"location":"modules/kallisto/#links","text":"Kallisto paper Kallisto + sleuth paper . Sleuth can test for differential gene expression. Link to Voom paper","title":"Links"},{"location":"modules/kraken/","text":"Sample identification with Kraken To identify a sample from sequencing reads, we can use the tool Kraken . This tool can also be used to identify members in a mixed set of reads, for metagenomics. e.g. reads from one sample Kraken 95% Staphylococcus aureus . e.g. mixed reads Kraken 50% Staphylococcus aureus , 40% Campylobacter concisus , 10% unclassified. In this tutorial we will use Kraken to confirm the identify of reads from a bacterial isolate. New to Galaxy? First try the introduction and then learn some key tasks Get data Log in to your Galaxy instance (for example, Galaxy Australia, usegalaxy.org.au ). Use shared data If you are using Galaxy Australia, you can import the data from a shared data library. In the top menu bar, go to Shared Data . Click on Data Libraries . Click on Galaxy Australia Training Material: Sample Identification . Tick the box next to the four files. Click the To History button, select As Datasets . Name a new history and click Import . In the top menu bar, click Analyze Data . You should now have four files in your current history. Or, import from the web Only follow this step if unable to load the data files from shared data, as described above . In a new browser tab, go to this webpage: Right click on a name: select copy link address In Galaxy, go to Get Data and then Upload File Click Paste/Fetch data A box will appear: paste in link address Click Start Click Close The file will now appear in the top of your history panel. Repeat this for all the files on Zenodo. Shorten file names Click on the pencil icon next to the file name. In the centre Galaxy panel, click in the box under Name Shorten the file name. Then click Save You should now have four files in your current history: Run Kraken We have a sample that should be Staphylococcus aureus . The paired-end FASTQ read files are: staph_R1.fq and staph_R2.fq . (We will look at the other set of files later on in the tutorial). Go to Tools search bar and type in Kraken . Click on the Kraken tool. Set the following parameters: Single or paired reads : Paired Forward strand: staph_R1.fq Reverse strand: staph_R2.fq Select a Kraken database: Minikraken leave other settings as they are Click Execute Examine the output The output is a file called Kraken on data x and x: Classification . This will be at the top of your history pane. Click Refresh if the file hasn t yet turned green. When the file is green, click on the eye icon to view. We will turn this output into something easier to read in the next step. Column 2 is the sequence ID. Column 3 is the taxon ID (from NCBI). Column 5 is a summary of all the taxon IDs that each k-mer in the sequence matched to (taxon ID:number of k-mers). Kraken report Go to Tools search bar and type in Kraken-report . Click on the Kraken-report tool. Set the following parameters: Kraken output : Kraken on data x and x: Classification Select a Kraken database : Minikraken Click Execute The output file is called Kraken-report on data x . Click on the eye icon to view. Column 1: percentage of reads in the clade/taxon in Column 6 Column 2: number of reads in the clade. Column 3: number of reads in the clade but not further classified. Column 4: code indicating the rank of the classification: (U)nclassified, (D)omain, (K)ingdom, (P)hylum, (C)lass, (O)rder, (F)amily, (G)enus, (S)pecies). Column 5: NCBI taxonomy ID. Approximately 95% of reads were classified as Staphylococcus aureus , confirming the correct identity of our bacterial sample. Of these reads, roughly half were uniquely present in S. aureus subsp. aureus , and most of those were uniquely present in strain HO 5096 0412. The sample strain is therefore most related to the HO 5096 0412 strain. The remaining reads within the S. aureus clade were classified into various taxa. Scroll down column 3 to see the number of reads assigned directly to the taxon in column 6. These are all very low and can be disregarded. Next Re-run Kraken with another sample. This sample should be Enterococcus faecalis . Use the files ent_R1.fq and ent_R2.fq . Run Kraken with these files. These are paired-end reads. With the Classification file from Kraken, run Kraken-report . Cick on the eye icon to view the Kraken-report file. 63% are classified to the genus Enterococcus , and most of these to E. faecalis . However, if we scroll down the table of results, we see that 31% are classified to the genus Mycobacterium , mostly M. abscessus . These are not in the same phylum as Enterococcus . This sample is probably contaminated. Links Kraken paper Kraken software See this history in Galaxy If you want to see this Galaxy history without performing the steps above: Log in to Galaxy Australia: https://usegalaxy.org.au/ Go to Shared Data Click Histories Click Completed-kraken-analysis Click Import (at the top right corner) The analysis should now be showing as your current history. What s next? To use the tutorials on this website: see the list in the left hand panel or, click the menu button (three horizontal bars) in the top left of the page You can find more tutorials at the Galaxy Training Network: http://galaxyproject.github.io/training-material/","title":"Sample identification"},{"location":"modules/kraken/#sample-identification-with-kraken","text":"To identify a sample from sequencing reads, we can use the tool Kraken . This tool can also be used to identify members in a mixed set of reads, for metagenomics. e.g. reads from one sample Kraken 95% Staphylococcus aureus . e.g. mixed reads Kraken 50% Staphylococcus aureus , 40% Campylobacter concisus , 10% unclassified. In this tutorial we will use Kraken to confirm the identify of reads from a bacterial isolate. New to Galaxy? First try the introduction and then learn some key tasks","title":"Sample identification with Kraken"},{"location":"modules/kraken/#get-data","text":"Log in to your Galaxy instance (for example, Galaxy Australia, usegalaxy.org.au ).","title":"Get data"},{"location":"modules/kraken/#use-shared-data","text":"If you are using Galaxy Australia, you can import the data from a shared data library. In the top menu bar, go to Shared Data . Click on Data Libraries . Click on Galaxy Australia Training Material: Sample Identification . Tick the box next to the four files. Click the To History button, select As Datasets . Name a new history and click Import . In the top menu bar, click Analyze Data . You should now have four files in your current history.","title":"Use shared data"},{"location":"modules/kraken/#or-import-from-the-web","text":"Only follow this step if unable to load the data files from shared data, as described above . In a new browser tab, go to this webpage: Right click on a name: select copy link address In Galaxy, go to Get Data and then Upload File Click Paste/Fetch data A box will appear: paste in link address Click Start Click Close The file will now appear in the top of your history panel. Repeat this for all the files on Zenodo.","title":"Or, import from the web"},{"location":"modules/kraken/#shorten-file-names","text":"Click on the pencil icon next to the file name. In the centre Galaxy panel, click in the box under Name Shorten the file name. Then click Save You should now have four files in your current history:","title":"Shorten file names"},{"location":"modules/kraken/#run-kraken","text":"We have a sample that should be Staphylococcus aureus . The paired-end FASTQ read files are: staph_R1.fq and staph_R2.fq . (We will look at the other set of files later on in the tutorial). Go to Tools search bar and type in Kraken . Click on the Kraken tool. Set the following parameters: Single or paired reads : Paired Forward strand: staph_R1.fq Reverse strand: staph_R2.fq Select a Kraken database: Minikraken leave other settings as they are Click Execute","title":"Run Kraken"},{"location":"modules/kraken/#examine-the-output","text":"The output is a file called Kraken on data x and x: Classification . This will be at the top of your history pane. Click Refresh if the file hasn t yet turned green. When the file is green, click on the eye icon to view. We will turn this output into something easier to read in the next step. Column 2 is the sequence ID. Column 3 is the taxon ID (from NCBI). Column 5 is a summary of all the taxon IDs that each k-mer in the sequence matched to (taxon ID:number of k-mers).","title":"Examine the output"},{"location":"modules/kraken/#kraken-report","text":"Go to Tools search bar and type in Kraken-report . Click on the Kraken-report tool. Set the following parameters: Kraken output : Kraken on data x and x: Classification Select a Kraken database : Minikraken Click Execute The output file is called Kraken-report on data x . Click on the eye icon to view. Column 1: percentage of reads in the clade/taxon in Column 6 Column 2: number of reads in the clade. Column 3: number of reads in the clade but not further classified. Column 4: code indicating the rank of the classification: (U)nclassified, (D)omain, (K)ingdom, (P)hylum, (C)lass, (O)rder, (F)amily, (G)enus, (S)pecies). Column 5: NCBI taxonomy ID. Approximately 95% of reads were classified as Staphylococcus aureus , confirming the correct identity of our bacterial sample. Of these reads, roughly half were uniquely present in S. aureus subsp. aureus , and most of those were uniquely present in strain HO 5096 0412. The sample strain is therefore most related to the HO 5096 0412 strain. The remaining reads within the S. aureus clade were classified into various taxa. Scroll down column 3 to see the number of reads assigned directly to the taxon in column 6. These are all very low and can be disregarded.","title":"Kraken report"},{"location":"modules/kraken/#next","text":"Re-run Kraken with another sample. This sample should be Enterococcus faecalis . Use the files ent_R1.fq and ent_R2.fq . Run Kraken with these files. These are paired-end reads. With the Classification file from Kraken, run Kraken-report . Cick on the eye icon to view the Kraken-report file. 63% are classified to the genus Enterococcus , and most of these to E. faecalis . However, if we scroll down the table of results, we see that 31% are classified to the genus Mycobacterium , mostly M. abscessus . These are not in the same phylum as Enterococcus . This sample is probably contaminated.","title":"Next"},{"location":"modules/kraken/#links","text":"Kraken paper Kraken software","title":"Links"},{"location":"modules/kraken/#see-this-history-in-galaxy","text":"If you want to see this Galaxy history without performing the steps above: Log in to Galaxy Australia: https://usegalaxy.org.au/ Go to Shared Data Click Histories Click Completed-kraken-analysis Click Import (at the top right corner) The analysis should now be showing as your current history.","title":"See this history in Galaxy"},{"location":"modules/kraken/#whats-next","text":"To use the tutorials on this website: see the list in the left hand panel or, click the menu button (three horizontal bars) in the top left of the page You can find more tutorials at the Galaxy Training Network: http://galaxyproject.github.io/training-material/","title":"What's next?"},{"location":"modules/login/","text":"","title":"Home"},{"location":"modules/mauve/","text":"Mauve Mauve is a program to align multiple genomes. Documentation: http://darlinglab.org/mauve/mauve.html What it does: aligns genomes and identifies homologous blocks these are likely from a common ancestor or gained via horizontal transfer blocks may have moved or been inverted in the genome Mauve - align three strains We will align three genomes of Streptococcus pneumoniae . Open Mauve. Go to File : Align with Progressive Mauve Add Sequence select the sequence(s). Use .fasta or .gbk files. if using a reference sequence, add that first. Align Specify a name for the alignment. Save A console window will open and show the progress of the run. When finished, the alignment will open: Each row is a genome. Each coloured block is genetically similar. If you are using annotated genomes, zoom in (with the magnifying glass) to see annotations. For a different view, go to View: Style: Solid LCB colouring Click on a block to align all genomes around that block. The blue block is inverted in genome 3 (i.e., the reverse complement). Mauve - align two assemblies from the same sample In this example, we will align two genomes from the same sample that have been assembled with different tools. Genome 1: Assembled from long reads; corrected with short reads. Genome 2: Assembled from short reads. Align the genomes: Go to File : Align with Progressive Mauve Add sequences. Add the long-read assembly sequence first. Align Name Save View the alignment: Genome 2 has many contigs as it has been assembled using short reads. These have been laid out in the order in which they appear in the file. We need to re-arrange these contigs to align with the reference genome (Genome 1). Re-order the contigs in Genome 2: Go to Tools : Move Contigs Specify output folder Add sequences (add the long-read assembly first) Start The Mauve Console window will show the progress. The re-ordered contigs will then be displayed: Most of the contigs in Genome 2 can be aligned to one (red) section of Genome 1.","title":"Mauve"},{"location":"modules/mauve/#mauve","text":"Mauve is a program to align multiple genomes. Documentation: http://darlinglab.org/mauve/mauve.html What it does: aligns genomes and identifies homologous blocks these are likely from a common ancestor or gained via horizontal transfer blocks may have moved or been inverted in the genome","title":"Mauve"},{"location":"modules/mauve/#mauve-align-three-strains","text":"We will align three genomes of Streptococcus pneumoniae . Open Mauve. Go to File : Align with Progressive Mauve Add Sequence select the sequence(s). Use .fasta or .gbk files. if using a reference sequence, add that first. Align Specify a name for the alignment. Save A console window will open and show the progress of the run. When finished, the alignment will open: Each row is a genome. Each coloured block is genetically similar. If you are using annotated genomes, zoom in (with the magnifying glass) to see annotations. For a different view, go to View: Style: Solid LCB colouring Click on a block to align all genomes around that block. The blue block is inverted in genome 3 (i.e., the reverse complement).","title":"Mauve - align three strains"},{"location":"modules/mauve/#mauve-align-two-assemblies-from-the-same-sample","text":"In this example, we will align two genomes from the same sample that have been assembled with different tools. Genome 1: Assembled from long reads; corrected with short reads. Genome 2: Assembled from short reads. Align the genomes: Go to File : Align with Progressive Mauve Add sequences. Add the long-read assembly sequence first. Align Name Save View the alignment: Genome 2 has many contigs as it has been assembled using short reads. These have been laid out in the order in which they appear in the file. We need to re-arrange these contigs to align with the reference genome (Genome 1). Re-order the contigs in Genome 2: Go to Tools : Move Contigs Specify output folder Add sequences (add the long-read assembly first) Start The Mauve Console window will show the progress. The re-ordered contigs will then be displayed: Most of the contigs in Genome 2 can be aligned to one (red) section of Genome 1.","title":"Mauve - align two assemblies from the same sample"},{"location":"modules/metagenomics/","text":"Metagenomics in Galaxy New to Galaxy? First try the introduction and then learn some key tasks Metagenomics is a complicated topic but there are several tools in Galaxy that can run partial or total analyses. The Galaxy Training Network has several excellent tutorials and background information on the topic. We recommend starting with this tutorial that uses 16S data from two types of soil samples. Log in to Galaxy Log in to your Galaxy instance (for example, Galaxy Australia, usegalaxy.org.au ). Use shared data If you are using Galaxy Australia, you can import the data from a shared data library. In the top menu bar, go to Shared Data . Click on Data Libraries . Click on Galaxy Australia Training Material: Assembly: Metagenomics: Soil metagenomics . Tick the boxes next to the five files. Click the To History button, select As Datasets . Name a new history and click Import . In the top menu bar, click Analyze Data . You should now have five files in your current history. Follow the tutorial Follow the steps in the first half of this tutorial that covers metagenomics analyses using amplicon data: Click here for tutorial link. The workflow for this tutorial looks like this: Workflow part 1 Workflow part 2 See this history in Galaxy If you want to see this Galaxy history without performing the steps above: Log in to Galaxy Australia: https://usegalaxy.org.au/ Go to Shared Data Click Histories Click Completed-soil-metagenomics-analysis Click Import (at the top right corner) The analysis should now be showing as your current history. What s next? To use the tutorials on this website: see the list in the left hand panel or, click the menu button (three horizontal bars) in the top left of the page You can find more tutorials at the Galaxy Training Network: http://galaxyproject.github.io/training-material/","title":"Metagenomics"},{"location":"modules/metagenomics/#metagenomics-in-galaxy","text":"New to Galaxy? First try the introduction and then learn some key tasks Metagenomics is a complicated topic but there are several tools in Galaxy that can run partial or total analyses. The Galaxy Training Network has several excellent tutorials and background information on the topic. We recommend starting with this tutorial that uses 16S data from two types of soil samples.","title":"Metagenomics in Galaxy"},{"location":"modules/metagenomics/#log-in-to-galaxy","text":"Log in to your Galaxy instance (for example, Galaxy Australia, usegalaxy.org.au ).","title":"Log in to Galaxy"},{"location":"modules/metagenomics/#use-shared-data","text":"If you are using Galaxy Australia, you can import the data from a shared data library. In the top menu bar, go to Shared Data . Click on Data Libraries . Click on Galaxy Australia Training Material: Assembly: Metagenomics: Soil metagenomics . Tick the boxes next to the five files. Click the To History button, select As Datasets . Name a new history and click Import . In the top menu bar, click Analyze Data . You should now have five files in your current history.","title":"Use shared data"},{"location":"modules/metagenomics/#follow-the-tutorial","text":"Follow the steps in the first half of this tutorial that covers metagenomics analyses using amplicon data: Click here for tutorial link. The workflow for this tutorial looks like this: Workflow part 1 Workflow part 2","title":"Follow the tutorial"},{"location":"modules/metagenomics/#see-this-history-in-galaxy","text":"If you want to see this Galaxy history without performing the steps above: Log in to Galaxy Australia: https://usegalaxy.org.au/ Go to Shared Data Click Histories Click Completed-soil-metagenomics-analysis Click Import (at the top right corner) The analysis should now be showing as your current history.","title":"See this history in Galaxy"},{"location":"modules/metagenomics/#whats-next","text":"To use the tutorials on this website: see the list in the left hand panel or, click the menu button (three horizontal bars) in the top left of the page You can find more tutorials at the Galaxy Training Network: http://galaxyproject.github.io/training-material/","title":"What's next?"},{"location":"modules/metagenomics/metagenomics/","text":"Metagenomics In development Based on material by QFAB http.qfab.org Tutorial Overview In this tutorial we cover the concepts of metagenomics analysis in 16S rRNA. What\u2019s not covered: This introductory tutorial covers the steps required for the first part of a 16S rRNA metagenomics study applied to samples from a single site. As such we only show analyses that can be applied to a single environment, that is, identifying what organisms are in your sample (diversity), and how many organisms there are in the sample (richness). Similar same steps would be applied if you had samples from multiple sites, with additional analyses that can compare the diversity and richness between samples. The latter part is not covered by this tutorial. Background 16S rRNA sequencing The application of culture-independent rRNA-based phylogenetics to the study of bacterial diversity was first explored by researchers in the 1980s e.g. [2] and was used by Carl Woese to delineate the main branches of life [1]. These rRNA based analyses remain central methods in microbiology both for the exploration of species diversity but also or the application in bacterial identification. The development of pyrosequencing technologies and the emergence of next generation DNA sequencing have changed the scales at which scientists can systematically survey the 16S rRNA content of biological samples and has been a key driver behind initiatives such as the human microbiome project [5]. In the broadest sense, 16S rRNA sequencing should not be confused with metagenomics which has been defined as the application of shotgun sequencing to DNA obtained directly from an environmental sample or series of related samples, producing at least 50Mbp of randomly sampled sequence data [3] This is also in contrast to functional metagenomics whereby environmental DNA is first cloned and subsequently screened for specific functional activities e.g. [4]. Kunin et al elegantly argue that 16S rRNA profiling should in fact exist as a mandatory Premetagenome community composition profiling step in more comprehensive surveys [3]. There is discussion and debate as to how deep this sequencing could be performed but at JGI at least, a single 384 well plate is routinely surveyed. Irrespective of the nuances of terminology, metagenomics is a baseline technology for understanding the ecology and evolution of microbial ecosystems upon which hypotheses and experimental strategies are built. Experimental design is key Most 16S strategies are biased towards the measurement of bacterial and archaeal constituents of the environment and eukaryotes (e.g. fungi and protists) are excluded. This compromises the ability to measure the whole ecological spectrum of a microbial community; the larger and non-coding and thus uninterpretable parts of the eukaryotic genome are problematic. This is largely driving strategies that encompass additional metatranscriptomics surveys. The studies are thus oriented towards the sequence-tractable bacterial, archaeal and viral components of the research community. Limitations of the 16S rRNA profiling strategy The 16S rRNA gene copy number of variable between bacterial species and this is further confounded by PCR amplification biases which skew estimates of the community composition. Goals The goal of the 16S study is to understand the complexity of the community being sampled. This is assessed as a function of the number of species in the community (richness) and their relative abundances (evenness). The workflow in Galaxy The process presented in this workshop is aimed to be an introduction for molecular biologists to 16S rRNA metagenomics analysis. The workflow is also implemented such that it can be run using a standard desktop or laptop environment without the requirements of a large cluster or cloud infrastructure. The steps are: De-replicate reads Cluster reads into Operational Taxonomic Units (OTU) Chimera detection and removal OTU Table Visualize taxa in a pie chart Phylogenetic Tree Rarefaction analysis Collector Curve analysis Where is the data in this tutorial from? We use the same data from the Mothur Marine community analysis , that is the study from the Global Ocean Sampling (GOS) Expedition . However we only concentrate on one site as opposed to the 14 sites they sampled. Preparation Log in to your Galaxy instance (for example, Galaxy Australia, usegalaxy.org.au ). Use shared data If you are using Galaxy Australia, you can import the data from a shared data library. In the top menu bar, go to Shared Data . Click on Data Libraries . Click on Galaxy Australia Training Material: Metagenomics: 16S Metagenomics . Tick the boxes next to the three files. Click the To History button, select As Datasets . Name a new history and click Import . In the top menu bar, click Analyze Data . You should now have three files in your current history. Or, import from the web Only follow this step if unable to load the data files from shared data, as described above . In a new browser tab, go to this webpage: Right click on file name: select copy link address In Galaxy, go to Get Data and then Upload File Click Paste/Fetch data A box will appear: paste in link address Click Start Click Close The file will now appear in the top of your history panel. Repeat for the other two files. Shorten file names Click on the pencil icon next to the file name In the centre Galaxy panel, click in the box under Name Shorten the file name Then click Save Keep only the unique reads The aim here is to reduce the number of reads in the input file into clusters of identical sequences. De-replicate In the tool panel (left hand side) search for the Dereplicate tool and click on it. For Input sequence file select the seqs.fasta dataset For Criteria used for duplicate detection select Full length There are two possible modes in the dereplication tool: (1) full-length where the full length of the two sequences must be identical before being grouped together or (2) prefix where the first section of the two sequences are identical Click on Execute Output Click on the name of the job Dereplicate on data XX in the History panel (right hand side) and you will see the number of sequences have been reduced from 1,472 in the input data seqs.fasta to 1,459 sequences. Examine the output by clicking on the eye icon in the top right hand corner of this step. NOTE: The size=XXXX annotation in the id line of the fasta file indicates the number of reads that were exactly the same as this representative sequence. (OPTIONAL) You can rename the output by clicking on the pencil icon on the top right hand corner of the output. Click on the \u201cAttributes tab\u201d and then type in a new name in the \u201cName\u201d field and click on Save. Cluster sequences into groups The aim here is to cluster the output from Step 1 into groups of sequences that are 97% similar to each other. Similar sequences are assumed to belong to the same taxonomic groups. * These groups are known as Operational Taxonomic Units (OTUs) and the number of clusters that result are indicative of the number of species that are present in this sample. Cluter OTUs In the tool panel on the left hand side, search for the Cluster OTU tool. For Input sequence file , select the output from Step 1: Dereplicate on data XX For Minimum cluster radius , enter 0.97. This value corresponds to 97% sequence identity. Click on Execute Output Click on the name of the job Cluster OTU on data 4 in the History panel (right hand side) and you will find that the number of sequences has decreased to 559 sequences. Examine the output by clicking on the eye icon in the top right hand corner of this step. The output file is in Fasta format where each sequence is the representative of an OTU. Figure 2 below, provides a summary of the clustering approach. Reads are represented by solid dots. Those that are 97% similar to each other are grouped together into an OTU as represented by the blue dashed circles. In this example, there are 3 OTUs. The reads that represents the centroid of each cluster are shown as red dots and these red dots form the representative of the corresponding OTU. These are the sequences that are reported in the output file. In this tutorial, the output file has 559 sequences, meaning that 559 OTUs were identified. NOTE: The number of sequences that you have may not be exactly the same as mentioned here. This is so as the clustering process starts by randomly selecting a read as the center of a cluster (see figure below). Since this is a random selection, each time you perform a clustering process, a different read can be selected and thus affect the end result. For more detailed description of the OTU clustering algorithm, click here . Figure 2. Please Note: The \u201csize=XX\u201d annotation in the identifier line is NOT the number of sequences that is in the corresponding cluster (OTU). This value is the same as the input file. Remove chimeras The aim here is to: identify and filter chimeric sequences utilizing a reference dataset containing nucleotide sequences believed to be free of chimeras. Uchime In the tool panel on the left hand side, search for the Uchime tool. For Mode to detect chimeras , select \u201cref\u201d (reference mode). There are two possible modes in the Uchime tool for chimera detection: (1) ref where a reference dataset of parent sequences believed to be chimera-free is provided by the user or (2) de-novo where a database is constructed from the query sequences by using the abundance data. For the second field Input reference file select the Cluster OTU on data XX For the third field Reference Database , select the gold.fasta . This file (downloaded from http://drive5.com/uchime/gold.fa) contains nucleotide sequences believed to be free of chimeras. Click on Execute Output Click on Uchime on data XX and data XX:chimeras These sequences have been classified to be chimeras Click on Uchime on data XX and data XX:non chimeras These sequences have been classified to be non-chimeras Only the \u201cnon-chimeras\u201d output be used for the subsequent steps. All other sequences, chimeras and nonclassified will be disregarded. There are 2 hidden log files also generated from this tool, they contain detailed job information about the classification into non-chimeras and chimeras. (OPTIONAL) To access the hidden outputs, click on the cog wheel in the upper right corner of the History panel and select \u201cInclude Hidden Datasets\u201d. Click on Uchime on data XX and data XX:Human-readable output The chimeric alignments (with respect to the putative parents) are listed following the human-readable format of the USEARCH-Tool-Suite . Click on Uchime on data XX and data XX:Tabbed output For each sequence there are 18 values logged in a tabbed format, as following: NOTE: The number of sequences that you have may not be exactly the same due to the results of the OTU Clustering step in Section 2. How many reads per OTU cluster? The aim here is to generate a full OTU Table which consists of the number of reads found for each group, the representative sequence and the predicted taxonomy. This is accomplished by running two tools and merging the resulting outputs together. The first tool \u201cMap Reads to OTU\u201d, as the name indicates will map read sequences to the OTU cluster representatives from Section 2. In section 2, we clustered the reads into clusters that are 97% similar to all other members of that cluster. Then for each cluster, a representative sequence was selected. However, we do not know the number of reads that are in each cluster. Furthermore, in Section 3 we removed sequences that were potential chimeric sequences; this alters the number of remaining clusters. Therefore, in this step we first find out which of the remaining clusters each read belongs with 97% similarity and this will give us a count of the number of reads per cluster. The second tool \u201cRDP Multi Classifier\u201d will assign each the OTU representative sequence to a taxonomy level. The last step then merges the information to generate the OTU Table Map reads to OTU - how many reads per OTU? In the tool panel on the left hand side, search for the Map Reads to OTU tool. For OTU fasta file select dataset Uchime on data XX and data XX: non_chimeras For Input reads file select dataset Dereplicate on data XX For Minimum identity enter 0.97. Like before, this value corresponds to 97% sequence similarity. Click on Execute Output Click on Map Reads to OTU on data XX and data XX:rabund The output is tab delimited and follows the rabund format from mothur . Note that this is just a one-line file which wraps around in the Galaxy display window to look like a multi-line file. In other words, this is a matrix of one row only, and many columns. The first column is a label (this is usually the identity) The second column is the number of OTUs found Each subsequent column is the number of reads found for the corresponding OTUs. Click on Map Reads to OTU on data XX and data XX:Pre-OTU Table This is the pre OTU table with the following columns: OTU label, Count, Sequence Click on Map Reads to OTU on data XX and data XX:relabelled OTU This is a FASTA file. The input reads from Uchime on data XX and data XX: non_chimeras have been re-labelled with convenient labels for parsing, e.g. OTU_1, OTU_2, OTU_N, where N is the number of OTUs. There is one hidden log file also generated from this tool, which reports whether a query sequence matches or not matches the database. (OPTIONAL) To access the hidden output, click on the cog wheel in the upper right corner of the History panel and select \u201cInclude Hidden Datasets\u201d. Click on Map Reads to OTU on data XX and data XX:hit list This file reports query sequences that match or did not match the database Cluster OTU on data XX following the UC-format of the USEARCH-Tool-Suite . Note that a given read may match two or more OTUs given the identity threshold. In such cases the tool will tend to assign the read to the OTU with highest identity and will break ties arbitrarily. Some reads may not match any OTU for these reasons: (1) the read is chimeric, (2) the read has more than 3% errors, (3) the read has a singleton sequence so was discarded. RDP Classifier - what taxa match the OTUs? A complete OTU table contains the taxonomy assignment of each sequence. To complete the Pre-OTU Table, we will use the RDP MultiClassifier tool. In the tool panel on the left hand side, search for the RDP MultiClassifier tool. For Select Gene Trainings Model select \u201c16S rRNA\u201d from the dropdown list. For Select to generate an OTU Table click Yes . For Relabelled OTU input reads of the \u2018Map Reads to OTU\u2019 tool in FASTA format choose Map Reads to OTU on data XX and data XX:relabelled OTU For PRE OTU table of the \u2018Map Reads to OTU\u2019 tool select Map Reads to OTU on data XX and data XX:Pre-OTU Table For Assignment confidence cutoff , enter 0.8. This value is used to specify the assignment confidence cutoff used to determine the assignment count in the hierarchical format. The range is between 0 and 1 with 0.8 being the default value. For sequences that are shorter than 250 base pairs, the tool developers recommend using 0.5 as the confidence threshold. (http://rdp.cme.msu.edu/tutorials/classifier/RDPtutorial_MULTICLASSIFIER.html) For Tab delimited output format , select \u201cfixrank\u201c from the drop-down list Click on Execute Output Click on RDP MultiClassifier on data XX and data XX:classification_assignment_hierarchical.tab Sequence count for each taxon in the hierarchy Click on RDP MultiClassifier on data XX and data XX:classification_assignment_details.tab Sequence-by-sequence classification results including confidence scores at each level of the hierarchy Click on RDP MultiClassifier on data XX and data XX:OTU_Table.tab The OTU table is composed of the following columns: OTU label, Count, Sequence, Phylogenetic lineage. View taxa with Krona Krona is a tool that shows you a pie chart of the taxa found in your sample. Tidy the output Cut some columns Search for Cut columns Cut columns : c2,c4,c7,c10,c13,c16,c19 Delimited by : Tab From : the RDP MultiClassifier OTU_Table.tab Execute Take out the quotation marks Search for Regex Find And Replace For Select lines from : the previous file, Cut on data XX Click Insert Check Find Regex : Replacement : delete any text in here and leave empty Execute Krona Search for Visualize with Krona For Select input file : the previous file, Regex Find And Replace on data XX Is this output from MOTHUR : No Execute Click on the eye icon next to the output file. This is a pie chart showing the abundance of taxa. The higher taxonomic levels are in the centre, and the lower taxonomic levels are in the outer circle. Click on a taxon to display the information in more detail. Most of our sample is classified as Bacteria. A particularly abundant genus is Martelelella . Phylogeny How are our taxa related phylogenetically? We will run two tools to investigate. PyNAST The aim here is to build and visualise the phylogenetic tree. First, we align the sequences to a template sequence using PyNAST. PyNast is a reimplementation of the NAST algorithm to align each provided sequence (the \u201ccandidate\u201d sequence) to the best-matching sequence in a pre-aligned database of sequences (the \u201ctemplate\u201d sequence). In the tool panel on the left hand side, search for the PyNAST tool. For Candidate file (Fasta format) select Uchime on data XX and data XX:non_chimeras For Template alignment file (Fasta format) , select core_set_aligned.fasta For Minimum sequence length , enter 1000. The minimum sequence length is the length to include in the NAST alignment and it has to be set according to your set of your sequences. Click Execute Outputs: Click on PyNAST on data XX and data XX: pynast aligned This file contains the alignment. You will find that the number of sequences might have decreased. This means that the alignment for some sequences failed. More information is given in the log file and the failure file, which are hidden outputs. (OPTIONAL) To access the hidden outputs, click on the cog wheel in the upper right corner of the History panel and select \u201cInclude Hidden Datasets\u201d. Click on PyNAST on data XX and data XX: pynast log file This file is composed of the following columns: candidate sequence ID candidate nucleotide count errors template ID BLAST percent identify to template candidate nucleotide count post-NAST You will observe that some sequences have an entry in the error column: \u201cNo search results\u201d. These are the sequences that failed to align. Click on PyNAST on data XX and data XX: pynast failure This file contains the sequences that failed to align. We already found out through the log file which sequences failed to align; here those sequences are given in FASTA format. FastTree Next we use \u201cFast Tree\u201d to generate the phylogenetic tree using the aligned sequenced file. In the tool panel on the left hand side, search for the FastTree tool. For Aligned sequences file (FASTA format) , select PyNAST on data XX and data XX:pynast aligned For Protein or nucleotide alignment select \u201cNucleotide\u201d Click Execute Output Click on FastTree on data 18:tree.nhx To visualize the phylogenetic tree, click on the \u201cVisualize\u201d icon under the file name. Click on Phyloviz Phyloviz is the interactive Phylogenetic Tree Visualizer included from Galaxy. Currently Phyloviz has only one entry point, which is via the \u201cVisualize - View in Phyloviz\u201d icon that will appear for all supported phylogenetic data sets. Phyloviz will lay out the tree in a linear format. All phylogenetic distances are normalized to a value between 0 to 1.0 inclusive, and a default value of 250px would represent 1.0 units from a parent node to child node. The laying out of phyloviz is \u201cin-place\u201d meaning that the tree is presented in the same order as it is represented in the data and that no nodes are shuffled. Phyloviz supports a number of user interaction features to help you present, analyze and share the visualization. Please see the Galaxy wiki for more information on Phyloviz. (OPTIONAL) To access the hidden log file, click on the cog wheel in the upper right corner of the History panel and select \u201cInclude Hidden Datasets\u201d. Click on FastTree on data XX:log_FastTree_run.txt This is the log file from Fast tree and shows job related information. It is not required for the analysis workflow. Was sampling sufficient? Rarefaction analysis The aim here is to generate an intra-sample rarefaction curve to determine if the sampling effort was sufficient. Rarefaction curves provide a way of comparing the richness observed in different samples although it is a better measure of sample diversity. Briefly, it plots the average number of OTUs that you would expect to observe against the number of sequences. If the curve plateaus off, then it is a good indication that the sample is saturated and you have captured the majority (if not all) of species/strains present in the sample. However, if the curve does not plateau off, this suggests there may still be more species/strains that have not been captured in your data and you need increase the size of your sample. Rarefaction tool In the tool panel on the left hand side, search for the Rarefaction tool. For Input file format select rabund from the dropdown list For OTU list (rabund or sabund format) , select Map Reads to OTU on data XX and data XX:rabund For Minimum identity used in OTU clustering , enter the minimum identity used when you generated the OTU Table. In this tutorial we used 0.97. In case you can\u2019t remember the minimum identity used, go back to the output file Cluster OTU on data XX in your History. Click on the file name to extend the dataset information. Click on the \u201cView details\u201d icon. In the Input Parameter section you will find the \u201cMinimum cluster radius\u201d you used in the OTU Clustering step. Click Execute Output Click on Rarefaction on data XX . The output is a tabbed delimited file with the following columns: The first column (numsampled): the number sampled and indicates the level of sampling intensity (by default this information is provided every 100 individuals) The second column (0.03) is the average number of OTUs that were observed for that sampling intensity based on the number of iterations, which is 1000 by default. This would represents the data for the y-axis for the corresponding minimum identity (1-0.97=0.03). The third column (lci) and fourth column (hci) represent the confidence intervals. To plot the rarefaction curve: Click on Rarefaction on data XX to extend the dataset information. Click on the \u201cVisualize\u201d icon Select \u201cScatterplot\u201d Under Column 1, highlight X Column (the number sampled) Under Column 2, highlight Y Column (the average number of OTUs that were observed for that sampling intensity based on the number of iterations, which is 1,000 by default). Click Draw Estimate diversity The aim here is to estimate the diversity and richness of the sample. Collector s curves describe how the richness or diversity change as you add more samples into the dataset. If a collector s curve plateaus, it is a good indication that enough samples were collected and you can have confidence in the last value of the curve. Collector s curve In the tool panel on the left hand side, search for the Collector Curve tool. For OTU list (rabund, sabund, list or shared format) , select Map Reads to OTU on data XX and data XX:rabund For Labels - OTU labels , Click Select and make sure that 0.03 appears in the box. This correspondd to 97%. Note that this value is the inverse of other similarity thresholds we used thus far (1-0.97=0.03) and is the default value. For Calculators , there is no need to change anything from the default calculators: chao - Community richness - the Chao estimator invsimpson - Community diversity - the Simpson index npshannon - Community diversity - the non-parametric Shannon index. Click Execute Output Click on Collector Curve on data XX:tab This file provides a summary containing the following fields: number of sequences, the sample coverage, the number of observed OTUs, and then a summary of each of the selected calculators that were applied to the sample. Click on Collector Curve on data XX:tab (invsimpson) The Simpson\u2019s Inverse Index also referred to as Simpson\u2019s Reciprocal Index is a measure of diversity. Species diversity is a way to compare samples that incorporates both a measure of evenness (representation by each species) and richness (number of each species in the sample). This index starts with the value of 1 as the lowest possible figure. This figure would represent a community containing only one species. The higher the value, the greater the diversity. The maximum value is the number of species in the sample. Click on Collector Curve on data XX:tab (npshannon) The non-parametric Shannon Index , like the Simpson\u2019s Inverse Index, is a diversity estimator and calculates a non-parametric estimate of the classical Shannon diversity Index for an OTU definition. The interpretation of the curve has to be performed in a comparison to the diversity curve of one or more communities. Click on Collector Curve on data XX:tab (chao) The Chao1 estimator is a species richness estimator. The species richness estimators estimate the total number of species present in a community. The Chao1 index estimator the species richness for one community based on the observed species frequencies or presence/absence data. If you compare the rarefaction curve (observed richness) and Chao1 estimator curve (estimated richness), you will observe a gap. The greater estimated richness in comparison to the observed richness usually indicates that with further sampling, the richness would likely continue to increase. Please note that if a sample contains many singletons, it is likely that more undetected OTUs exist, and the Chao1 index will estimate greater species richness than it would for a sample without rare OTUs. These three outputs are tabbed delimited with the following columns: First column (numsampled): is the number sampled and indicates the level of sampling intensity (by default this information is provided every 100 individuals) The second column (0.03) is the average number of OTUs that were observed for that sampling intensity based on the number of iterations, which is 1000 by default. This would represents the data for the y-axis for the corresponding minimum identity (1-0.97=0.03). The third column (lci) and fourth column (hci) represent the confidence intervals These files can be plotted similar to the rarefaction curve in the previous step. Plot the collector\u2019s curves Click on Collector Curve on data XX:tab (invsimpson) to extend the dataset information. Click on the now accessible \u201cVisualize\u201d icon Select \u201cScatterplot\u201d Under Column 1, highlight X Column (the number sampled) Under Column 2, highlight Y Column (the average number of OTUs that were observed for that sampling intensity based on the number of iterations, which is 1,000 by default). Click Draw Repeat to plot the curves from the other calculators. The workflow canvas References Woese CR, Kandler O, Wheelis ML (1990) Towards a natural system of organisms: proposal for the domains Archaea, Bacteria, and Eucarya. Proc Natl Acad Sci U S A 87: 4576-4579. Lane DJ, Pace B, Olsen GJ, Stahl DA, Sogin ML, et al. (1985) Rapid determination of 16S ribosomal RNA sequences for phylogenetic analyses. Proc Natl Acad Sci U S A 82: 6955-6959. Kunin V, Copeland A, Lapidus A, Mavromatis K, Hugenholtz P (2008) A bioinformatician s guide to metagenomics. Microbiol Mol Biol Rev 72: 557-578. Handelsman J, Rondon MR, Brady SF, Clardy J, Goodman RM (1998) Molecular biological access to the chemistry of unknown soil microbes: a new frontier for natural products. Chem Biol 5: R245-249. Turnbaugh PJ, Ley RE, Hamady M, Fraser-Liggett CM, Knight R, et al. (2007) The human microbiome project. Nature 449: 804-810. Holmes S, Alekseyenko A, Timme A, Nelson T, Pasricha PJ, et al. (2011) Visualization and statistical comparisons of microbial communities using R packages on phylochip data. Pac Symp Biocomput: 142-153. Hamady M, Lozupone C, Knight R (2010) Fast UniFrac: facilitating high-throughput phylogenetic analyses of microbial communities including analysis of pyrosequencing and PhyloChip data. ISME J 4: 17-27. Brodie EL, Desantis TZ, Joyner DC, Baek SM, Larsen JT, et al. (2006) Application of a high-density oligonucleotide microarray approach to study bacterial population dynamics during uranium reduction and reoxidation. Appl Environ Microbiol 72: 6288-6298.","title":"Metagenomics"},{"location":"modules/metagenomics/metagenomics/#metagenomics","text":"In development Based on material by QFAB http.qfab.org","title":"Metagenomics"},{"location":"modules/metagenomics/metagenomics/#tutorial-overview","text":"In this tutorial we cover the concepts of metagenomics analysis in 16S rRNA. What\u2019s not covered: This introductory tutorial covers the steps required for the first part of a 16S rRNA metagenomics study applied to samples from a single site. As such we only show analyses that can be applied to a single environment, that is, identifying what organisms are in your sample (diversity), and how many organisms there are in the sample (richness). Similar same steps would be applied if you had samples from multiple sites, with additional analyses that can compare the diversity and richness between samples. The latter part is not covered by this tutorial.","title":"Tutorial Overview"},{"location":"modules/metagenomics/metagenomics/#background","text":"","title":"Background"},{"location":"modules/metagenomics/metagenomics/#16s-rrna-sequencing","text":"The application of culture-independent rRNA-based phylogenetics to the study of bacterial diversity was first explored by researchers in the 1980s e.g. [2] and was used by Carl Woese to delineate the main branches of life [1]. These rRNA based analyses remain central methods in microbiology both for the exploration of species diversity but also or the application in bacterial identification. The development of pyrosequencing technologies and the emergence of next generation DNA sequencing have changed the scales at which scientists can systematically survey the 16S rRNA content of biological samples and has been a key driver behind initiatives such as the human microbiome project [5]. In the broadest sense, 16S rRNA sequencing should not be confused with metagenomics which has been defined as the application of shotgun sequencing to DNA obtained directly from an environmental sample or series of related samples, producing at least 50Mbp of randomly sampled sequence data [3] This is also in contrast to functional metagenomics whereby environmental DNA is first cloned and subsequently screened for specific functional activities e.g. [4]. Kunin et al elegantly argue that 16S rRNA profiling should in fact exist as a mandatory Premetagenome community composition profiling step in more comprehensive surveys [3]. There is discussion and debate as to how deep this sequencing could be performed but at JGI at least, a single 384 well plate is routinely surveyed. Irrespective of the nuances of terminology, metagenomics is a baseline technology for understanding the ecology and evolution of microbial ecosystems upon which hypotheses and experimental strategies are built.","title":"16S rRNA sequencing"},{"location":"modules/metagenomics/metagenomics/#experimental-design-is-key","text":"Most 16S strategies are biased towards the measurement of bacterial and archaeal constituents of the environment and eukaryotes (e.g. fungi and protists) are excluded. This compromises the ability to measure the whole ecological spectrum of a microbial community; the larger and non-coding and thus uninterpretable parts of the eukaryotic genome are problematic. This is largely driving strategies that encompass additional metatranscriptomics surveys. The studies are thus oriented towards the sequence-tractable bacterial, archaeal and viral components of the research community.","title":"Experimental design is key"},{"location":"modules/metagenomics/metagenomics/#limitations-of-the-16s-rrna-profiling-strategy","text":"The 16S rRNA gene copy number of variable between bacterial species and this is further confounded by PCR amplification biases which skew estimates of the community composition.","title":"Limitations of the 16S rRNA profiling strategy"},{"location":"modules/metagenomics/metagenomics/#goals","text":"The goal of the 16S study is to understand the complexity of the community being sampled. This is assessed as a function of the number of species in the community (richness) and their relative abundances (evenness).","title":"Goals"},{"location":"modules/metagenomics/metagenomics/#the-workflow-in-galaxy","text":"The process presented in this workshop is aimed to be an introduction for molecular biologists to 16S rRNA metagenomics analysis. The workflow is also implemented such that it can be run using a standard desktop or laptop environment without the requirements of a large cluster or cloud infrastructure. The steps are: De-replicate reads Cluster reads into Operational Taxonomic Units (OTU) Chimera detection and removal OTU Table Visualize taxa in a pie chart Phylogenetic Tree Rarefaction analysis Collector Curve analysis","title":"The workflow in Galaxy"},{"location":"modules/metagenomics/metagenomics/#where-is-the-data-in-this-tutorial-from","text":"We use the same data from the Mothur Marine community analysis , that is the study from the Global Ocean Sampling (GOS) Expedition . However we only concentrate on one site as opposed to the 14 sites they sampled.","title":"Where is the data in this tutorial from?"},{"location":"modules/metagenomics/metagenomics/#preparation","text":"Log in to your Galaxy instance (for example, Galaxy Australia, usegalaxy.org.au ).","title":"Preparation"},{"location":"modules/metagenomics/metagenomics/#use-shared-data","text":"If you are using Galaxy Australia, you can import the data from a shared data library. In the top menu bar, go to Shared Data . Click on Data Libraries . Click on Galaxy Australia Training Material: Metagenomics: 16S Metagenomics . Tick the boxes next to the three files. Click the To History button, select As Datasets . Name a new history and click Import . In the top menu bar, click Analyze Data . You should now have three files in your current history.","title":"Use shared data"},{"location":"modules/metagenomics/metagenomics/#or-import-from-the-web","text":"Only follow this step if unable to load the data files from shared data, as described above . In a new browser tab, go to this webpage: Right click on file name: select copy link address In Galaxy, go to Get Data and then Upload File Click Paste/Fetch data A box will appear: paste in link address Click Start Click Close The file will now appear in the top of your history panel. Repeat for the other two files.","title":"Or, import from the web"},{"location":"modules/metagenomics/metagenomics/#shorten-file-names","text":"Click on the pencil icon next to the file name In the centre Galaxy panel, click in the box under Name Shorten the file name Then click Save","title":"Shorten file names"},{"location":"modules/metagenomics/metagenomics/#keep-only-the-unique-reads","text":"The aim here is to reduce the number of reads in the input file into clusters of identical sequences.","title":"Keep only the unique reads"},{"location":"modules/metagenomics/metagenomics/#de-replicate","text":"In the tool panel (left hand side) search for the Dereplicate tool and click on it. For Input sequence file select the seqs.fasta dataset For Criteria used for duplicate detection select Full length There are two possible modes in the dereplication tool: (1) full-length where the full length of the two sequences must be identical before being grouped together or (2) prefix where the first section of the two sequences are identical Click on Execute","title":"De-replicate"},{"location":"modules/metagenomics/metagenomics/#output","text":"Click on the name of the job Dereplicate on data XX in the History panel (right hand side) and you will see the number of sequences have been reduced from 1,472 in the input data seqs.fasta to 1,459 sequences. Examine the output by clicking on the eye icon in the top right hand corner of this step. NOTE: The size=XXXX annotation in the id line of the fasta file indicates the number of reads that were exactly the same as this representative sequence. (OPTIONAL) You can rename the output by clicking on the pencil icon on the top right hand corner of the output. Click on the \u201cAttributes tab\u201d and then type in a new name in the \u201cName\u201d field and click on Save.","title":"Output"},{"location":"modules/metagenomics/metagenomics/#cluster-sequences-into-groups","text":"The aim here is to cluster the output from Step 1 into groups of sequences that are 97% similar to each other. Similar sequences are assumed to belong to the same taxonomic groups. * These groups are known as Operational Taxonomic Units (OTUs) and the number of clusters that result are indicative of the number of species that are present in this sample.","title":"Cluster sequences into groups"},{"location":"modules/metagenomics/metagenomics/#cluter-otus","text":"In the tool panel on the left hand side, search for the Cluster OTU tool. For Input sequence file , select the output from Step 1: Dereplicate on data XX For Minimum cluster radius , enter 0.97. This value corresponds to 97% sequence identity. Click on Execute","title":"Cluter OTUs"},{"location":"modules/metagenomics/metagenomics/#output_1","text":"Click on the name of the job Cluster OTU on data 4 in the History panel (right hand side) and you will find that the number of sequences has decreased to 559 sequences. Examine the output by clicking on the eye icon in the top right hand corner of this step. The output file is in Fasta format where each sequence is the representative of an OTU. Figure 2 below, provides a summary of the clustering approach. Reads are represented by solid dots. Those that are 97% similar to each other are grouped together into an OTU as represented by the blue dashed circles. In this example, there are 3 OTUs. The reads that represents the centroid of each cluster are shown as red dots and these red dots form the representative of the corresponding OTU. These are the sequences that are reported in the output file. In this tutorial, the output file has 559 sequences, meaning that 559 OTUs were identified. NOTE: The number of sequences that you have may not be exactly the same as mentioned here. This is so as the clustering process starts by randomly selecting a read as the center of a cluster (see figure below). Since this is a random selection, each time you perform a clustering process, a different read can be selected and thus affect the end result. For more detailed description of the OTU clustering algorithm, click here . Figure 2. Please Note: The \u201csize=XX\u201d annotation in the identifier line is NOT the number of sequences that is in the corresponding cluster (OTU). This value is the same as the input file.","title":"Output"},{"location":"modules/metagenomics/metagenomics/#remove-chimeras","text":"The aim here is to: identify and filter chimeric sequences utilizing a reference dataset containing nucleotide sequences believed to be free of chimeras.","title":"Remove chimeras"},{"location":"modules/metagenomics/metagenomics/#uchime","text":"In the tool panel on the left hand side, search for the Uchime tool. For Mode to detect chimeras , select \u201cref\u201d (reference mode). There are two possible modes in the Uchime tool for chimera detection: (1) ref where a reference dataset of parent sequences believed to be chimera-free is provided by the user or (2) de-novo where a database is constructed from the query sequences by using the abundance data. For the second field Input reference file select the Cluster OTU on data XX For the third field Reference Database , select the gold.fasta . This file (downloaded from http://drive5.com/uchime/gold.fa) contains nucleotide sequences believed to be free of chimeras. Click on Execute","title":"Uchime"},{"location":"modules/metagenomics/metagenomics/#output_2","text":"Click on Uchime on data XX and data XX:chimeras These sequences have been classified to be chimeras Click on Uchime on data XX and data XX:non chimeras These sequences have been classified to be non-chimeras Only the \u201cnon-chimeras\u201d output be used for the subsequent steps. All other sequences, chimeras and nonclassified will be disregarded. There are 2 hidden log files also generated from this tool, they contain detailed job information about the classification into non-chimeras and chimeras. (OPTIONAL) To access the hidden outputs, click on the cog wheel in the upper right corner of the History panel and select \u201cInclude Hidden Datasets\u201d. Click on Uchime on data XX and data XX:Human-readable output The chimeric alignments (with respect to the putative parents) are listed following the human-readable format of the USEARCH-Tool-Suite . Click on Uchime on data XX and data XX:Tabbed output For each sequence there are 18 values logged in a tabbed format, as following: NOTE: The number of sequences that you have may not be exactly the same due to the results of the OTU Clustering step in Section 2.","title":"Output"},{"location":"modules/metagenomics/metagenomics/#how-many-reads-per-otu-cluster","text":"The aim here is to generate a full OTU Table which consists of the number of reads found for each group, the representative sequence and the predicted taxonomy. This is accomplished by running two tools and merging the resulting outputs together. The first tool \u201cMap Reads to OTU\u201d, as the name indicates will map read sequences to the OTU cluster representatives from Section 2. In section 2, we clustered the reads into clusters that are 97% similar to all other members of that cluster. Then for each cluster, a representative sequence was selected. However, we do not know the number of reads that are in each cluster. Furthermore, in Section 3 we removed sequences that were potential chimeric sequences; this alters the number of remaining clusters. Therefore, in this step we first find out which of the remaining clusters each read belongs with 97% similarity and this will give us a count of the number of reads per cluster. The second tool \u201cRDP Multi Classifier\u201d will assign each the OTU representative sequence to a taxonomy level. The last step then merges the information to generate the OTU Table","title":"How many reads per OTU cluster?"},{"location":"modules/metagenomics/metagenomics/#map-reads-to-otu-how-many-reads-per-otu","text":"In the tool panel on the left hand side, search for the Map Reads to OTU tool. For OTU fasta file select dataset Uchime on data XX and data XX: non_chimeras For Input reads file select dataset Dereplicate on data XX For Minimum identity enter 0.97. Like before, this value corresponds to 97% sequence similarity. Click on Execute","title":"Map reads to OTU - how many reads per OTU?"},{"location":"modules/metagenomics/metagenomics/#output_3","text":"Click on Map Reads to OTU on data XX and data XX:rabund The output is tab delimited and follows the rabund format from mothur . Note that this is just a one-line file which wraps around in the Galaxy display window to look like a multi-line file. In other words, this is a matrix of one row only, and many columns. The first column is a label (this is usually the identity) The second column is the number of OTUs found Each subsequent column is the number of reads found for the corresponding OTUs. Click on Map Reads to OTU on data XX and data XX:Pre-OTU Table This is the pre OTU table with the following columns: OTU label, Count, Sequence Click on Map Reads to OTU on data XX and data XX:relabelled OTU This is a FASTA file. The input reads from Uchime on data XX and data XX: non_chimeras have been re-labelled with convenient labels for parsing, e.g. OTU_1, OTU_2, OTU_N, where N is the number of OTUs. There is one hidden log file also generated from this tool, which reports whether a query sequence matches or not matches the database. (OPTIONAL) To access the hidden output, click on the cog wheel in the upper right corner of the History panel and select \u201cInclude Hidden Datasets\u201d. Click on Map Reads to OTU on data XX and data XX:hit list This file reports query sequences that match or did not match the database Cluster OTU on data XX following the UC-format of the USEARCH-Tool-Suite . Note that a given read may match two or more OTUs given the identity threshold. In such cases the tool will tend to assign the read to the OTU with highest identity and will break ties arbitrarily. Some reads may not match any OTU for these reasons: (1) the read is chimeric, (2) the read has more than 3% errors, (3) the read has a singleton sequence so was discarded.","title":"Output"},{"location":"modules/metagenomics/metagenomics/#rdp-classifier-what-taxa-match-the-otus","text":"A complete OTU table contains the taxonomy assignment of each sequence. To complete the Pre-OTU Table, we will use the RDP MultiClassifier tool. In the tool panel on the left hand side, search for the RDP MultiClassifier tool. For Select Gene Trainings Model select \u201c16S rRNA\u201d from the dropdown list. For Select to generate an OTU Table click Yes . For Relabelled OTU input reads of the \u2018Map Reads to OTU\u2019 tool in FASTA format choose Map Reads to OTU on data XX and data XX:relabelled OTU For PRE OTU table of the \u2018Map Reads to OTU\u2019 tool select Map Reads to OTU on data XX and data XX:Pre-OTU Table For Assignment confidence cutoff , enter 0.8. This value is used to specify the assignment confidence cutoff used to determine the assignment count in the hierarchical format. The range is between 0 and 1 with 0.8 being the default value. For sequences that are shorter than 250 base pairs, the tool developers recommend using 0.5 as the confidence threshold. (http://rdp.cme.msu.edu/tutorials/classifier/RDPtutorial_MULTICLASSIFIER.html) For Tab delimited output format , select \u201cfixrank\u201c from the drop-down list Click on Execute","title":"RDP Classifier - what taxa match the OTUs?"},{"location":"modules/metagenomics/metagenomics/#output_4","text":"Click on RDP MultiClassifier on data XX and data XX:classification_assignment_hierarchical.tab Sequence count for each taxon in the hierarchy Click on RDP MultiClassifier on data XX and data XX:classification_assignment_details.tab Sequence-by-sequence classification results including confidence scores at each level of the hierarchy Click on RDP MultiClassifier on data XX and data XX:OTU_Table.tab The OTU table is composed of the following columns: OTU label, Count, Sequence, Phylogenetic lineage.","title":"Output"},{"location":"modules/metagenomics/metagenomics/#view-taxa-with-krona","text":"Krona is a tool that shows you a pie chart of the taxa found in your sample.","title":"View taxa with Krona"},{"location":"modules/metagenomics/metagenomics/#tidy-the-output","text":"Cut some columns Search for Cut columns Cut columns : c2,c4,c7,c10,c13,c16,c19 Delimited by : Tab From : the RDP MultiClassifier OTU_Table.tab Execute Take out the quotation marks Search for Regex Find And Replace For Select lines from : the previous file, Cut on data XX Click Insert Check Find Regex : Replacement : delete any text in here and leave empty Execute","title":"Tidy the output"},{"location":"modules/metagenomics/metagenomics/#krona","text":"Search for Visualize with Krona For Select input file : the previous file, Regex Find And Replace on data XX Is this output from MOTHUR : No Execute Click on the eye icon next to the output file. This is a pie chart showing the abundance of taxa. The higher taxonomic levels are in the centre, and the lower taxonomic levels are in the outer circle. Click on a taxon to display the information in more detail. Most of our sample is classified as Bacteria. A particularly abundant genus is Martelelella .","title":"Krona"},{"location":"modules/metagenomics/metagenomics/#phylogeny","text":"How are our taxa related phylogenetically? We will run two tools to investigate.","title":"Phylogeny"},{"location":"modules/metagenomics/metagenomics/#pynast","text":"The aim here is to build and visualise the phylogenetic tree. First, we align the sequences to a template sequence using PyNAST. PyNast is a reimplementation of the NAST algorithm to align each provided sequence (the \u201ccandidate\u201d sequence) to the best-matching sequence in a pre-aligned database of sequences (the \u201ctemplate\u201d sequence). In the tool panel on the left hand side, search for the PyNAST tool. For Candidate file (Fasta format) select Uchime on data XX and data XX:non_chimeras For Template alignment file (Fasta format) , select core_set_aligned.fasta For Minimum sequence length , enter 1000. The minimum sequence length is the length to include in the NAST alignment and it has to be set according to your set of your sequences. Click Execute Outputs: Click on PyNAST on data XX and data XX: pynast aligned This file contains the alignment. You will find that the number of sequences might have decreased. This means that the alignment for some sequences failed. More information is given in the log file and the failure file, which are hidden outputs. (OPTIONAL) To access the hidden outputs, click on the cog wheel in the upper right corner of the History panel and select \u201cInclude Hidden Datasets\u201d. Click on PyNAST on data XX and data XX: pynast log file This file is composed of the following columns: candidate sequence ID candidate nucleotide count errors template ID BLAST percent identify to template candidate nucleotide count post-NAST You will observe that some sequences have an entry in the error column: \u201cNo search results\u201d. These are the sequences that failed to align. Click on PyNAST on data XX and data XX: pynast failure This file contains the sequences that failed to align. We already found out through the log file which sequences failed to align; here those sequences are given in FASTA format.","title":"PyNAST"},{"location":"modules/metagenomics/metagenomics/#fasttree","text":"Next we use \u201cFast Tree\u201d to generate the phylogenetic tree using the aligned sequenced file. In the tool panel on the left hand side, search for the FastTree tool. For Aligned sequences file (FASTA format) , select PyNAST on data XX and data XX:pynast aligned For Protein or nucleotide alignment select \u201cNucleotide\u201d Click Execute Output Click on FastTree on data 18:tree.nhx To visualize the phylogenetic tree, click on the \u201cVisualize\u201d icon under the file name. Click on Phyloviz Phyloviz is the interactive Phylogenetic Tree Visualizer included from Galaxy. Currently Phyloviz has only one entry point, which is via the \u201cVisualize - View in Phyloviz\u201d icon that will appear for all supported phylogenetic data sets. Phyloviz will lay out the tree in a linear format. All phylogenetic distances are normalized to a value between 0 to 1.0 inclusive, and a default value of 250px would represent 1.0 units from a parent node to child node. The laying out of phyloviz is \u201cin-place\u201d meaning that the tree is presented in the same order as it is represented in the data and that no nodes are shuffled. Phyloviz supports a number of user interaction features to help you present, analyze and share the visualization. Please see the Galaxy wiki for more information on Phyloviz. (OPTIONAL) To access the hidden log file, click on the cog wheel in the upper right corner of the History panel and select \u201cInclude Hidden Datasets\u201d. Click on FastTree on data XX:log_FastTree_run.txt This is the log file from Fast tree and shows job related information. It is not required for the analysis workflow.","title":"FastTree"},{"location":"modules/metagenomics/metagenomics/#was-sampling-sufficient-rarefaction-analysis","text":"The aim here is to generate an intra-sample rarefaction curve to determine if the sampling effort was sufficient. Rarefaction curves provide a way of comparing the richness observed in different samples although it is a better measure of sample diversity. Briefly, it plots the average number of OTUs that you would expect to observe against the number of sequences. If the curve plateaus off, then it is a good indication that the sample is saturated and you have captured the majority (if not all) of species/strains present in the sample. However, if the curve does not plateau off, this suggests there may still be more species/strains that have not been captured in your data and you need increase the size of your sample.","title":"Was sampling sufficient? Rarefaction analysis"},{"location":"modules/metagenomics/metagenomics/#rarefaction-tool","text":"In the tool panel on the left hand side, search for the Rarefaction tool. For Input file format select rabund from the dropdown list For OTU list (rabund or sabund format) , select Map Reads to OTU on data XX and data XX:rabund For Minimum identity used in OTU clustering , enter the minimum identity used when you generated the OTU Table. In this tutorial we used 0.97. In case you can\u2019t remember the minimum identity used, go back to the output file Cluster OTU on data XX in your History. Click on the file name to extend the dataset information. Click on the \u201cView details\u201d icon. In the Input Parameter section you will find the \u201cMinimum cluster radius\u201d you used in the OTU Clustering step. Click Execute","title":"Rarefaction tool"},{"location":"modules/metagenomics/metagenomics/#output_5","text":"Click on Rarefaction on data XX . The output is a tabbed delimited file with the following columns: The first column (numsampled): the number sampled and indicates the level of sampling intensity (by default this information is provided every 100 individuals) The second column (0.03) is the average number of OTUs that were observed for that sampling intensity based on the number of iterations, which is 1000 by default. This would represents the data for the y-axis for the corresponding minimum identity (1-0.97=0.03). The third column (lci) and fourth column (hci) represent the confidence intervals. To plot the rarefaction curve: Click on Rarefaction on data XX to extend the dataset information. Click on the \u201cVisualize\u201d icon Select \u201cScatterplot\u201d Under Column 1, highlight X Column (the number sampled) Under Column 2, highlight Y Column (the average number of OTUs that were observed for that sampling intensity based on the number of iterations, which is 1,000 by default). Click Draw","title":"Output"},{"location":"modules/metagenomics/metagenomics/#estimate-diversity","text":"The aim here is to estimate the diversity and richness of the sample. Collector s curves describe how the richness or diversity change as you add more samples into the dataset. If a collector s curve plateaus, it is a good indication that enough samples were collected and you can have confidence in the last value of the curve.","title":"Estimate diversity"},{"location":"modules/metagenomics/metagenomics/#collectors-curve","text":"In the tool panel on the left hand side, search for the Collector Curve tool. For OTU list (rabund, sabund, list or shared format) , select Map Reads to OTU on data XX and data XX:rabund For Labels - OTU labels , Click Select and make sure that 0.03 appears in the box. This correspondd to 97%. Note that this value is the inverse of other similarity thresholds we used thus far (1-0.97=0.03) and is the default value. For Calculators , there is no need to change anything from the default calculators: chao - Community richness - the Chao estimator invsimpson - Community diversity - the Simpson index npshannon - Community diversity - the non-parametric Shannon index. Click Execute","title":"Collector's curve"},{"location":"modules/metagenomics/metagenomics/#output_6","text":"Click on Collector Curve on data XX:tab This file provides a summary containing the following fields: number of sequences, the sample coverage, the number of observed OTUs, and then a summary of each of the selected calculators that were applied to the sample. Click on Collector Curve on data XX:tab (invsimpson) The Simpson\u2019s Inverse Index also referred to as Simpson\u2019s Reciprocal Index is a measure of diversity. Species diversity is a way to compare samples that incorporates both a measure of evenness (representation by each species) and richness (number of each species in the sample). This index starts with the value of 1 as the lowest possible figure. This figure would represent a community containing only one species. The higher the value, the greater the diversity. The maximum value is the number of species in the sample. Click on Collector Curve on data XX:tab (npshannon) The non-parametric Shannon Index , like the Simpson\u2019s Inverse Index, is a diversity estimator and calculates a non-parametric estimate of the classical Shannon diversity Index for an OTU definition. The interpretation of the curve has to be performed in a comparison to the diversity curve of one or more communities. Click on Collector Curve on data XX:tab (chao) The Chao1 estimator is a species richness estimator. The species richness estimators estimate the total number of species present in a community. The Chao1 index estimator the species richness for one community based on the observed species frequencies or presence/absence data. If you compare the rarefaction curve (observed richness) and Chao1 estimator curve (estimated richness), you will observe a gap. The greater estimated richness in comparison to the observed richness usually indicates that with further sampling, the richness would likely continue to increase. Please note that if a sample contains many singletons, it is likely that more undetected OTUs exist, and the Chao1 index will estimate greater species richness than it would for a sample without rare OTUs. These three outputs are tabbed delimited with the following columns: First column (numsampled): is the number sampled and indicates the level of sampling intensity (by default this information is provided every 100 individuals) The second column (0.03) is the average number of OTUs that were observed for that sampling intensity based on the number of iterations, which is 1000 by default. This would represents the data for the y-axis for the corresponding minimum identity (1-0.97=0.03). The third column (lci) and fourth column (hci) represent the confidence intervals These files can be plotted similar to the rarefaction curve in the previous step.","title":"Output"},{"location":"modules/metagenomics/metagenomics/#plot-the-collectors-curves","text":"Click on Collector Curve on data XX:tab (invsimpson) to extend the dataset information. Click on the now accessible \u201cVisualize\u201d icon Select \u201cScatterplot\u201d Under Column 1, highlight X Column (the number sampled) Under Column 2, highlight Y Column (the average number of OTUs that were observed for that sampling intensity based on the number of iterations, which is 1,000 by default). Click Draw Repeat to plot the curves from the other calculators.","title":"Plot the collector\u2019s curves"},{"location":"modules/metagenomics/metagenomics/#the-workflow-canvas","text":"","title":"The workflow canvas"},{"location":"modules/metagenomics/metagenomics/#references","text":"Woese CR, Kandler O, Wheelis ML (1990) Towards a natural system of organisms: proposal for the domains Archaea, Bacteria, and Eucarya. Proc Natl Acad Sci U S A 87: 4576-4579. Lane DJ, Pace B, Olsen GJ, Stahl DA, Sogin ML, et al. (1985) Rapid determination of 16S ribosomal RNA sequences for phylogenetic analyses. Proc Natl Acad Sci U S A 82: 6955-6959. Kunin V, Copeland A, Lapidus A, Mavromatis K, Hugenholtz P (2008) A bioinformatician s guide to metagenomics. Microbiol Mol Biol Rev 72: 557-578. Handelsman J, Rondon MR, Brady SF, Clardy J, Goodman RM (1998) Molecular biological access to the chemistry of unknown soil microbes: a new frontier for natural products. Chem Biol 5: R245-249. Turnbaugh PJ, Ley RE, Hamady M, Fraser-Liggett CM, Knight R, et al. (2007) The human microbiome project. Nature 449: 804-810. Holmes S, Alekseyenko A, Timme A, Nelson T, Pasricha PJ, et al. (2011) Visualization and statistical comparisons of microbial communities using R packages on phylochip data. Pac Symp Biocomput: 142-153. Hamady M, Lozupone C, Knight R (2010) Fast UniFrac: facilitating high-throughput phylogenetic analyses of microbial communities including analysis of pyrosequencing and PhyloChip data. ISME J 4: 17-27. Brodie EL, Desantis TZ, Joyner DC, Baek SM, Larsen JT, et al. (2006) Application of a high-density oligonucleotide microarray approach to study bacterial population dynamics during uranium reduction and reoxidation. Appl Environ Microbiol 72: 6288-6298.","title":"References"},{"location":"modules/mlst/","text":"Using MLST for bacterial typing A bacterial type is a particular species with certain features. For example, the bacterial type may be Staphyloccus aureus with resistance to penicillin. To type a bacteria, we can focus on several genes, and see which allele is present for each of these genes. Each bacterial species will have its own scheme, which is the set of genes that are looked at. Overall, this process is called multi-locus (= several genes) sequence typing, or MLST. In this tutorial, we will perform MLST on a bacterial sequence. New to Galaxy? First try the introduction and then learn some key tasks Get data Log in to your Galaxy instance (for example, Galaxy Australia, usegalaxy.org.au ). If you are using Galaxy Australia, look for the file we need in the Shared Data . In the top centre panel, go to Shared Data Click Histories Click on the history named MLST-tutorial Click Import (in the top right corner) This should now be your current history. Alternatively , we can get the sequence from Genbank. In a new browser tab, go to this address: https://www.ncbi.nlm.nih.gov/nuccore/NC_002952.2 Click Send to Choose File Choose FASTA Click Create File This downloads sequence.fasta to your computer. In the tool panel, go to Get Data - Upload File Choose local file : select sequence.fasta Start Close This file should now be in your History panel. When it has completed uploading, the file name will be green. Run MLST In the tool panel search box, search for MLST. Click MLST The tool interface will appear in the centre Galaxy panel. For input_file , choose the sequence file that you uploaded, e.g. sequence.fasta Click Execute What does MLST do? Takes a fasta or genbank file Scans this against all the typing schemes Finds the best match (e.g. a scheme for the correct species) Reports the output: the scheme chosen the sequence type (e.g. ST = 21) the genes in this scheme and the alleles found in the data View results The results file will be at the top of your history panel. Click on the eye icon to view the output. Your results will be a single line. For example: Column 2: the scheme Column 3: the sequence type The rest of the columns: the gene names with the allele number in brackets. It is this combination of alleles that defines the sequence type. In this example, our sequence type is ST=36. This is methicillin-resistant Staphylococcus aureus (or MRSA). See this history in Galaxy If you want to see this Galaxy history without performing the steps above: Log in to Galaxy Australia: https://usegalaxy.org.au/ Go to Shared Data Click Histories Click Completed-MLST-analysis Click Import (at the top right corner) The MLST analysis should now be showing as your current history. What s next? To use the tutorials on this website: see the list in the left hand panel or, click the menu button (three horizontal bars) in the top left of the page You can find more tutorials at the Galaxy Training Network: http://galaxyproject.github.io/training-material/","title":"Using MLST"},{"location":"modules/mlst/#using-mlst-for-bacterial-typing","text":"A bacterial type is a particular species with certain features. For example, the bacterial type may be Staphyloccus aureus with resistance to penicillin. To type a bacteria, we can focus on several genes, and see which allele is present for each of these genes. Each bacterial species will have its own scheme, which is the set of genes that are looked at. Overall, this process is called multi-locus (= several genes) sequence typing, or MLST. In this tutorial, we will perform MLST on a bacterial sequence. New to Galaxy? First try the introduction and then learn some key tasks","title":"Using MLST for bacterial typing"},{"location":"modules/mlst/#get-data","text":"Log in to your Galaxy instance (for example, Galaxy Australia, usegalaxy.org.au ). If you are using Galaxy Australia, look for the file we need in the Shared Data . In the top centre panel, go to Shared Data Click Histories Click on the history named MLST-tutorial Click Import (in the top right corner) This should now be your current history. Alternatively , we can get the sequence from Genbank. In a new browser tab, go to this address: https://www.ncbi.nlm.nih.gov/nuccore/NC_002952.2 Click Send to Choose File Choose FASTA Click Create File This downloads sequence.fasta to your computer. In the tool panel, go to Get Data - Upload File Choose local file : select sequence.fasta Start Close This file should now be in your History panel. When it has completed uploading, the file name will be green.","title":"Get data"},{"location":"modules/mlst/#run-mlst","text":"In the tool panel search box, search for MLST. Click MLST The tool interface will appear in the centre Galaxy panel. For input_file , choose the sequence file that you uploaded, e.g. sequence.fasta Click Execute","title":"Run MLST"},{"location":"modules/mlst/#what-does-mlst-do","text":"Takes a fasta or genbank file Scans this against all the typing schemes Finds the best match (e.g. a scheme for the correct species) Reports the output: the scheme chosen the sequence type (e.g. ST = 21) the genes in this scheme and the alleles found in the data","title":"What does MLST do?"},{"location":"modules/mlst/#view-results","text":"The results file will be at the top of your history panel. Click on the eye icon to view the output. Your results will be a single line. For example: Column 2: the scheme Column 3: the sequence type The rest of the columns: the gene names with the allele number in brackets. It is this combination of alleles that defines the sequence type. In this example, our sequence type is ST=36. This is methicillin-resistant Staphylococcus aureus (or MRSA).","title":"View results"},{"location":"modules/mlst/#see-this-history-in-galaxy","text":"If you want to see this Galaxy history without performing the steps above: Log in to Galaxy Australia: https://usegalaxy.org.au/ Go to Shared Data Click Histories Click Completed-MLST-analysis Click Import (at the top right corner) The MLST analysis should now be showing as your current history.","title":"See this history in Galaxy"},{"location":"modules/mlst/#whats-next","text":"To use the tutorials on this website: see the list in the left hand panel or, click the menu button (three horizontal bars) in the top left of the page You can find more tutorials at the Galaxy Training Network: http://galaxyproject.github.io/training-material/","title":"What's next?"},{"location":"modules/nanopore/","text":"Nanopore data - bacterial genome assembly Base calling the reads, then canu, then Polish, or use racon Get data MinION data = Metrichor server = calls bases = fast5 files check the output from Metrichor we could assume we have fastq files, but we need original (fast5?) files to use in Nanopolish Enterobacter kobei from DOI: 10.1099/mgen.0.000085 ERR1341575 (MinION pass reads) - fastq Alternative data: E coli K-12 MG1655 from Loman http://www.nature.com.ezp.lib.unimelb.edu.au/nmeth/journal/v12/n8/full/nmeth.3444.html Assess data: poretools https://poretools.readthedocs.io/en/latest/ Convert fast5 to fastq poretools fastq fast5/ (is fast5 a dir?) plot histogram of read sizes poretools hist fast5/ how to viz? Assemble canu canu -p Ekobei -d Ekobei -genomeSize =?m -nanopore-raw fastq = 14 contigs, max 1.5m need fewer contigs. Polish nanopolish uses info from raw signal data (before they have been called as bases) http://simpsonlab.github.io/2015/03/30/optimizing-hmm/ https://github.com/jts/nanopolish this can polish the assembly Evaluate","title":"Nanopore data - bacterial genome assembly"},{"location":"modules/nanopore/#nanopore-data-bacterial-genome-assembly","text":"Base calling the reads, then canu, then Polish, or use racon","title":"Nanopore data - bacterial genome assembly"},{"location":"modules/nanopore/#get-data","text":"MinION data = Metrichor server = calls bases = fast5 files check the output from Metrichor we could assume we have fastq files, but we need original (fast5?) files to use in Nanopolish Enterobacter kobei from DOI: 10.1099/mgen.0.000085 ERR1341575 (MinION pass reads) - fastq Alternative data: E coli K-12 MG1655 from Loman http://www.nature.com.ezp.lib.unimelb.edu.au/nmeth/journal/v12/n8/full/nmeth.3444.html","title":"Get data"},{"location":"modules/nanopore/#assess-data-poretools","text":"https://poretools.readthedocs.io/en/latest/","title":"Assess data: poretools"},{"location":"modules/nanopore/#convert-fast5-to-fastq","text":"poretools fastq fast5/ (is fast5 a dir?)","title":"Convert fast5 to fastq"},{"location":"modules/nanopore/#plot-histogram-of-read-sizes","text":"poretools hist fast5/ how to viz?","title":"plot histogram of read sizes"},{"location":"modules/nanopore/#assemble","text":"canu canu -p Ekobei -d Ekobei -genomeSize =?m -nanopore-raw fastq = 14 contigs, max 1.5m need fewer contigs.","title":"Assemble"},{"location":"modules/nanopore/#polish","text":"nanopolish","title":"Polish"},{"location":"modules/nanopore/#uses-info-from-raw-signal-data-before-they-have-been-called-as-bases","text":"http://simpsonlab.github.io/2015/03/30/optimizing-hmm/ https://github.com/jts/nanopolish this can polish the assembly","title":"uses info from raw signal data (before they have been called as bases)"},{"location":"modules/nanopore/#evaluate","text":"","title":"Evaluate"},{"location":"modules/pacbio/","text":"Assembly with PacBio data and SMRT Portal Keywords: de novo assembly, PacBio, PacificBiosciences, HGAP, SMRT Portal, Microbial Genomics Virtual Laboratory This tutorial will show you how to assemble a bacterial genome de novo , using the PacBio SMRT Portal on the mGVL. We will use an analysis pipeline called HGAP, the Hierarchical Genome Assembly Process. Start Open your mGVL dashboard. You should see SMRT Portal as one of the instance services on your GVL dashboard. Open up the SMRT portal web link (to the right) and register/log on. Input We will use a dataset from a Streptococcus pyogenes bacteria. If this has already been loaded onto SMRT portal (e.g. for use during a workshop), proceed to the next step ( Assembly ). Otherwise: Load the PacBio data (your own, or the training dataset) onto your GVL. In the SMRT Portal, go to Design Job , the top left tab. Go to Import and Manage . Click Import SMRT cells . Work out where you put the data on your GVL, and make sure the file path is showing. If not, click Add and enter the file path to the data. A SMRT cell is the collection of data from a particular cell in the machine. It includes .bax.h5 files. Click on the file path and then Scan to check for new data. Assembly HGAP process overview We will use the Hierarchical Genome Assembly Process (HGAP). This flowchart shows the steps in the process: Set up job In the SMRT Portal, go to the top left tab, Design Job . Go to Create New . An Analysis window should appear. Tick all the boxes, then Next . Under Job Name enter a name. To the right, under Groups choose all . Under Protocols choose RS_HGAP_Assembly.3 . There is an ellipsis underneath Protocols - click on the ellipsis. This brings up the settings. Click on Assembly . For Compute Minimum Seed Read Length : ensure box is ticked For Number of Seed Read Chunks : enter 12 Change the Genome Size to an approximately correct size for the species. For S. pyogenes , enter 1800000. For Target Coverage : enter 10 For Overlapper Error Rate : enter 0.04 Leave all other settings as they are. Click Apply Your protocol window should look like this: Click Ok . In the SMRT Cells Available window, select the file to be used. Click on the arrow to transfer these files to the SMRT Cells in Job window. You can drag the column widths of the Url column so that you can see the URLs of the file paths better. Click Save (bottom right hand side). Next to Save , click Start . The Monitor Jobs window should open. As each step proceeds, new items will appear under the Reports and Data tabs on the left. Inputs and Outputs The connections between the names of assembly stages and outputs is not always clear. This flowchart shows how each stage of the HGAP process corresponds to protocol window names and outputs: Results If the job is still running, click on the centre tab Monitor Jobs . Otherwise, click on the top right tab, View Data . Double click on the job name to open its reports. Click on different Reports in the left hand panel. Things to look at: General: Filtering (polymerase reads) number of reads post-filter read length (=average) General: Subread Filtering (subreads) number of reads post-filter read length (average) Assembly: Pre-Assembly (pre-assembled reads) length cutoff (the computed minimum seed read length) read length (average) Assembly: Corrections Consensus calling results: Consensus concordance should be 99%. Graph: corrections across reference: With the first run of polishing, we expect a lot of corrections but they should be randomly distributed. Note: only unitigs 0 and 1 shown. Assembly: Top Corrections This is a list of all the corrections made. Note: only first 15 shown. Resequencing: Coverage Coverage across reference: discard contigs 20X coverage others should have fairly consistent coverage. spikes could be collapsed repeats. valleys could be mis-assembly - e.g. draft assembly was incorrect and so remapped reads didn t support this part of the assembly. Note: only unitigs 0 and 1 shown. Graph: Depth of Coverage: The is the number of reference regions vs. coverage. Assembly: Polished Assembly Number of contigs Max contig length Graph: confidence vs depth. Multi-copy plasmids may have higher coverage. Output The polished assembly as a FASTA file. download to local computer; or open file in (GVL) Galaxy; or open file in GVL command line and perform further analysis. Next Further options: Circularise the contigs; e.g. with Circlator. Polish the assembly using Illumina data; e.g. with Pilon. Annotate the polished assembly; e.g. with Prokka. Links to more information PacBio E. coli data set HGAP overview A full ist of reports and terminology Video overview of HGAP on SMRT portal More about the SMRT bell template","title":"Home"},{"location":"modules/pacbio/#assembly-with-pacbio-data-and-smrt-portal","text":"Keywords: de novo assembly, PacBio, PacificBiosciences, HGAP, SMRT Portal, Microbial Genomics Virtual Laboratory This tutorial will show you how to assemble a bacterial genome de novo , using the PacBio SMRT Portal on the mGVL. We will use an analysis pipeline called HGAP, the Hierarchical Genome Assembly Process.","title":"Assembly with PacBio data and SMRT Portal"},{"location":"modules/pacbio/#start","text":"Open your mGVL dashboard. You should see SMRT Portal as one of the instance services on your GVL dashboard. Open up the SMRT portal web link (to the right) and register/log on.","title":"Start"},{"location":"modules/pacbio/#input","text":"We will use a dataset from a Streptococcus pyogenes bacteria. If this has already been loaded onto SMRT portal (e.g. for use during a workshop), proceed to the next step ( Assembly ). Otherwise: Load the PacBio data (your own, or the training dataset) onto your GVL. In the SMRT Portal, go to Design Job , the top left tab. Go to Import and Manage . Click Import SMRT cells . Work out where you put the data on your GVL, and make sure the file path is showing. If not, click Add and enter the file path to the data. A SMRT cell is the collection of data from a particular cell in the machine. It includes .bax.h5 files. Click on the file path and then Scan to check for new data.","title":"Input"},{"location":"modules/pacbio/#assembly","text":"","title":"Assembly"},{"location":"modules/pacbio/#hgap-process-overview","text":"We will use the Hierarchical Genome Assembly Process (HGAP). This flowchart shows the steps in the process:","title":"HGAP process overview"},{"location":"modules/pacbio/#set-up-job","text":"In the SMRT Portal, go to the top left tab, Design Job . Go to Create New . An Analysis window should appear. Tick all the boxes, then Next . Under Job Name enter a name. To the right, under Groups choose all . Under Protocols choose RS_HGAP_Assembly.3 . There is an ellipsis underneath Protocols - click on the ellipsis. This brings up the settings. Click on Assembly . For Compute Minimum Seed Read Length : ensure box is ticked For Number of Seed Read Chunks : enter 12 Change the Genome Size to an approximately correct size for the species. For S. pyogenes , enter 1800000. For Target Coverage : enter 10 For Overlapper Error Rate : enter 0.04 Leave all other settings as they are. Click Apply Your protocol window should look like this: Click Ok . In the SMRT Cells Available window, select the file to be used. Click on the arrow to transfer these files to the SMRT Cells in Job window. You can drag the column widths of the Url column so that you can see the URLs of the file paths better. Click Save (bottom right hand side). Next to Save , click Start . The Monitor Jobs window should open. As each step proceeds, new items will appear under the Reports and Data tabs on the left.","title":"Set up job"},{"location":"modules/pacbio/#inputs-and-outputs","text":"The connections between the names of assembly stages and outputs is not always clear. This flowchart shows how each stage of the HGAP process corresponds to protocol window names and outputs:","title":"Inputs and Outputs"},{"location":"modules/pacbio/#results","text":"If the job is still running, click on the centre tab Monitor Jobs . Otherwise, click on the top right tab, View Data . Double click on the job name to open its reports. Click on different Reports in the left hand panel. Things to look at: General: Filtering (polymerase reads) number of reads post-filter read length (=average) General: Subread Filtering (subreads) number of reads post-filter read length (average) Assembly: Pre-Assembly (pre-assembled reads) length cutoff (the computed minimum seed read length) read length (average) Assembly: Corrections Consensus calling results: Consensus concordance should be 99%. Graph: corrections across reference: With the first run of polishing, we expect a lot of corrections but they should be randomly distributed. Note: only unitigs 0 and 1 shown. Assembly: Top Corrections This is a list of all the corrections made. Note: only first 15 shown. Resequencing: Coverage Coverage across reference: discard contigs 20X coverage others should have fairly consistent coverage. spikes could be collapsed repeats. valleys could be mis-assembly - e.g. draft assembly was incorrect and so remapped reads didn t support this part of the assembly. Note: only unitigs 0 and 1 shown. Graph: Depth of Coverage: The is the number of reference regions vs. coverage. Assembly: Polished Assembly Number of contigs Max contig length Graph: confidence vs depth. Multi-copy plasmids may have higher coverage.","title":"Results"},{"location":"modules/pacbio/#output","text":"The polished assembly as a FASTA file. download to local computer; or open file in (GVL) Galaxy; or open file in GVL command line and perform further analysis.","title":"Output"},{"location":"modules/pacbio/#next","text":"Further options: Circularise the contigs; e.g. with Circlator. Polish the assembly using Illumina data; e.g. with Pilon. Annotate the polished assembly; e.g. with Prokka.","title":"Next"},{"location":"modules/pacbio/#links-to-more-information","text":"PacBio E. coli data set HGAP overview A full ist of reports and terminology Video overview of HGAP on SMRT portal More about the SMRT bell template","title":"Links to more information"},{"location":"modules/pathway_tools/annotation/","text":"Pathway prediction and annotations for new organisms Connect to mGVL using VNC Go to the mGVL dashboard Click on the link next to the Lubuntu Destkop (http://your-mgvl-ip-address/vnc) A new browser will appear, enter your user credentials to login Create a new Database Once logged in, you should see the following desktop with the 3 Pathway tools icons on the left. Click on the first icon Pathway Tools v1.9.5 . This will bring up a new window. In the Pathway Tools window, click on Tools menu item and then click on PathLogic In the new window, at the top, click on Database and then Create New Provide the metadata for the new database. In the Database(required) section, enter the following values: Organism/Project ID: SEPSIS25707 Database Name: SepsisCycl Leave the other parameters as the default values, that is: Version: 1.0 (default) DB Storage Type: File (default) Under the Taxonomy (required) section: Check Box if this is a multi-organism database: uncheck In Organism taxonomic class , type 1314 and click on Select . The species name of Streptococcus pyogenes will appear and a popup window will also appear. Click on to close the window. Create organism?: click yes Strain: HKU419(ARP) for sample (25707) Genome Source: ARP NCBI taxonomy ID: 1314 (come back to this after Select Strain in next step) Rank : Select Strain Leave all other fields as their default values: Full Species Name: autocomplete from the previous step Abbreviated Species Name: auto complete from the previous step Subspecies : leave it blank Default Codon Table: 11 - Bacterial and Plant Plastid Mitochondrial Codon Table : 0 - Unspecified Leave Credits(optional) section black. Click OK . Another window will appear while processing. Wait for processing to complete, until you see the next window. Click on Enter Replicon Editor . Specify Replicon details In this view, you provide the details of the annotated assembly. Each chromosome will have a separate entry. For each chromosome, you need to provide the GBK and FNA annotation files from Prokka. Name : 1 ( This is the chromosome name or number) Circular: checked if circulator was performed or leave it unchecked Select annotation file: select the gbk from prokka Select sequence file: select the fna from prokka Leave other fields as their default value: Type: Chromosome (default) Code: Bacterial, Archaeal and Plant Plastid (default) ID: leave it blank (default) Links to other database: NCBI Reference Sequences Database (default) Relationship : same Entity (default) Click OK The metada for this database has now been created. Next we need to predict the pathways and annotate them. Predict and annotate the pathways Still on the previous window, from the top menu bar, click on Build and then Automated Build . This will take a while depending on the number of replicons that was included for this database. Once the process has completed, a new window, Pathway Scoring Parameters will appear. Leave the values as their default values. Taxonomic Pruning: Enabled (default) Pathway Prediction Score Cutoff: 0.15 (default). A higher cutoff value for the prediction score will mean less pathways are predicted. This is the level of stringency imposed on the prediction. Click OK . Again the process can take several minutes depending on the number of replicons included. Wait until the process is complete. Click on Database and then Save DB from the menu. The pathways have now been predicted, annotated and stored in the database. Verify and test the new Database Now that the database has been created, we need to verify that it is available for use. First we check that it is present in the Pathway Tools. On the main screen, Pathway Tools - Available Databases , you should see a new entry at the bottom of the pre-existing list, Streptococcus pyogenes HKU419(ARP) . To see this newly created database in the Pathway Tools web-application, start up the Pathway Tools Web service (if not already running). You can close all the windows, to exit Pathway Tools If the web server is not already running, click on the Pathway Tool Web server from the desktop, this is the third icon. Open an internet brwoser and go to http://your-mgvl-ip-address:1555 Using the new database in Pathway Tools Web service On the main screen, click on change organism database at the top right corner, under the search box. A new window will appear, select the newly created species from the list Click OK A statistic table of the species will be available for overview. Click on the Pathways link in the table, which will show the list of all predicted pathways in this database. You can navigate through the list of pathways and highlight a pathway of interest. For example, Click on the + symbol next to Detoxification Click on the + symbol again next to Antibiotic Resistance Click on peptidoglycan biosynthesis V (beta-lactam resistance) which will bring up the pathway view Go back to the previous screen with the statistics summary table and this time click on the 1 under the Replicon heading in the top table. The following genome browser will be shown. From here forward the user can explore using the Pathway Tools webservice.","title":"Home"},{"location":"modules/pathway_tools/annotation/#pathway-prediction-and-annotations-for-new-organisms","text":"","title":"Pathway prediction and annotations for new organisms"},{"location":"modules/pathway_tools/annotation/#connect-to-mgvl-using-vnc","text":"Go to the mGVL dashboard Click on the link next to the Lubuntu Destkop (http://your-mgvl-ip-address/vnc) A new browser will appear, enter your user credentials to login","title":"Connect to mGVL using VNC"},{"location":"modules/pathway_tools/annotation/#create-a-new-database","text":"Once logged in, you should see the following desktop with the 3 Pathway tools icons on the left. Click on the first icon Pathway Tools v1.9.5 . This will bring up a new window. In the Pathway Tools window, click on Tools menu item and then click on PathLogic In the new window, at the top, click on Database and then Create New Provide the metadata for the new database. In the Database(required) section, enter the following values: Organism/Project ID: SEPSIS25707 Database Name: SepsisCycl Leave the other parameters as the default values, that is: Version: 1.0 (default) DB Storage Type: File (default) Under the Taxonomy (required) section: Check Box if this is a multi-organism database: uncheck In Organism taxonomic class , type 1314 and click on Select . The species name of Streptococcus pyogenes will appear and a popup window will also appear. Click on to close the window. Create organism?: click yes Strain: HKU419(ARP) for sample (25707) Genome Source: ARP NCBI taxonomy ID: 1314 (come back to this after Select Strain in next step) Rank : Select Strain Leave all other fields as their default values: Full Species Name: autocomplete from the previous step Abbreviated Species Name: auto complete from the previous step Subspecies : leave it blank Default Codon Table: 11 - Bacterial and Plant Plastid Mitochondrial Codon Table : 0 - Unspecified Leave Credits(optional) section black. Click OK . Another window will appear while processing. Wait for processing to complete, until you see the next window. Click on Enter Replicon Editor .","title":"Create a new Database"},{"location":"modules/pathway_tools/annotation/#specify-replicon-details","text":"In this view, you provide the details of the annotated assembly. Each chromosome will have a separate entry. For each chromosome, you need to provide the GBK and FNA annotation files from Prokka. Name : 1 ( This is the chromosome name or number) Circular: checked if circulator was performed or leave it unchecked Select annotation file: select the gbk from prokka Select sequence file: select the fna from prokka Leave other fields as their default value: Type: Chromosome (default) Code: Bacterial, Archaeal and Plant Plastid (default) ID: leave it blank (default) Links to other database: NCBI Reference Sequences Database (default) Relationship : same Entity (default) Click OK The metada for this database has now been created. Next we need to predict the pathways and annotate them.","title":"Specify Replicon details"},{"location":"modules/pathway_tools/annotation/#predict-and-annotate-the-pathways","text":"Still on the previous window, from the top menu bar, click on Build and then Automated Build . This will take a while depending on the number of replicons that was included for this database. Once the process has completed, a new window, Pathway Scoring Parameters will appear. Leave the values as their default values. Taxonomic Pruning: Enabled (default) Pathway Prediction Score Cutoff: 0.15 (default). A higher cutoff value for the prediction score will mean less pathways are predicted. This is the level of stringency imposed on the prediction. Click OK . Again the process can take several minutes depending on the number of replicons included. Wait until the process is complete. Click on Database and then Save DB from the menu. The pathways have now been predicted, annotated and stored in the database.","title":"Predict and annotate the pathways"},{"location":"modules/pathway_tools/annotation/#verify-and-test-the-new-database","text":"Now that the database has been created, we need to verify that it is available for use. First we check that it is present in the Pathway Tools. On the main screen, Pathway Tools - Available Databases , you should see a new entry at the bottom of the pre-existing list, Streptococcus pyogenes HKU419(ARP) . To see this newly created database in the Pathway Tools web-application, start up the Pathway Tools Web service (if not already running). You can close all the windows, to exit Pathway Tools If the web server is not already running, click on the Pathway Tool Web server from the desktop, this is the third icon. Open an internet brwoser and go to http://your-mgvl-ip-address:1555","title":"Verify and test the new Database"},{"location":"modules/pathway_tools/annotation/#using-the-new-database-in-pathway-tools-web-service","text":"On the main screen, click on change organism database at the top right corner, under the search box. A new window will appear, select the newly created species from the list Click OK A statistic table of the species will be available for overview. Click on the Pathways link in the table, which will show the list of all predicted pathways in this database. You can navigate through the list of pathways and highlight a pathway of interest. For example, Click on the + symbol next to Detoxification Click on the + symbol again next to Antibiotic Resistance Click on peptidoglycan biosynthesis V (beta-lactam resistance) which will bring up the pathway view Go back to the previous screen with the statistics summary table and this time click on the 1 under the Replicon heading in the top table. The following genome browser will be shown. From here forward the user can explore using the Pathway Tools webservice.","title":"Using the new database in Pathway Tools Web service"},{"location":"modules/pear/","text":"Pear Pear is a tool to merge paired-end sequencing reads, prior to downstream tasks such as assembly. Get data Input: paired-end reads. We will use a set of Illumina MiSeq reads from the bacteria Staphylococcus aureus . Go to your Galaxy server. In the tool panel, go to Get Data: Upload File Select Paste/Fetch data In the box, paste in: ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR171/008/ERR1712338/ERR1712338_2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR171/008/ERR1712338/ERR1712338_1.fastq.gz Click Start and then Close . These two files will upload to your current Galaxy history. Using the pencil icon, change the filetype to fastqsanger , and shorten the name of the file. Run Pear In the tool panel, go to NGS Analysis: NGS QC and manipulation: Pear Dataset type : Paired-end Name of file that contains the forward paired-end reads : ERR1712338_1.fastq Name of file that contains the reverse paired-end reads : ERR1712338_2.fastq Leave other settings as per defaults, except: Maximal proportion of uncalled bases in a read : 0.01 omits reads if 1% of the reads is missing (N) Output files : Select all Your tool interface should look like this: Click Execute Results There are four output files. Assembled reads : merged paired-end reads. Unassembled forward reads and Unassembled reverse reads : remaining, unmerged reads. Discarded reads : Did not meet quality specified In this case, most of the reads have been merged (~360MB); 90MB are unmerged, and 350 sequences have been discarded. Next Run Trimmomatic to trim sequences before assembling. Links Pear paper Pear software","title":"Pear"},{"location":"modules/pear/#pear","text":"Pear is a tool to merge paired-end sequencing reads, prior to downstream tasks such as assembly.","title":"Pear"},{"location":"modules/pear/#get-data","text":"Input: paired-end reads. We will use a set of Illumina MiSeq reads from the bacteria Staphylococcus aureus . Go to your Galaxy server. In the tool panel, go to Get Data: Upload File Select Paste/Fetch data In the box, paste in: ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR171/008/ERR1712338/ERR1712338_2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR171/008/ERR1712338/ERR1712338_1.fastq.gz Click Start and then Close . These two files will upload to your current Galaxy history. Using the pencil icon, change the filetype to fastqsanger , and shorten the name of the file.","title":"Get data"},{"location":"modules/pear/#run-pear","text":"In the tool panel, go to NGS Analysis: NGS QC and manipulation: Pear Dataset type : Paired-end Name of file that contains the forward paired-end reads : ERR1712338_1.fastq Name of file that contains the reverse paired-end reads : ERR1712338_2.fastq Leave other settings as per defaults, except: Maximal proportion of uncalled bases in a read : 0.01 omits reads if 1% of the reads is missing (N) Output files : Select all Your tool interface should look like this: Click Execute","title":"Run Pear"},{"location":"modules/pear/#results","text":"There are four output files. Assembled reads : merged paired-end reads. Unassembled forward reads and Unassembled reverse reads : remaining, unmerged reads. Discarded reads : Did not meet quality specified In this case, most of the reads have been merged (~360MB); 90MB are unmerged, and 350 sequences have been discarded.","title":"Results"},{"location":"modules/pear/#next","text":"Run Trimmomatic to trim sequences before assembling.","title":"Next"},{"location":"modules/pear/#links","text":"Pear paper Pear software","title":"Links"},{"location":"modules/prokka/","text":"Genome annotation using Prokka Background In this section we will use a software tool called Prokka to annotate a draft genome sequence. Prokka is a \u201cwrapper\u201d; it collects together several pieces of software (from various authors), and so avoids \u201cre-inventing the wheel\u201d. Prokka finds and annotates features (both protein coding regions and RNA genes, i.e. tRNA, rRNA) present on on a sequence. Note, Prokka uses a two-step process for the annotation of protein coding regions: first, protein coding regions on the genome are identified using Prodigal ; second, the function of the encoded protein is predicted by similarity to proteins in one of many protein or protein domain databases. Prokka is a software tool that can be used to annotate bacterial, archaeal and viral genomes quickly, generating standard output files in GenBank, EMBL and gff formats. More information about Prokka can be found here . New to Galaxy? First try the introduction and then learn some key tasks Input data Log in to your Galaxy instance (for example, Galaxy Australia, usegalaxy.org.au ). Prokka requires assembled contigs. If you are continuing on from the previous workshop ( Assembly with Spades ), this file will be in your current history named something like SPAdes contigs(fasta) . Or, to upload a file of contigs: Use shared data If you are using Galaxy Australia, you can import the data from a shared data library. In the top menu bar, go to Shared Data . Click on Data Libraries . Click on Galaxy Australia Training Material: Annotation: Microbial Annotation . Tick the box next to the file. Click the To History button, select As Datasets . Name a new history and click Import . In the top menu bar, click Analyze Data . You should now have one file in your current history. Or, import from the web Only follow this step if unable to load the data files from shared data, as described above . In a new browser tab, go to this webpage: Find the file called contigs.fasta Right click on file name: select copy link address In Galaxy, go to Get Data and then Upload File Click Paste/Fetch data A box will appear: paste in link address Click Start Click Close The file will now appear in the top of your history panel. When uploaded, the file name will turn green. Shorten file name Click on the pencil icon next to the file name. In the centre Galaxy panel, click in the box under Name Shorten the file name. Then click Save Run Prokka Go to the Tool panel and search for prokka in the search box. Click on Prokka For Contigs to annotate : your contigs.fasta file All the other settings can be left as they are. Your tool interface should look like this: Click Execute How do I choose settings when running a tool? In this case, the default settings are appropriate for our data set and analysis. Under the tool interface in Galaxy there will usually be a more detailed description of the tool options, and a link to the tool s documentation. It is recommended that you read about the tool parameters in more detail in the documentation, and adjust to your data and analysis accordingly. Examine the output Once Prokka has finished, examine each of its output files. The GFF and GBK files contain all of the information about the features annotated (in different formats.) The .txt file contains a summary of the number of features annotated. The .faa file contains the protein sequences of the genes annotated. The .ffn file contains the nucleotide sequences of the genes annotated. View annotated features in JBrowse Now that we have annotated the draft genome sequence, we would like to view the sequence in the JBrowse genome viewer. Go to the Galaxy tool panel, and use the top search box to search for JBrowse . Click JBrowse Leave most settings as they are, except for: Under Reference genome to display choose Use a genome from history . Under Select the reference genome choose Prokka on data XX:fna . This .fna sequence is the fasta nucleotide sequence, and will be the reference against which annotations are displayed. For Genetic Code choose 11: The Bacterial, Archaeal and Plant Plastid Code . Click Insert Track Group Under Track Category type in gene annotations . Click Insert Annotation Track For Track Type choose GFF/GFF3/BED/GBK Features For GFF/GFF3/BED Track Data select Prokka on data XX:gff [Note: not wildtype.gff] Your tool interface should look like this: Click Execute JBrowse output A new file will be created, called JBrowse on data XX and data XX - Complete . In place of XX , there will be numbers that will refer to the files that Galaxy used in your particular history. Click on the eye icon next to the file name. The JBrowse window will appear in the centre Galaxy panel. Under Available Tracks on the left, tick the box for Prokka on data XX:gff . Select a contig to view, in the drop down box. You can only see one contig displayed at a time. Choose the longest contig. Use the plus and minus buttons to zoom in and out, and the arrows to move left or right (or click and drag within the window to move left or right). Zoom in to see the reference sequence at the top. JBrowse displays the sequence and a 6-frame amino acid translation. Zoomed in view: Right click on a gene/feature annotation (the bars on the annotation track), then select View Details to see more information. gene name product name you can download the FASTA sequence by clicking on the disk icon. Extension exercise Open the JBrowse file that you created to show the gene annotations. In the drop-down box, select the longest contig. Find an annotation for a hypothetical protein . Alternatively, in the coordinates box, enter 32,500 . Zoom out; see the annotation that spans this location. It is a hypothetical protein . Click on this annotation. See the nucleotide sequence: select and copy. Go to the NCBI page and BLAST this sequence to see what it matches. Try the blastx option, which will translate your nucleotide sequence into a protein sequence. For Enter Query Sequence , paste your sequence into the box. For Genetic Code choose Bacteria and Archaea . For Database , try the SwissProt database. You can also re-try with other options to see how the database affects the results. All other options can be left as default. Click BLAST . What does your sequence match? In our example, the top hit is to a carboxylase enzyme. In the Show Conserved Domains window, we can see that this is part of a superfamily of metallo-dependent hydrolases. Click through to find out more about this superfamily. Genome annotation is constantly refined. We can see here that some manual investigation gave us more information about an annotation. This would need to be experimentally confirmed. For a detailed description of the BLAST output, see the top right corner of the page and click Blast report description . See this history in Galaxy If you want to see this Galaxy history without performing the steps above: Log in to Galaxy Australia: https://usegalaxy.org.au/ Go to Shared Data Click Histories Click Completed-annotation-analysis Click Import (at the top right corner) The analysis should now be showing as your current history. More information Here are some references covering more information about genome annotation. Prokaryote genome annotation: Seemann T. Prokka: rapid prokaryotic genome annotation. Bioinformatics. 2014 Jul 15;30(14):2068\u20139. Eukaryote genome annotation: Yandell M, Ence D. A beginner\u2019s guide to eukaryotic genome annotation. Nat Rev Genet. 2012 Apr 18;13(5):329\u201342. Wheat genome annotation: Clavijo BJ, et al. An improved assembly and annotation of the allohexaploid wheat genome identifies complete families of agronomic genes and provides genomic evidence for chromosomal translocations. Genome Res. 2017 May;27(5):885\u201396. Human genome annotation: Harrow J, et al. GENCODE: the reference human genome annotation for The ENCODE Project. Genome Res. 2012 Sep;22(9):1760\u201374. Annotation in the clinical setting: Steward CA, Parker APJ, Minassian BA, Sisodiya SM, Frankish A, Harrow J. Genome annotation for clinical genomic diagnostics: strengths and weaknesses. Genome Med. 2017 May 30;9(1):49. Assessing your annotation: Sim\u00e3o FA, Waterhouse RM, Ioannidis P, Kriventseva EV, Zdobnov EM. BUSCO: assessing genome assembly and annotation completeness with single-copy orthologs. Bioinformatics. 2015 Oct 1;31(19):3210\u20132. Comparative genome annotation: K\u00f6nig S, Romoth L, Stanke M. Comparative Genome Annotation. Methods Mol Biol. 2018;1704:189\u2013212. What s next? To use the tutorials on this website: see the list in the left hand panel or, click the menu button (three horizontal bars) in the top left of the page You can find more tutorials at the Galaxy Training Network: http://galaxyproject.github.io/training-material/","title":"Genome annotation"},{"location":"modules/prokka/#genome-annotation-using-prokka","text":"","title":"Genome annotation using Prokka"},{"location":"modules/prokka/#background","text":"In this section we will use a software tool called Prokka to annotate a draft genome sequence. Prokka is a \u201cwrapper\u201d; it collects together several pieces of software (from various authors), and so avoids \u201cre-inventing the wheel\u201d. Prokka finds and annotates features (both protein coding regions and RNA genes, i.e. tRNA, rRNA) present on on a sequence. Note, Prokka uses a two-step process for the annotation of protein coding regions: first, protein coding regions on the genome are identified using Prodigal ; second, the function of the encoded protein is predicted by similarity to proteins in one of many protein or protein domain databases. Prokka is a software tool that can be used to annotate bacterial, archaeal and viral genomes quickly, generating standard output files in GenBank, EMBL and gff formats. More information about Prokka can be found here . New to Galaxy? First try the introduction and then learn some key tasks","title":"Background"},{"location":"modules/prokka/#input-data","text":"Log in to your Galaxy instance (for example, Galaxy Australia, usegalaxy.org.au ). Prokka requires assembled contigs. If you are continuing on from the previous workshop ( Assembly with Spades ), this file will be in your current history named something like SPAdes contigs(fasta) . Or, to upload a file of contigs:","title":"Input data"},{"location":"modules/prokka/#use-shared-data","text":"If you are using Galaxy Australia, you can import the data from a shared data library. In the top menu bar, go to Shared Data . Click on Data Libraries . Click on Galaxy Australia Training Material: Annotation: Microbial Annotation . Tick the box next to the file. Click the To History button, select As Datasets . Name a new history and click Import . In the top menu bar, click Analyze Data . You should now have one file in your current history.","title":"Use shared data"},{"location":"modules/prokka/#or-import-from-the-web","text":"Only follow this step if unable to load the data files from shared data, as described above . In a new browser tab, go to this webpage: Find the file called contigs.fasta Right click on file name: select copy link address In Galaxy, go to Get Data and then Upload File Click Paste/Fetch data A box will appear: paste in link address Click Start Click Close The file will now appear in the top of your history panel. When uploaded, the file name will turn green.","title":"Or, import from the web"},{"location":"modules/prokka/#shorten-file-name","text":"Click on the pencil icon next to the file name. In the centre Galaxy panel, click in the box under Name Shorten the file name. Then click Save","title":"Shorten file name"},{"location":"modules/prokka/#run-prokka","text":"Go to the Tool panel and search for prokka in the search box. Click on Prokka For Contigs to annotate : your contigs.fasta file All the other settings can be left as they are. Your tool interface should look like this: Click Execute","title":"Run Prokka"},{"location":"modules/prokka/#how-do-i-choose-settings-when-running-a-tool","text":"In this case, the default settings are appropriate for our data set and analysis. Under the tool interface in Galaxy there will usually be a more detailed description of the tool options, and a link to the tool s documentation. It is recommended that you read about the tool parameters in more detail in the documentation, and adjust to your data and analysis accordingly.","title":"How do I choose settings when running a tool?"},{"location":"modules/prokka/#examine-the-output","text":"Once Prokka has finished, examine each of its output files. The GFF and GBK files contain all of the information about the features annotated (in different formats.) The .txt file contains a summary of the number of features annotated. The .faa file contains the protein sequences of the genes annotated. The .ffn file contains the nucleotide sequences of the genes annotated.","title":"Examine the output"},{"location":"modules/prokka/#view-annotated-features-in-jbrowse","text":"Now that we have annotated the draft genome sequence, we would like to view the sequence in the JBrowse genome viewer. Go to the Galaxy tool panel, and use the top search box to search for JBrowse . Click JBrowse Leave most settings as they are, except for: Under Reference genome to display choose Use a genome from history . Under Select the reference genome choose Prokka on data XX:fna . This .fna sequence is the fasta nucleotide sequence, and will be the reference against which annotations are displayed. For Genetic Code choose 11: The Bacterial, Archaeal and Plant Plastid Code . Click Insert Track Group Under Track Category type in gene annotations . Click Insert Annotation Track For Track Type choose GFF/GFF3/BED/GBK Features For GFF/GFF3/BED Track Data select Prokka on data XX:gff [Note: not wildtype.gff] Your tool interface should look like this: Click Execute","title":"View annotated features in JBrowse"},{"location":"modules/prokka/#jbrowse-output","text":"A new file will be created, called JBrowse on data XX and data XX - Complete . In place of XX , there will be numbers that will refer to the files that Galaxy used in your particular history. Click on the eye icon next to the file name. The JBrowse window will appear in the centre Galaxy panel. Under Available Tracks on the left, tick the box for Prokka on data XX:gff . Select a contig to view, in the drop down box. You can only see one contig displayed at a time. Choose the longest contig. Use the plus and minus buttons to zoom in and out, and the arrows to move left or right (or click and drag within the window to move left or right). Zoom in to see the reference sequence at the top. JBrowse displays the sequence and a 6-frame amino acid translation. Zoomed in view: Right click on a gene/feature annotation (the bars on the annotation track), then select View Details to see more information. gene name product name you can download the FASTA sequence by clicking on the disk icon.","title":"JBrowse output"},{"location":"modules/prokka/#extension-exercise","text":"Open the JBrowse file that you created to show the gene annotations. In the drop-down box, select the longest contig. Find an annotation for a hypothetical protein . Alternatively, in the coordinates box, enter 32,500 . Zoom out; see the annotation that spans this location. It is a hypothetical protein . Click on this annotation. See the nucleotide sequence: select and copy. Go to the NCBI page and BLAST this sequence to see what it matches. Try the blastx option, which will translate your nucleotide sequence into a protein sequence. For Enter Query Sequence , paste your sequence into the box. For Genetic Code choose Bacteria and Archaea . For Database , try the SwissProt database. You can also re-try with other options to see how the database affects the results. All other options can be left as default. Click BLAST . What does your sequence match? In our example, the top hit is to a carboxylase enzyme. In the Show Conserved Domains window, we can see that this is part of a superfamily of metallo-dependent hydrolases. Click through to find out more about this superfamily. Genome annotation is constantly refined. We can see here that some manual investigation gave us more information about an annotation. This would need to be experimentally confirmed. For a detailed description of the BLAST output, see the top right corner of the page and click Blast report description .","title":"Extension exercise"},{"location":"modules/prokka/#see-this-history-in-galaxy","text":"If you want to see this Galaxy history without performing the steps above: Log in to Galaxy Australia: https://usegalaxy.org.au/ Go to Shared Data Click Histories Click Completed-annotation-analysis Click Import (at the top right corner) The analysis should now be showing as your current history.","title":"See this history in Galaxy"},{"location":"modules/prokka/#more-information","text":"Here are some references covering more information about genome annotation. Prokaryote genome annotation: Seemann T. Prokka: rapid prokaryotic genome annotation. Bioinformatics. 2014 Jul 15;30(14):2068\u20139. Eukaryote genome annotation: Yandell M, Ence D. A beginner\u2019s guide to eukaryotic genome annotation. Nat Rev Genet. 2012 Apr 18;13(5):329\u201342. Wheat genome annotation: Clavijo BJ, et al. An improved assembly and annotation of the allohexaploid wheat genome identifies complete families of agronomic genes and provides genomic evidence for chromosomal translocations. Genome Res. 2017 May;27(5):885\u201396. Human genome annotation: Harrow J, et al. GENCODE: the reference human genome annotation for The ENCODE Project. Genome Res. 2012 Sep;22(9):1760\u201374. Annotation in the clinical setting: Steward CA, Parker APJ, Minassian BA, Sisodiya SM, Frankish A, Harrow J. Genome annotation for clinical genomic diagnostics: strengths and weaknesses. Genome Med. 2017 May 30;9(1):49. Assessing your annotation: Sim\u00e3o FA, Waterhouse RM, Ioannidis P, Kriventseva EV, Zdobnov EM. BUSCO: assessing genome assembly and annotation completeness with single-copy orthologs. Bioinformatics. 2015 Oct 1;31(19):3210\u20132. Comparative genome annotation: K\u00f6nig S, Romoth L, Stanke M. Comparative Genome Annotation. Methods Mol Biol. 2018;1704:189\u2013212.","title":"More information"},{"location":"modules/prokka/#whats-next","text":"To use the tutorials on this website: see the list in the left hand panel or, click the menu button (three horizontal bars) in the top left of the page You can find more tutorials at the Galaxy Training Network: http://galaxyproject.github.io/training-material/","title":"What's next?"},{"location":"modules/refseq/","text":"Reference genomes How to get a reference genome into Galaxy? In a web browser, go to https://www.ncbi.nlm.nih.gov/assembly/ Enter the taxon into the search bar. For example, enter Mycoplasma synoviae Choose the assembly you want, for example, ASM314756v1 Under the Assembly Definition , click on the GenBank sequence ID, for example, CP021129.1 In the top right, click on Send to For Choose Destination select File For Format choose FASTA Click Create File The file will download locally. In Galaxy, go to Get Data and choose Upload File . Click Choose local file Select the file. Click Start Click Close You should now have the file in the top of your current history. Re-name it, e.g. to Mycoplasma_reference.fasta How to convert this into other formats? Tools in Galaxy may require various formats for your reference genome. For bacterial genomes, an easy way to get the required formats is to use the tool Prokka . Go to the Tool panel and search for prokka in the search box. Click on Prokka For Contigs to annotate : your reference.fasta file All the other settings can be left as they are. Click Execute Prokka outputs lots of different files including a genbank, fasta and gff version of your input fasta file.","title":"Reference genomes"},{"location":"modules/refseq/#reference-genomes","text":"How to get a reference genome into Galaxy? In a web browser, go to https://www.ncbi.nlm.nih.gov/assembly/ Enter the taxon into the search bar. For example, enter Mycoplasma synoviae Choose the assembly you want, for example, ASM314756v1 Under the Assembly Definition , click on the GenBank sequence ID, for example, CP021129.1 In the top right, click on Send to For Choose Destination select File For Format choose FASTA Click Create File The file will download locally. In Galaxy, go to Get Data and choose Upload File . Click Choose local file Select the file. Click Start Click Close You should now have the file in the top of your current history. Re-name it, e.g. to Mycoplasma_reference.fasta How to convert this into other formats? Tools in Galaxy may require various formats for your reference genome. For bacterial genomes, an easy way to get the required formats is to use the tool Prokka . Go to the Tool panel and search for prokka in the search box. Click on Prokka For Contigs to annotate : your reference.fasta file All the other settings can be left as they are. Click Execute Prokka outputs lots of different files including a genbank, fasta and gff version of your input fasta file.","title":"Reference genomes"},{"location":"modules/resources/","text":"Resources Under development More training material for Galaxy Galaxy Training Network Melbourne Bioinformatics GOBLET I have a problem in Galaxy For Galaxy Australia, email [add email] Can I attend a Galaxy workshop? Yes: please search the EMBL-ABR Training page: http://www.embl-abr.org.au/about/events/ Can I get this new tool into Galaxy? Possibly: instructions/email Can I use a different Galaxy server? Yes: There are many public Galaxy servers, listed here: https://galaxyproject.org/public-galaxy-servers/ Can I set up my own Galaxy server? Yes: instructions I have a suggestion for a new tutorial topic Thank you: please email anna.syme@unimelb.edu.au","title":"Resources"},{"location":"modules/resources/#resources","text":"Under development","title":"Resources"},{"location":"modules/resources/#more-training-material-for-galaxy","text":"Galaxy Training Network Melbourne Bioinformatics GOBLET","title":"More training material for Galaxy"},{"location":"modules/resources/#i-have-a-problem-in-galaxy","text":"For Galaxy Australia, email [add email]","title":"I have a problem in Galaxy"},{"location":"modules/resources/#can-i-attend-a-galaxy-workshop","text":"Yes: please search the EMBL-ABR Training page: http://www.embl-abr.org.au/about/events/","title":"Can I attend a Galaxy workshop?"},{"location":"modules/resources/#can-i-get-this-new-tool-into-galaxy","text":"Possibly: instructions/email","title":"Can I get this new tool into Galaxy?"},{"location":"modules/resources/#can-i-use-a-different-galaxy-server","text":"Yes: There are many public Galaxy servers, listed here: https://galaxyproject.org/public-galaxy-servers/","title":"Can I use a different Galaxy server?"},{"location":"modules/resources/#can-i-set-up-my-own-galaxy-server","text":"Yes: instructions","title":"Can I set up my own Galaxy server?"},{"location":"modules/resources/#i-have-a-suggestion-for-a-new-tutorial-topic","text":"Thank you: please email anna.syme@unimelb.edu.au","title":"I have a suggestion for a new tutorial topic"},{"location":"modules/rna-humans/","text":"RNA-seq: Eukaryotes now updated for Galaxy Australia Basic http://melbournebioinformatics.github.io/MelBioInf_docs/tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/ Advanced http://melbournebioinformatics.github.io/MelBioInf_docs/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/","title":"RNA-seq - eukaryotes"},{"location":"modules/rna-humans/#rna-seq-eukaryotes","text":"now updated for Galaxy Australia","title":"RNA-seq: Eukaryotes"},{"location":"modules/rna-humans/#basic","text":"http://melbournebioinformatics.github.io/MelBioInf_docs/tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/","title":"Basic"},{"location":"modules/rna-humans/#advanced","text":"http://melbournebioinformatics.github.io/MelBioInf_docs/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/","title":"Advanced"},{"location":"modules/roary/","text":"Find pan-genomes using Roary (using the commandline) A concept in comparative microbial genomics is core and pan genomes. If we analyse DNA from several bacterial strains, we may want to know which genes they have in common and which are unique to some strains. The core genome is the group of genes shared by all strains in the clade of interest. Gene sequences are similar but not necessarily identical. Core genome SNPs are those SNPs found in the genes in the core genome; i.e. at a particular site, the nucleotide varies. We can use these SNPs to infer relationships between the strains. The accessory genome is the group of genes that are not in all the strains. These genes may be in one or more strain. The pan genome is the sum of the core and accessory genomes. That is, a combination of all the genes that are found in the clade of interest. Bacteria can horizontally-transfer genes to other bacteria via plasmids, and so their accessory genome can be large relative to those of eurkaryotes. Bacterial accessory genomes often house genes for drug resistance. This tutorial demonstrates how to calculate the pan and core genomes of a set of input bacterial samples, using the tool Roary . Get data Input: annotated genome in GFF3 format, per sample ( e.g. output from Prokka) samples must be from same species Our data: Five strains of Staphylococcus aureus , in .gff format. Run Run roary roary -e --mafft -p 8 *.gff -e --mafft aligns the core genes using the tool MAFFT -p 8 uses 8 threads What does Roary do converts coding sequences into protein sequences clustered these protein sequences by several methods further refines clusters into orthologous genes for each sample, determines if gene is present/absent: produces gene_presence_absence.csv uses this gene p/a information to build a tree, using FastTree: produces accessory_binary_genes.fa.newick overall, calculates number of genes that are shared, and unique: produces summary_statistics.txt aligns the core genes (if option used, as above) for downstream analyses Output Core and pan genes Open the summary file: less summary_statistics.txt This shows a table of counts of shared genes (core genome) and total genes (pan genome). What are the core genes? query_pan_genome -a intersection -o core_genome_results *.gff What are the accessory genes? query_pan_genome -a complement -o accessory_genome_results *.gff What are the pan genes? query_pan_genome -a union -o pan_genome_results *.gff View: e.g. less core_genome_results | column -t Gene presence/absence Transfer the file gene_presence_absence.csv to your local computer and view in spreadsheet software. Search for the gene that confers methicillin resistance. It is only found in some of the strains. Vizualize with Phandango Copy these files to your local computer: accessory_binary_genes.fa.newick gene_presence_absence.csv Go to http://phandango.net drag and drop the two files onto the landing page. view the tree of samples and their core and pan genomes Links Roary: https://sanger-pathogens.github.io/Roary/ Roary publication: http://bioinformatics.oxfordjournals.org/content/31/22/3691","title":"Find pan-genomes using Roary (using the commandline)"},{"location":"modules/roary/#find-pan-genomes-using-roary-using-the-commandline","text":"A concept in comparative microbial genomics is core and pan genomes. If we analyse DNA from several bacterial strains, we may want to know which genes they have in common and which are unique to some strains. The core genome is the group of genes shared by all strains in the clade of interest. Gene sequences are similar but not necessarily identical. Core genome SNPs are those SNPs found in the genes in the core genome; i.e. at a particular site, the nucleotide varies. We can use these SNPs to infer relationships between the strains. The accessory genome is the group of genes that are not in all the strains. These genes may be in one or more strain. The pan genome is the sum of the core and accessory genomes. That is, a combination of all the genes that are found in the clade of interest. Bacteria can horizontally-transfer genes to other bacteria via plasmids, and so their accessory genome can be large relative to those of eurkaryotes. Bacterial accessory genomes often house genes for drug resistance. This tutorial demonstrates how to calculate the pan and core genomes of a set of input bacterial samples, using the tool Roary .","title":"Find pan-genomes using Roary (using the commandline)"},{"location":"modules/roary/#get-data","text":"Input: annotated genome in GFF3 format, per sample ( e.g. output from Prokka) samples must be from same species Our data: Five strains of Staphylococcus aureus , in .gff format.","title":"Get data"},{"location":"modules/roary/#run","text":"","title":"Run"},{"location":"modules/roary/#run-roary","text":"roary -e --mafft -p 8 *.gff -e --mafft aligns the core genes using the tool MAFFT -p 8 uses 8 threads","title":"Run roary"},{"location":"modules/roary/#what-does-roary-do","text":"converts coding sequences into protein sequences clustered these protein sequences by several methods further refines clusters into orthologous genes for each sample, determines if gene is present/absent: produces gene_presence_absence.csv uses this gene p/a information to build a tree, using FastTree: produces accessory_binary_genes.fa.newick overall, calculates number of genes that are shared, and unique: produces summary_statistics.txt aligns the core genes (if option used, as above) for downstream analyses","title":"What does Roary do"},{"location":"modules/roary/#output","text":"","title":"Output"},{"location":"modules/roary/#core-and-pan-genes","text":"Open the summary file: less summary_statistics.txt This shows a table of counts of shared genes (core genome) and total genes (pan genome). What are the core genes? query_pan_genome -a intersection -o core_genome_results *.gff What are the accessory genes? query_pan_genome -a complement -o accessory_genome_results *.gff What are the pan genes? query_pan_genome -a union -o pan_genome_results *.gff View: e.g. less core_genome_results | column -t","title":"Core and pan genes"},{"location":"modules/roary/#gene-presenceabsence","text":"Transfer the file gene_presence_absence.csv to your local computer and view in spreadsheet software. Search for the gene that confers methicillin resistance. It is only found in some of the strains.","title":"Gene presence/absence"},{"location":"modules/roary/#vizualize-with-phandango","text":"Copy these files to your local computer: accessory_binary_genes.fa.newick gene_presence_absence.csv Go to http://phandango.net drag and drop the two files onto the landing page. view the tree of samples and their core and pan genomes","title":"Vizualize with Phandango"},{"location":"modules/roary/#links","text":"Roary: https://sanger-pathogens.github.io/Roary/ Roary publication: http://bioinformatics.oxfordjournals.org/content/31/22/3691","title":"Links"},{"location":"modules/roary-galaxy/","text":"Find pan-genomes using Roary (in Galaxy) A concept in comparative microbial genomics is core and pan genomes. If we analyse DNA from several bacterial strains, we may want to know which genes they have in common and which are unique to some strains. The core genome is the group of genes shared by all strains in the clade of interest. Gene sequences are similar but not necessarily identical. Core genome SNPs are those SNPs found in the genes in the core genome; i.e. at a particular site, the nucleotide varies. We can use these SNPs to infer relationships between the strains. The accessory genome is the group of genes that are not in all the strains. These genes may be in one or more strain. The pan genome is the sum of the core and accessory genomes. That is, a combination of all the genes that are found in the clade of interest. Bacteria can horizontally-transfer genes to other bacteria via plasmids, and so their accessory genome can be large relative to those of eurkaryotes. Bacterial accessory genomes often house genes for drug resistance. This tutorial demonstrates how to calculate the pan and core genomes of a set of input bacterial samples, using the tool Roary . Get data Input: annotated genome in GFF3 format, per sample ( e.g. output from Prokka) samples must be from same species Our data: Five strains of Staphylococcus aureus , in .gff format. Galaxy or swift location: ask your demonstrator or load your own data. Run Run Roary In Galaxy, go to Tools NGS Analysis Pan Genomes Roary Set the following parameters (leave everything else unchanged): Individual gff files or a dataset collection : Individual select gff inputs to Roary : Select all the .gff input files Click Execute What does Roary do converts coding sequences into protein sequences clustered these protein sequences by several methods further refines clusters into orthologous genes for each sample, determines if gene is present/absent: produces gene_presence_absence.csv uses this gene p/a information to build a tree, using FastTree: produces accessory_binary_genes.fa.newick overall, calculates number of genes that are shared, and unique: produces summary_statistics.txt aligns the core genes (if option used, as above) for downstream analyses Output There are three output files. Summary statistics Click on the eye icon. This shows a table of counts of shared genes (core genome) and total genes (pan genome). Core gene alignment Click on the disk icon under this file to download it. Gene presence/absence Click on the disk icon under this file to download it. Open and view in spreadsheet software. Column 3 shows the annotated gene name. Column 4 shows the number of isolates that the gene was found it (in this case, ordered from 5 (all) to 1). Infer phylogeny using core gene snps Roary has produced an alignment of the core genes. We can use this alignment to infer a phylogenetic tree of the isolates. In Galaxy, go to Tools NGS Analysis NGS:Phylogenetics Phylogeneitc reconstruction with RaXML Set the following parameters (leave everything else unchanged): Source file : Roary on data x, data x, and others Core Gene Alignment Model Type : Nucleotide Substitution Model : GTRGAMMA Click Execute There are six output files. Click on Result . Under the file, click on the Visualize icon (a graph), then choose PhyloViz. These isolates are all very closely related and so the structure of the tree is narrow. To expand, go to the right hand box for Phyloviz Settings . Change the Phylogenetic Spacing to 2500 and the Vertical Spacing to 30. To return to the main Galaxy screen, go to the top panel and click on Analyze Data . Click on the disk icon under the Results file to download. Re-name with the file extension .tree Vizualize with Phandango If not done already, copy these files to your local computer: The raxml.tree (or the accessory_binary_genes.fa.newick ). gene_presence_absence.csv Go to http://phandango.net drag and drop the two files onto the landing page. view the tree of samples and their core and pan genomes each blue coloured column is a gene: genes are present or absent in each isolate the core genes are shared by all isolates","title":"Find pan-genomes using Roary (in Galaxy)"},{"location":"modules/roary-galaxy/#find-pan-genomes-using-roary-in-galaxy","text":"A concept in comparative microbial genomics is core and pan genomes. If we analyse DNA from several bacterial strains, we may want to know which genes they have in common and which are unique to some strains. The core genome is the group of genes shared by all strains in the clade of interest. Gene sequences are similar but not necessarily identical. Core genome SNPs are those SNPs found in the genes in the core genome; i.e. at a particular site, the nucleotide varies. We can use these SNPs to infer relationships between the strains. The accessory genome is the group of genes that are not in all the strains. These genes may be in one or more strain. The pan genome is the sum of the core and accessory genomes. That is, a combination of all the genes that are found in the clade of interest. Bacteria can horizontally-transfer genes to other bacteria via plasmids, and so their accessory genome can be large relative to those of eurkaryotes. Bacterial accessory genomes often house genes for drug resistance. This tutorial demonstrates how to calculate the pan and core genomes of a set of input bacterial samples, using the tool Roary .","title":"Find pan-genomes using Roary (in Galaxy)"},{"location":"modules/roary-galaxy/#get-data","text":"Input: annotated genome in GFF3 format, per sample ( e.g. output from Prokka) samples must be from same species Our data: Five strains of Staphylococcus aureus , in .gff format. Galaxy or swift location: ask your demonstrator or load your own data.","title":"Get data"},{"location":"modules/roary-galaxy/#run","text":"","title":"Run"},{"location":"modules/roary-galaxy/#run-roary","text":"In Galaxy, go to Tools NGS Analysis Pan Genomes Roary Set the following parameters (leave everything else unchanged): Individual gff files or a dataset collection : Individual select gff inputs to Roary : Select all the .gff input files Click Execute","title":"Run Roary"},{"location":"modules/roary-galaxy/#what-does-roary-do","text":"converts coding sequences into protein sequences clustered these protein sequences by several methods further refines clusters into orthologous genes for each sample, determines if gene is present/absent: produces gene_presence_absence.csv uses this gene p/a information to build a tree, using FastTree: produces accessory_binary_genes.fa.newick overall, calculates number of genes that are shared, and unique: produces summary_statistics.txt aligns the core genes (if option used, as above) for downstream analyses","title":"What does Roary do"},{"location":"modules/roary-galaxy/#output","text":"There are three output files.","title":"Output"},{"location":"modules/roary-galaxy/#summary-statistics","text":"Click on the eye icon. This shows a table of counts of shared genes (core genome) and total genes (pan genome).","title":"Summary statistics"},{"location":"modules/roary-galaxy/#core-gene-alignment","text":"Click on the disk icon under this file to download it.","title":"Core gene alignment"},{"location":"modules/roary-galaxy/#gene-presenceabsence","text":"Click on the disk icon under this file to download it. Open and view in spreadsheet software. Column 3 shows the annotated gene name. Column 4 shows the number of isolates that the gene was found it (in this case, ordered from 5 (all) to 1).","title":"Gene presence/absence"},{"location":"modules/roary-galaxy/#infer-phylogeny-using-core-gene-snps","text":"Roary has produced an alignment of the core genes. We can use this alignment to infer a phylogenetic tree of the isolates. In Galaxy, go to Tools NGS Analysis NGS:Phylogenetics Phylogeneitc reconstruction with RaXML Set the following parameters (leave everything else unchanged): Source file : Roary on data x, data x, and others Core Gene Alignment Model Type : Nucleotide Substitution Model : GTRGAMMA Click Execute There are six output files. Click on Result . Under the file, click on the Visualize icon (a graph), then choose PhyloViz. These isolates are all very closely related and so the structure of the tree is narrow. To expand, go to the right hand box for Phyloviz Settings . Change the Phylogenetic Spacing to 2500 and the Vertical Spacing to 30. To return to the main Galaxy screen, go to the top panel and click on Analyze Data . Click on the disk icon under the Results file to download. Re-name with the file extension .tree","title":"Infer phylogeny using core gene snps"},{"location":"modules/roary-galaxy/#vizualize-with-phandango","text":"If not done already, copy these files to your local computer: The raxml.tree (or the accessory_binary_genes.fa.newick ). gene_presence_absence.csv Go to http://phandango.net drag and drop the two files onto the landing page. view the tree of samples and their core and pan genomes each blue coloured column is a gene: genes are present or absent in each isolate the core genes are shared by all isolates","title":"Vizualize with Phandango"},{"location":"modules/salmon/","text":"Salmon Available on the ABRPI mGVL command line. Not yet available in mGVL Galaxy. What does Salmon do? quantifies transcripts from RNA-seq data uses quasi-mapping to map reads to reference - more detail here Start Note: testing on MDU first. Open a terminal. Log in to the ABRPI mGVL. Make a folder called RNA_seq mkdir salmon cd salmon Input RNA-seq reads in FASTA/FASTQ format Reference in FASTA format need to get the Ecoli transcriptome - containing genes and RNAs how to get? ```genbank2fasta.pl file.gbk file.ffn get these files from [ Nectar container ] - how to scp these in using terminal ##Run Index the reference . [ Note : the reads need to be in random order with respect to the reference . Are they ? ] ``` text salmon index - t ecoli . fasta - i ecoli_index -t specifies the input transcript fasta file -i specifies the name of the output index (folder) Quantify the reads [Salmon says quantify, don t count ? ] salmon quant -i ecoli_index -l A -r rna_seqs.fasta.gz -p 8 --numAuxModelSamples 50000 -o quants -i is the name of the index for the reference file -l A means that salmon will determine the library type for the input sequencing reads (whether they are stranded, anything else? page just says etc.) -p 8 use 8 threads (possible on this machine?) numAuxModelSamples default was 5 million but there are only ~2million reads per file; set this to 50,000 (not sure if ok) -o quants output directory for results is called quants Warming: Detected a potential strand bias 1% in an unstranded protocol check the file: quants/lib_format_counts.json for details (is this a problem?) Output less quant.sf shows the output: TPM - Transcripts per Million NumReads - estimated number of reads for this reference","title":"Salmon"},{"location":"modules/salmon/#salmon","text":"Available on the ABRPI mGVL command line. Not yet available in mGVL Galaxy. What does Salmon do? quantifies transcripts from RNA-seq data uses quasi-mapping to map reads to reference - more detail here","title":"Salmon"},{"location":"modules/salmon/#start","text":"Note: testing on MDU first. Open a terminal. Log in to the ABRPI mGVL. Make a folder called RNA_seq mkdir salmon cd salmon","title":"Start"},{"location":"modules/salmon/#input","text":"RNA-seq reads in FASTA/FASTQ format Reference in FASTA format need to get the Ecoli transcriptome - containing genes and RNAs how to get? ```genbank2fasta.pl file.gbk file.ffn get these files from [ Nectar container ] - how to scp these in using terminal ##Run Index the reference . [ Note : the reads need to be in random order with respect to the reference . Are they ? ] ``` text salmon index - t ecoli . fasta - i ecoli_index -t specifies the input transcript fasta file -i specifies the name of the output index (folder) Quantify the reads [Salmon says quantify, don t count ? ] salmon quant -i ecoli_index -l A -r rna_seqs.fasta.gz -p 8 --numAuxModelSamples 50000 -o quants -i is the name of the index for the reference file -l A means that salmon will determine the library type for the input sequencing reads (whether they are stranded, anything else? page just says etc.) -p 8 use 8 threads (possible on this machine?) numAuxModelSamples default was 5 million but there are only ~2million reads per file; set this to 50,000 (not sure if ok) -o quants output directory for results is called quants Warming: Detected a potential strand bias 1% in an unstranded protocol check the file: quants/lib_format_counts.json for details (is this a problem?)","title":"Input"},{"location":"modules/salmon/#output","text":"less quant.sf shows the output: TPM - Transcripts per Million NumReads - estimated number of reads for this reference","title":"Output"},{"location":"modules/scripts/","text":"Bash scripts It is possible to put some commands into a script, to run automatically. Here, we will put commands for several tools into a bash script and use it to assemble a bacterial genome. Bash is a programming language, and we can run a bash script in the bash shell (e.g. the terminal). Prepare the bash script Open the terminal. Open a text editor, e.g., nano. nano myscript.sh We now have a script called myscript.sh In the script, write #!/bin/bash echo This is my assembly script Ctrl-X to exit nano. Make the script executable: chmod 755 myscript.sh Run the script to test. ./myscript.sh The text should print to the screen: This is my assembly script Edit the bash script Now we will add to the bash script. This will be a simple script that is designed to assemble, trim, and correct. It will likely only work in simple cases. Open nano nano myscript.sh and type in the following (#comments and echo statements are optional but probably useful): #!/bin/bash #a script for assembling a bacterial genome #set a variable for the number of CPUS #this can then be easily changed if needed CPUS = 16 #this script requires 5 arguments #we will test whether 5 were entered if [ ! $# -eq 5 ] #if there are not 5 arguments then echo ERROR: needs 5 arguments echo usage: $0 outdir genomeSize (e.g. 2.1m) pacbio.fq R1.fq R2.fq #the usage shows what arguments are required exit 1 #exits fi #report back the arguments entered: #each argument is assigned a variable name from 1 to 5 #these variables are referred to with the $ sign echo your directory with output is called $1 echo your genomeSize is $2 echo your pacbio reads are in file $3 echo your R1 reads are in file $4 echo your R2 reads are in file $5 echo #echo on its own puts in a blank line on the screen #run canu to assemble the reads mkdir -p $1 #the p creates intermediate directories where required echo now running canu echo your canu output is in directory $1 /canu nice canu -p canu -d $1 /canu genomeSize = $2 -pacbio-raw $3 #commands are preceeded by nice in this script for particular servers #test canu output: number of contigs #make a variable called num_contigs num_contigs = $( grep -c $1 /canu/canu.contigs.fasta ) echo number of contigs found by canu is: $num_contigs echo if [ $num_contigs -gt 10 ] then echo there are more than 10 contigs echo the analysis is stopping exit 1 #in more complex scripts, you can add the option to repeat the analysis with different parameters, etc. fi #run circlator to trim the contigs echo now running circlator to trim and orient contigs echo your circlator output is in directory $1 /circlator nice circlator all --threads $CPUS --verbose $1 /canu/canu.contigs.fasta $1 /canu/canu.correctedReads.fasta.gz $1 /circlator #in more complex scripts, you can add information about which contigs were merged; where contigs were oriented; how contigs were trimmed, etc. #rename the contigs file cp $1 /circlator/06.fixstart.fasta $1 /contigs.fa echo your contigs from circlator are now in the file called contigs.fa #note: in this case, these contigs may not have been trimmed. #find smaller plasmids echo now looking in illumina reads for small plasmids echo indexing the contigs.fa file nice bwa index $1 /contigs.fa echo aligning illumina reads to the contigs.fa file nice bwa mem -t $CPUS $1 /contigs.fa $4 $5 | samtools sort $1 /aln.bam echo indexing the aln.bam file nice samtools index $1 /aln.bam echo extracting the unmapped illumina reads samtools fastq -f 4 -1 $1 /unmapped.R1.fastq -2 $1 /unmapped.R2.fastq -s $1 /unmapped.RS.fastq $1 /aln.bam echo assembling the unmapped reads with spades echo nice spades.py -t $CPUS -1 $1 /unmapped.R1.fastq -2 $1 /unmapped.R2.fastq -s $1 /unmapped.RS.fastq -o $1 /spades_assembly echo echo your output is in the spades_assembly directory echo #check if any contigs are 1500bp seqtk seq -L 1500 $1 /spades_assembly/contigs.fasta $1 /illumina_contigs.fasta echo extracting contigs 1500bp into illumina_contigs.fasta echo #in more complex scripts, you can trim and orient these contigs, or do further investigation into whether they are true plasmids #join the canu pacbio contigs and the small plasmids cat $1 /contigs.fa $1 /illumina_contigs.fasta $1 /allcontigs.fasta echo echo joining small contigs to the pacbio contigs as allcontigs.fasta echo #correct this assembly with Pilon echo preparing to correct contigs with illumina reads, using Pilon echo indexing allcontigs.fasta nice bwa index $1 /allcontigs.fasta echo aligning illumina reads to allcontigs.fasta nice bwa mem -t $CPUS $1 /allcontigs.fasta $4 $5 | samtools sort $1 /aln.bam echo indexing aln.bam nice samtools index $1 /aln.bam echo indexing allcontigs.fasta nice samtools faidx $1 /allcontigs.fasta echo correcting with Pilon nice pilon --genome $1 /allcontigs.fasta --frags $1 /aln.bam --output $1 /pilon1 --fix all --mindepth 0 .5 --changes --verbose --threads $CPUS echo echo Pilon finished echo #count how many changes were made pilon_changes = $( wc -l $1 /pilon1.changes ) echo there were $pilon_changes corrections made echo echo Assembly complete echo The sequence is saved as $1 /pilon1.fasta exit 1 Run To run, type in ./myscript.sh outdir 1 .9m subreads.fastq R1.fastq R2.fastq outdir is the name of the output directory 1.9m is the approximate genome size for the species subreads.fastq is the name of the Pacbio reads file R1.fastq is the name of the R1 Illumina reads file R2.fastq is the name of the R2 Illumina reads file If the Pacbio and Illumina reads files are not in the current directory, put in the whole path to those files The script should run to the end or may exit early if necessary (e.g. the canu assembly found too many contigs). There are many refinements and additions that could be made to this script, and some of these are noted as comments above.","title":"Home"},{"location":"modules/scripts/#bash-scripts","text":"It is possible to put some commands into a script, to run automatically. Here, we will put commands for several tools into a bash script and use it to assemble a bacterial genome. Bash is a programming language, and we can run a bash script in the bash shell (e.g. the terminal).","title":"Bash scripts"},{"location":"modules/scripts/#prepare-the-bash-script","text":"Open the terminal. Open a text editor, e.g., nano. nano myscript.sh We now have a script called myscript.sh In the script, write #!/bin/bash echo This is my assembly script Ctrl-X to exit nano. Make the script executable: chmod 755 myscript.sh Run the script to test. ./myscript.sh The text should print to the screen: This is my assembly script","title":"Prepare the bash script"},{"location":"modules/scripts/#edit-the-bash-script","text":"Now we will add to the bash script. This will be a simple script that is designed to assemble, trim, and correct. It will likely only work in simple cases. Open nano nano myscript.sh and type in the following (#comments and echo statements are optional but probably useful): #!/bin/bash #a script for assembling a bacterial genome #set a variable for the number of CPUS #this can then be easily changed if needed CPUS = 16 #this script requires 5 arguments #we will test whether 5 were entered if [ ! $# -eq 5 ] #if there are not 5 arguments then echo ERROR: needs 5 arguments echo usage: $0 outdir genomeSize (e.g. 2.1m) pacbio.fq R1.fq R2.fq #the usage shows what arguments are required exit 1 #exits fi #report back the arguments entered: #each argument is assigned a variable name from 1 to 5 #these variables are referred to with the $ sign echo your directory with output is called $1 echo your genomeSize is $2 echo your pacbio reads are in file $3 echo your R1 reads are in file $4 echo your R2 reads are in file $5 echo #echo on its own puts in a blank line on the screen #run canu to assemble the reads mkdir -p $1 #the p creates intermediate directories where required echo now running canu echo your canu output is in directory $1 /canu nice canu -p canu -d $1 /canu genomeSize = $2 -pacbio-raw $3 #commands are preceeded by nice in this script for particular servers #test canu output: number of contigs #make a variable called num_contigs num_contigs = $( grep -c $1 /canu/canu.contigs.fasta ) echo number of contigs found by canu is: $num_contigs echo if [ $num_contigs -gt 10 ] then echo there are more than 10 contigs echo the analysis is stopping exit 1 #in more complex scripts, you can add the option to repeat the analysis with different parameters, etc. fi #run circlator to trim the contigs echo now running circlator to trim and orient contigs echo your circlator output is in directory $1 /circlator nice circlator all --threads $CPUS --verbose $1 /canu/canu.contigs.fasta $1 /canu/canu.correctedReads.fasta.gz $1 /circlator #in more complex scripts, you can add information about which contigs were merged; where contigs were oriented; how contigs were trimmed, etc. #rename the contigs file cp $1 /circlator/06.fixstart.fasta $1 /contigs.fa echo your contigs from circlator are now in the file called contigs.fa #note: in this case, these contigs may not have been trimmed. #find smaller plasmids echo now looking in illumina reads for small plasmids echo indexing the contigs.fa file nice bwa index $1 /contigs.fa echo aligning illumina reads to the contigs.fa file nice bwa mem -t $CPUS $1 /contigs.fa $4 $5 | samtools sort $1 /aln.bam echo indexing the aln.bam file nice samtools index $1 /aln.bam echo extracting the unmapped illumina reads samtools fastq -f 4 -1 $1 /unmapped.R1.fastq -2 $1 /unmapped.R2.fastq -s $1 /unmapped.RS.fastq $1 /aln.bam echo assembling the unmapped reads with spades echo nice spades.py -t $CPUS -1 $1 /unmapped.R1.fastq -2 $1 /unmapped.R2.fastq -s $1 /unmapped.RS.fastq -o $1 /spades_assembly echo echo your output is in the spades_assembly directory echo #check if any contigs are 1500bp seqtk seq -L 1500 $1 /spades_assembly/contigs.fasta $1 /illumina_contigs.fasta echo extracting contigs 1500bp into illumina_contigs.fasta echo #in more complex scripts, you can trim and orient these contigs, or do further investigation into whether they are true plasmids #join the canu pacbio contigs and the small plasmids cat $1 /contigs.fa $1 /illumina_contigs.fasta $1 /allcontigs.fasta echo echo joining small contigs to the pacbio contigs as allcontigs.fasta echo #correct this assembly with Pilon echo preparing to correct contigs with illumina reads, using Pilon echo indexing allcontigs.fasta nice bwa index $1 /allcontigs.fasta echo aligning illumina reads to allcontigs.fasta nice bwa mem -t $CPUS $1 /allcontigs.fasta $4 $5 | samtools sort $1 /aln.bam echo indexing aln.bam nice samtools index $1 /aln.bam echo indexing allcontigs.fasta nice samtools faidx $1 /allcontigs.fasta echo correcting with Pilon nice pilon --genome $1 /allcontigs.fasta --frags $1 /aln.bam --output $1 /pilon1 --fix all --mindepth 0 .5 --changes --verbose --threads $CPUS echo echo Pilon finished echo #count how many changes were made pilon_changes = $( wc -l $1 /pilon1.changes ) echo there were $pilon_changes corrections made echo echo Assembly complete echo The sequence is saved as $1 /pilon1.fasta exit 1","title":"Edit the bash script"},{"location":"modules/scripts/#run","text":"To run, type in ./myscript.sh outdir 1 .9m subreads.fastq R1.fastq R2.fastq outdir is the name of the output directory 1.9m is the approximate genome size for the species subreads.fastq is the name of the Pacbio reads file R1.fastq is the name of the R1 Illumina reads file R2.fastq is the name of the R2 Illumina reads file If the Pacbio and Illumina reads files are not in the current directory, put in the whole path to those files The script should run to the end or may exit early if necessary (e.g. the canu assembly found too many contigs). There are many refinements and additions that could be made to this script, and some of these are noted as comments above.","title":"Run"},{"location":"modules/shortreads/","text":"Short read data How to get short-read FASTQ files into Galaxy? In the tool panel, click on Get Data Click on EBI SRA . This will open the ENA webpage. In the top right, next to the search bar, click on Advanced For Select domain click Read Select other options as required. For example: For Taxonomy and related enter Mycoplasma synoviae . In the bottom right corner click Search Click on Study Click PRJNA183042 In the column FASTQ files (Galaxy) click on File 1 Galaxy will open in a new tab and this file will start to upload to your current history. Go back to the EBI browser tab and click on File 2 . This will also upload into your current history. You can then close any open EBI browser tabs and any extra Galaxy tabs - you only need the one Galaxy tab open. To repeat for other short read data, select Read and then choose options required, such as taxon, library layout, instrument type, or date created.","title":"Short read data"},{"location":"modules/shortreads/#short-read-data","text":"How to get short-read FASTQ files into Galaxy? In the tool panel, click on Get Data Click on EBI SRA . This will open the ENA webpage. In the top right, next to the search bar, click on Advanced For Select domain click Read Select other options as required. For example: For Taxonomy and related enter Mycoplasma synoviae . In the bottom right corner click Search Click on Study Click PRJNA183042 In the column FASTQ files (Galaxy) click on File 1 Galaxy will open in a new tab and this file will start to upload to your current history. Go back to the EBI browser tab and click on File 2 . This will also upload into your current history. You can then close any open EBI browser tabs and any extra Galaxy tabs - you only need the one Galaxy tab open. To repeat for other short read data, select Read and then choose options required, such as taxon, library layout, instrument type, or date created.","title":"Short read data"},{"location":"modules/shovill/","text":"Assembly using Shovill","title":"Assembly using Shovill"},{"location":"modules/shovill/#assembly-using-shovill","text":"","title":"Assembly using Shovill"},{"location":"modules/snippy/","text":"Variant calling with Snippy Variant calling is the process of identifying differences between two genome samples. Differences are usually limited to single nucleotide polymorphisms (SNPs) and small insertions and deletions (indels). Larger structural variation such as inversions, duplications and large deletions are not typically covered by variant calling . In this tutorial, we will use the tool Snippy (link to Snippy is here ). Snippy uses a tool to align the reads to a reference genome, and another tool to decide ( call ) if the discrepancies are real variants. New to Galaxy? First try the introduction and then learn some key tasks Get data Log in to your Galaxy instance (for example, Galaxy Australia, usegalaxy.org.au ). Use shared data If you are using Galaxy Australia, you can import the data from a shared data library. In the top menu bar, go to Shared Data . Click on Data Libraries . Click on Galaxy Australia Training Material: Variant Calling: Microbial Variant Calling . Tick the boxes next to the five files. Click the To History button, select As Datasets . Name a new history and click Import . In the top menu bar, click Analyze Data . You should now have five files in your current history. Or, import from the web Only follow this step if unable to load the data files from shared data, as described above . In a new browser tab, go to this webpage: Find the file called mutant_R1.fastq Right click on file name: select copy link address In Galaxy, go to Get Data and then Upload File Click Paste/Fetch data A box will appear: paste in link address Click Start Click Close The file will now appear in the top of your history panel. Shorten the file name if you wish. (Click the pencil icon). Repeat these steps for the other set of reads (reverse reads) called mutant_R2.fq : In a new browser tab, go to this webpage: Find the file called mutant_R2.fastq Right click on file name: select copy link address In Galaxy, go to Get Data and then Upload File Click Paste/Fetch data A box will appear: paste in link address Click Start Click Close The file will now appear in the top of your history panel. Shorten the file name if you wish. Repeat these steps for three other files: wildtype.gbk , wildtype.gff and wildtype.fna . Shorten file names Click on the pencil icon next to the file name. In the centre Galaxy panel, click in the box under Name Shorten the file name. Then click Save Quality Control If you want to check the quality of your reads, see the Quality Control tutorial . Note: Skip over the Import Data section and instead use the file called mutant_R1.fastq that is already in your current history. Call variants with Snippy Go to the Tool panel and search for snippy in the search box. Click on snippy Set the following parameters (leave other settings as they are): For Reference type select Genbank . Then for Reference Genbank choose the wildtype.gbk file. For Single or Paired-end reads choose Paired . Then choose the first set of reads, mutant_R1.fastq and second set of reads, mutant_R2.fastq . Click Advanced Parameters and for Output selection , select all the files. Your tool interface should look a bit like this (not exactly, as tool versions are often updated): Click Execute . Examine Snippy output From Snippy, there are 10 output files in various formats. Go to the table file called snippy on data XX, data XX and data XX table and click on the eye icon. (The XX refers to the number that Galaxy has given your files.) We can see a list of variants. Look in column 3 to see which types the variants are, such as a SNP or a deletion. Look at the third variant called. This is a T A mutation, causing a stop codon. Look at column 14: the product of this gene is a methicillin resistance protein. Methicillin is an antibiotic. What might be the result of such a mutation? View Snippy output in JBrowse Go to the Galaxy tools panel, and use the search box at the top to search for JBrowse . Under Reference genome to display choose Use a genome from history . Under Select the reference genome choose wildtype.fna . This sequence will be the reference against which annotations are displayed. For Produce a Standalone Instance select Yes . For Genetic Code choose 11: The Bacterial, Archaeal and Plant Plastid Code . Under JBrowse-in-Galaxy Action choose New JBrowse Instance . We will now set up three different tracks - these are datasets displayed underneath the reference sequence (which is displayed as nucleotides in FASTA format). We will choose to display the sequence reads (the .bam file), the variants found by snippy (the .gff file) and the annotated reference genome (the wildtype.gff) Track 1 - sequence reads Click Insert Track Group For Track Cateogry name it sequence reads Click Insert Annotation Track For Track Type choose BAM Pileups For BAM Track Data select the snippy bam file For Autogenerate SNP Track select Yes Track 2 - variants Click Insert Track Group again For Track Category name it variants Click Insert Annotation Track For Track Type choose GFF/GFF3/BED/GBK Features For Track Data select the snippy snps gff file Track 3 - annotated reference Click Insert Track Group again For Track Category name it annotated reference Click Insert Annotation Track For Track Type choose GFF/GFF3/BED/GBK Features For Track Data select wildtype.gff Click Execute A new file will be created, called JBrowse on data XX and data XX - Complete . (This may take some time. If you would like to see a completed history for this tutorial, instructions are in the next section.) Click on the eye icon next to the file name. The JBrowse window will appear in the centre Galaxy panel. On the left, tick boxes to display the tracks Use the minus button to zoom out to see: sequence reads and their coverage (the grey graph) Use the plus button to zoom in to see: probable real variants (a whole column of snps) probable errors (single one here and there) In the coordinates box, type in 47299 and then Go to see the position of the SNP discussed above. the correct codon at this position is TGT, coding for the amino acid Cysteine, in the middle row of the amino acid translations. the mutation of T A turns this triplet into TGA, a stop codon. Extension exercise Let s repeat this tutorial using other data. We will investigate the bacteria Mycoplasma synoviae as it has a small genome and will not take too long to analyse. We want to find out if our sequencing reads vary from a reference genome. To get some sequencing reads, follow the instructions here . This will give us paired-end sequencing reads (two files). These files are very large. To subsample them so that we can run the analysis in a shorter time: Search for Select first in the tool search box Select first 4001 lines from the R1.fastq file Execute Repeat this for the R2.fastq file To obtain a reference genome, follow the instructions here including the Prokka step. This will give us 12 output files from Prokka. We need three of these files: the genbank file Prokka.gbk , the fasta file Prokka.fna , and the gff file Prokka.gff . Galaxy has interpreted the genbank file as a text file. Click on the pencil icon next to the genbank file; click Datatypes, and change the datatype to genbank . Run snippy on the sequencing reads. For the reference genome, use the Prokka:gbk file. Remember to include all the output files. Run JBrowse to produce a visualization of the called variants. For the reference genome, use the Prokka.fna file. As we did above, set up three Tracks to view underneath this reference genome nucleotide sequence: The snippy.bam file showing the reads mapped to the reference, the snippy.gff file showing the snps (the variants), and the annotated reference genome Prokka.gff . You should now have a JBrowse file showing the results of variant calling on these reads of the bacteria Mycoplasma synoviae . Can you find any variants? The workflow: See this history in Galaxy If you want to see this Galaxy history without performing the steps above: Log in to Galaxy Australia: https://usegalaxy.org.au/ Go to Shared Data Click Histories Click Variant_calling_with_snippy (or, for the extension part, click Variant-calling-bacteria-extension ) Click Import (at the top right corner) The analysis should now be showing as your current history. More information Here are some references covering more information about variant calling. Comparison of variant calling tools: Sandmann S, de Graaf AO, Karimi M, van der Reijden BA, Hellstr\u00f6m-Lindberg E, Jansen JH, Dugas M. Evaluating Variant Calling Tools for Non-Matched Next-Generation Sequencing Data. Sci Rep. 2017 Feb 24;7:43169. Human variant calling pipelines: Hwang S, Kim E, Lee I, Marcotte EM. Systematic comparison of variant calling pipelines using gold standard personal exome variants. Sci Rep. 2015 Dec 7;5:17875. Rare and common disease variants: Fritsche LG, Igl W, Bailey JNC, et al. A large genome-wide association study of age-related macular degeneration highlights contributions of rare and common variants. Nat Genet. 2016 Feb;48(2):134\u201343. Variant calling in the Ebola virus: Quick J, Loman NJ, Duraffour S, Simpson JT, et al. Real-time, portable genome sequencing for Ebola surveillance. Nature. 2016 Feb 11;530(7589):228\u201332. Variant calling to detect antibiotic-resistant bacteria: Kpeli G, Buultjens AH, Giulieri S, Owusu-Mireku E, Aboagye SY, Baines SL, Seemann T, Bulach D, Gon\u00e7alves da Silva A, Monk IR, Howden BP, Pluschke G, Yeboah-Manu D, Stinear T. Genomic analysis of ST88 community-acquired methicillin resistant Staphylococcus aureus in Ghana. PeerJ. 2017 Feb 28;5:e3047. High coverage artifacts: Li H. Toward better understanding of artifacts in variant calling from high-coverage samples. Bioinformatics. 2014 Oct 15;30(20):2843\u201351. What s next? To use the tutorials on this website: see the list in the left hand panel or, click the menu button (three horizontal bars) in the top left of the page You can find more tutorials at the Galaxy Training Network: http://galaxyproject.github.io/training-material/","title":"Variant calling - bacteria"},{"location":"modules/snippy/#variant-calling-with-snippy","text":"Variant calling is the process of identifying differences between two genome samples. Differences are usually limited to single nucleotide polymorphisms (SNPs) and small insertions and deletions (indels). Larger structural variation such as inversions, duplications and large deletions are not typically covered by variant calling . In this tutorial, we will use the tool Snippy (link to Snippy is here ). Snippy uses a tool to align the reads to a reference genome, and another tool to decide ( call ) if the discrepancies are real variants. New to Galaxy? First try the introduction and then learn some key tasks","title":"Variant calling with Snippy"},{"location":"modules/snippy/#get-data","text":"Log in to your Galaxy instance (for example, Galaxy Australia, usegalaxy.org.au ).","title":"Get data"},{"location":"modules/snippy/#use-shared-data","text":"If you are using Galaxy Australia, you can import the data from a shared data library. In the top menu bar, go to Shared Data . Click on Data Libraries . Click on Galaxy Australia Training Material: Variant Calling: Microbial Variant Calling . Tick the boxes next to the five files. Click the To History button, select As Datasets . Name a new history and click Import . In the top menu bar, click Analyze Data . You should now have five files in your current history.","title":"Use shared data"},{"location":"modules/snippy/#or-import-from-the-web","text":"Only follow this step if unable to load the data files from shared data, as described above . In a new browser tab, go to this webpage: Find the file called mutant_R1.fastq Right click on file name: select copy link address In Galaxy, go to Get Data and then Upload File Click Paste/Fetch data A box will appear: paste in link address Click Start Click Close The file will now appear in the top of your history panel. Shorten the file name if you wish. (Click the pencil icon). Repeat these steps for the other set of reads (reverse reads) called mutant_R2.fq : In a new browser tab, go to this webpage: Find the file called mutant_R2.fastq Right click on file name: select copy link address In Galaxy, go to Get Data and then Upload File Click Paste/Fetch data A box will appear: paste in link address Click Start Click Close The file will now appear in the top of your history panel. Shorten the file name if you wish. Repeat these steps for three other files: wildtype.gbk , wildtype.gff and wildtype.fna .","title":"Or, import from the web"},{"location":"modules/snippy/#shorten-file-names","text":"Click on the pencil icon next to the file name. In the centre Galaxy panel, click in the box under Name Shorten the file name. Then click Save","title":"Shorten file names"},{"location":"modules/snippy/#quality-control","text":"If you want to check the quality of your reads, see the Quality Control tutorial . Note: Skip over the Import Data section and instead use the file called mutant_R1.fastq that is already in your current history.","title":"Quality Control"},{"location":"modules/snippy/#call-variants-with-snippy","text":"Go to the Tool panel and search for snippy in the search box. Click on snippy Set the following parameters (leave other settings as they are): For Reference type select Genbank . Then for Reference Genbank choose the wildtype.gbk file. For Single or Paired-end reads choose Paired . Then choose the first set of reads, mutant_R1.fastq and second set of reads, mutant_R2.fastq . Click Advanced Parameters and for Output selection , select all the files. Your tool interface should look a bit like this (not exactly, as tool versions are often updated): Click Execute .","title":"Call variants with Snippy"},{"location":"modules/snippy/#examine-snippy-output","text":"From Snippy, there are 10 output files in various formats. Go to the table file called snippy on data XX, data XX and data XX table and click on the eye icon. (The XX refers to the number that Galaxy has given your files.) We can see a list of variants. Look in column 3 to see which types the variants are, such as a SNP or a deletion. Look at the third variant called. This is a T A mutation, causing a stop codon. Look at column 14: the product of this gene is a methicillin resistance protein. Methicillin is an antibiotic. What might be the result of such a mutation?","title":"Examine Snippy output"},{"location":"modules/snippy/#view-snippy-output-in-jbrowse","text":"Go to the Galaxy tools panel, and use the search box at the top to search for JBrowse . Under Reference genome to display choose Use a genome from history . Under Select the reference genome choose wildtype.fna . This sequence will be the reference against which annotations are displayed. For Produce a Standalone Instance select Yes . For Genetic Code choose 11: The Bacterial, Archaeal and Plant Plastid Code . Under JBrowse-in-Galaxy Action choose New JBrowse Instance . We will now set up three different tracks - these are datasets displayed underneath the reference sequence (which is displayed as nucleotides in FASTA format). We will choose to display the sequence reads (the .bam file), the variants found by snippy (the .gff file) and the annotated reference genome (the wildtype.gff) Track 1 - sequence reads Click Insert Track Group For Track Cateogry name it sequence reads Click Insert Annotation Track For Track Type choose BAM Pileups For BAM Track Data select the snippy bam file For Autogenerate SNP Track select Yes Track 2 - variants Click Insert Track Group again For Track Category name it variants Click Insert Annotation Track For Track Type choose GFF/GFF3/BED/GBK Features For Track Data select the snippy snps gff file Track 3 - annotated reference Click Insert Track Group again For Track Category name it annotated reference Click Insert Annotation Track For Track Type choose GFF/GFF3/BED/GBK Features For Track Data select wildtype.gff Click Execute A new file will be created, called JBrowse on data XX and data XX - Complete . (This may take some time. If you would like to see a completed history for this tutorial, instructions are in the next section.) Click on the eye icon next to the file name. The JBrowse window will appear in the centre Galaxy panel. On the left, tick boxes to display the tracks Use the minus button to zoom out to see: sequence reads and their coverage (the grey graph) Use the plus button to zoom in to see: probable real variants (a whole column of snps) probable errors (single one here and there) In the coordinates box, type in 47299 and then Go to see the position of the SNP discussed above. the correct codon at this position is TGT, coding for the amino acid Cysteine, in the middle row of the amino acid translations. the mutation of T A turns this triplet into TGA, a stop codon.","title":"View Snippy output in JBrowse"},{"location":"modules/snippy/#extension-exercise","text":"Let s repeat this tutorial using other data. We will investigate the bacteria Mycoplasma synoviae as it has a small genome and will not take too long to analyse. We want to find out if our sequencing reads vary from a reference genome. To get some sequencing reads, follow the instructions here . This will give us paired-end sequencing reads (two files). These files are very large. To subsample them so that we can run the analysis in a shorter time: Search for Select first in the tool search box Select first 4001 lines from the R1.fastq file Execute Repeat this for the R2.fastq file To obtain a reference genome, follow the instructions here including the Prokka step. This will give us 12 output files from Prokka. We need three of these files: the genbank file Prokka.gbk , the fasta file Prokka.fna , and the gff file Prokka.gff . Galaxy has interpreted the genbank file as a text file. Click on the pencil icon next to the genbank file; click Datatypes, and change the datatype to genbank . Run snippy on the sequencing reads. For the reference genome, use the Prokka:gbk file. Remember to include all the output files. Run JBrowse to produce a visualization of the called variants. For the reference genome, use the Prokka.fna file. As we did above, set up three Tracks to view underneath this reference genome nucleotide sequence: The snippy.bam file showing the reads mapped to the reference, the snippy.gff file showing the snps (the variants), and the annotated reference genome Prokka.gff . You should now have a JBrowse file showing the results of variant calling on these reads of the bacteria Mycoplasma synoviae . Can you find any variants? The workflow:","title":"Extension exercise"},{"location":"modules/snippy/#see-this-history-in-galaxy","text":"If you want to see this Galaxy history without performing the steps above: Log in to Galaxy Australia: https://usegalaxy.org.au/ Go to Shared Data Click Histories Click Variant_calling_with_snippy (or, for the extension part, click Variant-calling-bacteria-extension ) Click Import (at the top right corner) The analysis should now be showing as your current history.","title":"See this history in Galaxy"},{"location":"modules/snippy/#more-information","text":"Here are some references covering more information about variant calling. Comparison of variant calling tools: Sandmann S, de Graaf AO, Karimi M, van der Reijden BA, Hellstr\u00f6m-Lindberg E, Jansen JH, Dugas M. Evaluating Variant Calling Tools for Non-Matched Next-Generation Sequencing Data. Sci Rep. 2017 Feb 24;7:43169. Human variant calling pipelines: Hwang S, Kim E, Lee I, Marcotte EM. Systematic comparison of variant calling pipelines using gold standard personal exome variants. Sci Rep. 2015 Dec 7;5:17875. Rare and common disease variants: Fritsche LG, Igl W, Bailey JNC, et al. A large genome-wide association study of age-related macular degeneration highlights contributions of rare and common variants. Nat Genet. 2016 Feb;48(2):134\u201343. Variant calling in the Ebola virus: Quick J, Loman NJ, Duraffour S, Simpson JT, et al. Real-time, portable genome sequencing for Ebola surveillance. Nature. 2016 Feb 11;530(7589):228\u201332. Variant calling to detect antibiotic-resistant bacteria: Kpeli G, Buultjens AH, Giulieri S, Owusu-Mireku E, Aboagye SY, Baines SL, Seemann T, Bulach D, Gon\u00e7alves da Silva A, Monk IR, Howden BP, Pluschke G, Yeboah-Manu D, Stinear T. Genomic analysis of ST88 community-acquired methicillin resistant Staphylococcus aureus in Ghana. PeerJ. 2017 Feb 28;5:e3047. High coverage artifacts: Li H. Toward better understanding of artifacts in variant calling from high-coverage samples. Bioinformatics. 2014 Oct 15;30(20):2843\u201351.","title":"More information"},{"location":"modules/snippy/#whats-next","text":"To use the tutorials on this website: see the list in the left hand panel or, click the menu button (three horizontal bars) in the top left of the page You can find more tutorials at the Galaxy Training Network: http://galaxyproject.github.io/training-material/","title":"What's next?"},{"location":"modules/snippy-core/","text":"Snippy-core Run Snippy - ref genome + sets of reads - one output is a whole-genome alignment of snps (not just the core-genome snps) - = gubbins Get data Upload all these to galaxy: the Staph aureus BPH2986.gbk (from prokka annotation) =25749 (from sepsis done) manually change datatype to genbank the read files from samples 25745-25748 (paired end) change datatype from fastq to fastqsanger Make dataset collections In Galaxy - see the tick box at the top of the history pane. Click on this - select all the files for the collection change the search bar to say R1 instead of -1 check the pairs auto pair create list name list save https://galaxyproject.org/tutorials/collections/ Run Snippy Reference type: Genbank. choose genbank file Single or Paired-end reads: Paired collection","title":"Snippy-core"},{"location":"modules/snippy-core/#snippy-core","text":"Run Snippy - ref genome + sets of reads - one output is a whole-genome alignment of snps (not just the core-genome snps) - = gubbins","title":"Snippy-core"},{"location":"modules/snippy-core/#get-data","text":"Upload all these to galaxy: the Staph aureus BPH2986.gbk (from prokka annotation) =25749 (from sepsis done) manually change datatype to genbank the read files from samples 25745-25748 (paired end) change datatype from fastq to fastqsanger","title":"Get data"},{"location":"modules/snippy-core/#make-dataset-collections","text":"In Galaxy - see the tick box at the top of the history pane. Click on this - select all the files for the collection change the search bar to say R1 instead of -1 check the pairs auto pair create list name list save https://galaxyproject.org/tutorials/collections/","title":"Make dataset collections"},{"location":"modules/snippy-core/#run-snippy","text":"Reference type: Genbank. choose genbank file Single or Paired-end reads: Paired collection","title":"Run Snippy"},{"location":"modules/spades/","text":"Assembly using Spades Spades is one of a number of de novo assemblers that use short read sets as input (e.g. Illumina Reads), and the assembly method is based on de Bruijn graphs. For information about Spades see this link . New to Galaxy? First try the introduction and then learn some key tasks The data The read set for today is from an imaginary Staphylococcus aureus bacterium with a miniature genome. The whole genome shotgun method used to sequence our mutant strain read set was produced on an Illumina DNA sequencing instrument. The files we need for assembly are the mutant_R1.fastq and mutant_R2.fastq . The reads are paired-end. Each read is 150 bases long. The number of bases sequenced is equivalent to 19x the genome sequence of the wildtype strain. (Read coverage 19x - rather low!). Import the data Log in to your Galaxy instance (for example, Galaxy Australia, usegalaxy.org.au ). Use shared data If you are using Galaxy Australia, you can import the data from a shared data library. In the top menu bar, go to Shared Data . Click on Data Libraries . Click on Galaxy Australia Training Material: Assembly: Microbial Asssembly . Tick the boxes next to the two files. Click the To History button, select As Datasets . Name a new history and click Import . In the top menu bar, click Analyze Data . You should now have two files in your current history. Or, import from the web Only follow this step if unable to load the data files from shared data, as described above . In a new browser tab, go to this webpage: Find the file called mutant_R1.fastq Right click on file name: select copy link address In Galaxy, go to Get Data and then Upload File Click Paste/Fetch data A box will appear: paste in link address Click Start Click Close The file will now appear in the top of your history panel. Repeat for mutant_R2.fastq . Shorten file names Click on the pencil icon next to the file name. In the centre Galaxy panel, click in the box under Name Shorten the file name to mutant_R1.fastq Then click Save We now have two FASTQ read files in our history. Click on the eye icon next to one of the FASTQ sequence files. View the file in the centre Galaxy panel. Quality Control If you want to check the quality of your reads, see the Quality Control tutorial . Note: Skip over the Import Data section and instead use the file called mutant_R1.fastq that is already in your current history. Assemble the reads We will perform a de novo assembly of the mutant FASTQ reads into long contiguous sequences (in FASTA format.) Go to the Tool panel and search for spades in the search box. Click on SPAdes Set the following parameters (leave other settings as they are): Run only Assembly : Yes [the Yes button should be darker grey] Kmers to use separated by commas: 33,55,91 [note: no spaces] Coverage cutoff: auto Files Forward reads: mutant_R1.fastq Files Reverse reads: mutant_R2.fastq Your tool interface should look like this: Click Execute How do I choose settings when running a tool? In this case, most of the default settings are appropriate for our data set and analysis. Under the tool interface in Galaxy there will usually be a more detailed description of the tool options, and a link to the tool s documentation. It is recommended that you read about the tool parameters in more detail in the documentation, and adjust to your data and analysis accordingly. Examine the output Galaxy is now running Spades on the reads for you. When it is finished, you will have five (or more) new files in your history, including: two FASTA files of the resulting contigs and scaffolds two files for statistics about these the Spades logfile To view the output, click on the eye icon next to each of the files. Note that the short reads have been assembled into much longer contigs. (However, in this case, the contigs have not been assembled into larger scaffolds.) The stats files will give you the length of each of the contigs, and the file should look something like this: Extension exercise Look at the contigs.stats file. Find a contig that seems to have high coverage relative to the other contigs. Extract this sequence from the contigs.fasta file. Select the sequence for the contig (called a Node) of interest, and copy. Go to the NCBI page and BLAST this sequence to see what it matches. Try the blastx option, which will translate your nucleotide sequence into a protein sequence. For Enter Query Sequence , paste your sequence into the box. For Genetic Code choose Bacteria and Archaea . For Database , try the SwissProt database. You can also re-try with other options to see how the database affects the results. All other options can be left as default. Click BLAST . What does your sequence match? Does this suggest that the sequence is a repeat region in this bacterial genome? For a detailed description of the output, see the top right corner of the page and click Blast report description . See this history in Galaxy If you want to see this Galaxy history without performing the steps above: Log in to Galaxy Australia: https://usegalaxy.org.au/ Go to Shared Data Click Histories Click Completed-assembly-analysis Click Import (at the top right corner) The analysis should now be showing as your current history. More information Here are some references covering more information about genome assembly. More about de Bruijn graphs: Compeau PEC, Pevzner PA, Tesler G. How to apply de Bruijn graphs to genome assembly. Nat Biotechnol. 2011 Nov 8;29(11):987\u201391. An assembler for long reads: Koren S, Walenz BP, Berlin K, Miller JR, Phillippy AM. Canu: scalable and accurate long-read assembly via adaptive k-mer weighting and repeat separation Genome Res. 2017 May;27(5):722-736. An assembler for large genomes: Jackman SD, Vandervalk BP, Mohamadi H, Chu J, Yeo S, Hammond SA, Jahesh G, Khan H, Coombe L, Warren RL, Birol I. ABySS 2.0: resource-efficient assembly of large genomes using a Bloom filter. Genome Res. 2017 May;27(5):768\u201377. Visualizing genome assemblies: Wick RR, Schultz MB, Zobel J, Holt KE. Bandage: interactive visualization of de novo genome assemblies. Bioinformatics. 2015 Oct 15;31(20):3350\u20132. Yeast genome assembly: Goodwin S, Gurtowski J, Ethe-Sayers S, Deshpande P, Schatz MC, McCombie WR. Oxford Nanopore sequencing, hybrid error correction, and de novo assembly of a eukaryotic genome. Genome Res. 2015 Nov;25(11):1750\u20136. Animal genome assembly: Austin CM, Tan MH, Harrisson KA, Lee YP, Croft LJ, Sunnucks P, Pavlova A, Gan HM. De novo genome assembly and annotation of Australia\u2019s largest freshwater fish, the Murray cod ( Maccullochella peelii ), from Illumina and Nanopore sequencing read. Gigascience. 2017 Aug 1;6(8):1\u20136. Human genome assembly: Chaisson MJP, Wilson RK, Eichler EE. Genetic variation and the de novo assembly of human genomes. Nat Rev Genet. 2015 Nov;16(11):627\u201340. Plant genome assembly: Jiao W-B, Schneeberger K. The impact of third generation genomic technologies on plant genome assembly. Curr Opin Plant Biol. 2017 Apr;36:64\u201370. What s next? To use the tutorials on this website: see the list in the left hand panel or, click the menu button (three horizontal bars) in the top left of the page You can find more tutorials at the Galaxy Training Network: http://galaxyproject.github.io/training-material/","title":"Genome assembly"},{"location":"modules/spades/#assembly-using-spades","text":"Spades is one of a number of de novo assemblers that use short read sets as input (e.g. Illumina Reads), and the assembly method is based on de Bruijn graphs. For information about Spades see this link . New to Galaxy? First try the introduction and then learn some key tasks","title":"Assembly using Spades"},{"location":"modules/spades/#the-data","text":"The read set for today is from an imaginary Staphylococcus aureus bacterium with a miniature genome. The whole genome shotgun method used to sequence our mutant strain read set was produced on an Illumina DNA sequencing instrument. The files we need for assembly are the mutant_R1.fastq and mutant_R2.fastq . The reads are paired-end. Each read is 150 bases long. The number of bases sequenced is equivalent to 19x the genome sequence of the wildtype strain. (Read coverage 19x - rather low!).","title":"The data"},{"location":"modules/spades/#import-the-data","text":"Log in to your Galaxy instance (for example, Galaxy Australia, usegalaxy.org.au ).","title":"Import the data"},{"location":"modules/spades/#use-shared-data","text":"If you are using Galaxy Australia, you can import the data from a shared data library. In the top menu bar, go to Shared Data . Click on Data Libraries . Click on Galaxy Australia Training Material: Assembly: Microbial Asssembly . Tick the boxes next to the two files. Click the To History button, select As Datasets . Name a new history and click Import . In the top menu bar, click Analyze Data . You should now have two files in your current history.","title":"Use shared data"},{"location":"modules/spades/#or-import-from-the-web","text":"Only follow this step if unable to load the data files from shared data, as described above . In a new browser tab, go to this webpage: Find the file called mutant_R1.fastq Right click on file name: select copy link address In Galaxy, go to Get Data and then Upload File Click Paste/Fetch data A box will appear: paste in link address Click Start Click Close The file will now appear in the top of your history panel. Repeat for mutant_R2.fastq .","title":"Or, import from the web"},{"location":"modules/spades/#shorten-file-names","text":"Click on the pencil icon next to the file name. In the centre Galaxy panel, click in the box under Name Shorten the file name to mutant_R1.fastq Then click Save We now have two FASTQ read files in our history. Click on the eye icon next to one of the FASTQ sequence files. View the file in the centre Galaxy panel.","title":"Shorten file names"},{"location":"modules/spades/#quality-control","text":"If you want to check the quality of your reads, see the Quality Control tutorial . Note: Skip over the Import Data section and instead use the file called mutant_R1.fastq that is already in your current history.","title":"Quality Control"},{"location":"modules/spades/#assemble-the-reads","text":"We will perform a de novo assembly of the mutant FASTQ reads into long contiguous sequences (in FASTA format.) Go to the Tool panel and search for spades in the search box. Click on SPAdes Set the following parameters (leave other settings as they are): Run only Assembly : Yes [the Yes button should be darker grey] Kmers to use separated by commas: 33,55,91 [note: no spaces] Coverage cutoff: auto Files Forward reads: mutant_R1.fastq Files Reverse reads: mutant_R2.fastq Your tool interface should look like this: Click Execute","title":"Assemble the reads"},{"location":"modules/spades/#how-do-i-choose-settings-when-running-a-tool","text":"In this case, most of the default settings are appropriate for our data set and analysis. Under the tool interface in Galaxy there will usually be a more detailed description of the tool options, and a link to the tool s documentation. It is recommended that you read about the tool parameters in more detail in the documentation, and adjust to your data and analysis accordingly.","title":"How do I choose settings when running a tool?"},{"location":"modules/spades/#examine-the-output","text":"Galaxy is now running Spades on the reads for you. When it is finished, you will have five (or more) new files in your history, including: two FASTA files of the resulting contigs and scaffolds two files for statistics about these the Spades logfile To view the output, click on the eye icon next to each of the files. Note that the short reads have been assembled into much longer contigs. (However, in this case, the contigs have not been assembled into larger scaffolds.) The stats files will give you the length of each of the contigs, and the file should look something like this:","title":"Examine the output"},{"location":"modules/spades/#extension-exercise","text":"Look at the contigs.stats file. Find a contig that seems to have high coverage relative to the other contigs. Extract this sequence from the contigs.fasta file. Select the sequence for the contig (called a Node) of interest, and copy. Go to the NCBI page and BLAST this sequence to see what it matches. Try the blastx option, which will translate your nucleotide sequence into a protein sequence. For Enter Query Sequence , paste your sequence into the box. For Genetic Code choose Bacteria and Archaea . For Database , try the SwissProt database. You can also re-try with other options to see how the database affects the results. All other options can be left as default. Click BLAST . What does your sequence match? Does this suggest that the sequence is a repeat region in this bacterial genome? For a detailed description of the output, see the top right corner of the page and click Blast report description .","title":"Extension exercise"},{"location":"modules/spades/#see-this-history-in-galaxy","text":"If you want to see this Galaxy history without performing the steps above: Log in to Galaxy Australia: https://usegalaxy.org.au/ Go to Shared Data Click Histories Click Completed-assembly-analysis Click Import (at the top right corner) The analysis should now be showing as your current history.","title":"See this history in Galaxy"},{"location":"modules/spades/#more-information","text":"Here are some references covering more information about genome assembly. More about de Bruijn graphs: Compeau PEC, Pevzner PA, Tesler G. How to apply de Bruijn graphs to genome assembly. Nat Biotechnol. 2011 Nov 8;29(11):987\u201391. An assembler for long reads: Koren S, Walenz BP, Berlin K, Miller JR, Phillippy AM. Canu: scalable and accurate long-read assembly via adaptive k-mer weighting and repeat separation Genome Res. 2017 May;27(5):722-736. An assembler for large genomes: Jackman SD, Vandervalk BP, Mohamadi H, Chu J, Yeo S, Hammond SA, Jahesh G, Khan H, Coombe L, Warren RL, Birol I. ABySS 2.0: resource-efficient assembly of large genomes using a Bloom filter. Genome Res. 2017 May;27(5):768\u201377. Visualizing genome assemblies: Wick RR, Schultz MB, Zobel J, Holt KE. Bandage: interactive visualization of de novo genome assemblies. Bioinformatics. 2015 Oct 15;31(20):3350\u20132. Yeast genome assembly: Goodwin S, Gurtowski J, Ethe-Sayers S, Deshpande P, Schatz MC, McCombie WR. Oxford Nanopore sequencing, hybrid error correction, and de novo assembly of a eukaryotic genome. Genome Res. 2015 Nov;25(11):1750\u20136. Animal genome assembly: Austin CM, Tan MH, Harrisson KA, Lee YP, Croft LJ, Sunnucks P, Pavlova A, Gan HM. De novo genome assembly and annotation of Australia\u2019s largest freshwater fish, the Murray cod ( Maccullochella peelii ), from Illumina and Nanopore sequencing read. Gigascience. 2017 Aug 1;6(8):1\u20136. Human genome assembly: Chaisson MJP, Wilson RK, Eichler EE. Genetic variation and the de novo assembly of human genomes. Nat Rev Genet. 2015 Nov;16(11):627\u201340. Plant genome assembly: Jiao W-B, Schneeberger K. The impact of third generation genomic technologies on plant genome assembly. Curr Opin Plant Biol. 2017 Apr;36:64\u201370.","title":"More information"},{"location":"modules/spades/#whats-next","text":"To use the tutorials on this website: see the list in the left hand panel or, click the menu button (three horizontal bars) in the top left of the page You can find more tutorials at the Galaxy Training Network: http://galaxyproject.github.io/training-material/","title":"What's next?"},{"location":"modules/spades_cmdline/","text":"Spades - commandline This tutorial follows on from PacBio assembly with commandline tools . Short-read assembly: a comparison So far, we have assembled the long PacBio reads into one contig (the chromosome) and found an additional plasmid in the Illumina short reads. If we only had Illumina reads, we could also assemble these using the tool Spades. You can try this here, or try it later on your own data. Get data We will use the same Illumina data as we used above: illumina_R1.fastq.gz : the Illumina forward reads illumina_R2.fastq.gz : the Illumina reverse reads This is from Sample 25747. Assemble Run Spades: spades.py -1 illumina_R1.fastq.gz -2 illumina_R2.fastq.gz --careful --cov-cutoff auto -o spades_assembly_all_illumina -1 is input file of forward reads -2 is input file of reverse reads --careful minimizes mismatches and short indels --cov-cutoff auto computes the coverage threshold (rather than the default setting, off ) -o is the output directory Results Move into the output directory and look at the contigs: infoseq contigs.fasta Next Run Prokka to annotate the contigs.","title":"Spades - commandline"},{"location":"modules/spades_cmdline/#spades-commandline","text":"This tutorial follows on from PacBio assembly with commandline tools .","title":"Spades - commandline"},{"location":"modules/spades_cmdline/#short-read-assembly-a-comparison","text":"So far, we have assembled the long PacBio reads into one contig (the chromosome) and found an additional plasmid in the Illumina short reads. If we only had Illumina reads, we could also assemble these using the tool Spades. You can try this here, or try it later on your own data.","title":"Short-read assembly: a comparison"},{"location":"modules/spades_cmdline/#get-data","text":"We will use the same Illumina data as we used above: illumina_R1.fastq.gz : the Illumina forward reads illumina_R2.fastq.gz : the Illumina reverse reads This is from Sample 25747.","title":"Get data"},{"location":"modules/spades_cmdline/#assemble","text":"Run Spades: spades.py -1 illumina_R1.fastq.gz -2 illumina_R2.fastq.gz --careful --cov-cutoff auto -o spades_assembly_all_illumina -1 is input file of forward reads -2 is input file of reverse reads --careful minimizes mismatches and short indels --cov-cutoff auto computes the coverage threshold (rather than the default setting, off ) -o is the output directory","title":"Assemble"},{"location":"modules/spades_cmdline/#results","text":"Move into the output directory and look at the contigs: infoseq contigs.fasta","title":"Results"},{"location":"modules/spades_cmdline/#next","text":"Run Prokka to annotate the contigs.","title":"Next"},{"location":"modules/trimmomatic/","text":"Trimming reads Introduction After checking your input sequence reads for quality (e.g. using FastQC) it might be necessary to trim the reads. Here we will use the Trimmomatic tool. For more information, see the Trimmomatic webpage and the manual. Learning Objectives At the end of this tutorial you should be able to: input sequence reads to Trimmomatic trim using appropriate parameters, and examine the output trimmed reads. Start Open your Galaxy instance. Find your quality-checked Illumina sequence reads. e.g. mutant_R1.fastq and mutant_R2.fastq We want to trim the parts of the reads that are of low quality. Based on the FastQC results, we might want to trim the reads like this: trim Illumina adapters leading and trailing bases - trim if quality is below 15 sliding window - trim once average quality is below 20 Trimmomatic functions Trim adapters This function trims adapters, barcodes and other contaminants from the reads. This should always be the first trimming step if it is used. You need to supply a FASTA file of possible adapter sequences, barcodes, etc. to trim. See the Trimmomatic webpage for detailed instructions. The default quality settings are sensible. Trim poor quality leading and trailing bases Trims bases from the start or end of a read if they drop below a certain quality. Use FastQC report to decide whether this step is warranted and what quality value to use. A quality threshold value of 10-15 is a good starting point. Trim poor quality sections This function uses a sliding window to measure average quality and trims accordingly. The default quality parameters are sensible for this step. Trim short reads Once all trimming steps are complete, this function makes sure that the reads are still longer than a specified minimum length. If not, the read is removed from the file and its pair is put into the orphan file. The most appropriate value for this parameter will depend on the FastQC report, specifically the length of the high quality section of the Per Base Sequence Quality graph. Each read library should be trimmed separately with parameters dependent on their own FastQC reports. Run Trimmomatic Go to Tools NGS Analysis NGS: QC and manipulation Trimmomatic . Paired end data? : Yes Input Type : Pair of datasets Input FASTQ file (R1/first of pair) : mutant_R1.fastq Input FASTQ file (R2/second of pair) : mutant_R2.fastq Perform initial ILLUMINACLIP step : Yes How accurate the match between the two adapter ligated reads must be for PE palindrome read alignment : 40 How accurate the match between any adapter etc. sequence must be against a read : 15 leave the first Trimmomatic Operation as is click on + Insert Trimmomatic Operation Select Trimmomatic operation to perform : Cut bases off the start of a read, if below a threshold quality (LEADING) Minimum quality required to keep a base : 15 click on + Insert Trimmomatic Operation Select Trimmomatic operation to perform : Cut bases off the end of a read, if below a threshold quality (TRAILING) Minimum quality required to keep a base : 15 click Execute Examine output Trimmomatic should produce 2 pairs files (1 left and 1 right hand end) and 1 or 2 single \u201corphaned reads\u201d files. The output files are the ones you should use for assembly. There are four output files, still in FASTQ format: R1 reads that have a pair in the R2 file R2 reads that have a pair in the R1 file R1 reads with no pair (R2 match was low quality: deleted) R2 reads with no pair (R1 match was low quality: deleted) Examine each file with the eye icon. Look for: Number of reads orphaned by the trimming / cleanup process. Number of pairs lost totally. What next? Next: use the output FASTQ files for Assembly, e.g. with Spades","title":"Trimming reads"},{"location":"modules/trimmomatic/#trimming-reads","text":"","title":"Trimming reads"},{"location":"modules/trimmomatic/#introduction","text":"After checking your input sequence reads for quality (e.g. using FastQC) it might be necessary to trim the reads. Here we will use the Trimmomatic tool. For more information, see the Trimmomatic webpage and the manual.","title":"Introduction"},{"location":"modules/trimmomatic/#learning-objectives","text":"At the end of this tutorial you should be able to: input sequence reads to Trimmomatic trim using appropriate parameters, and examine the output trimmed reads.","title":"Learning Objectives"},{"location":"modules/trimmomatic/#start","text":"Open your Galaxy instance. Find your quality-checked Illumina sequence reads. e.g. mutant_R1.fastq and mutant_R2.fastq We want to trim the parts of the reads that are of low quality. Based on the FastQC results, we might want to trim the reads like this: trim Illumina adapters leading and trailing bases - trim if quality is below 15 sliding window - trim once average quality is below 20","title":"Start"},{"location":"modules/trimmomatic/#trimmomatic-functions","text":"","title":"Trimmomatic functions"},{"location":"modules/trimmomatic/#trim-adapters","text":"This function trims adapters, barcodes and other contaminants from the reads. This should always be the first trimming step if it is used. You need to supply a FASTA file of possible adapter sequences, barcodes, etc. to trim. See the Trimmomatic webpage for detailed instructions. The default quality settings are sensible.","title":"Trim adapters"},{"location":"modules/trimmomatic/#trim-poor-quality-leading-and-trailing-bases","text":"Trims bases from the start or end of a read if they drop below a certain quality. Use FastQC report to decide whether this step is warranted and what quality value to use. A quality threshold value of 10-15 is a good starting point.","title":"Trim poor quality leading and trailing bases"},{"location":"modules/trimmomatic/#trim-poor-quality-sections","text":"This function uses a sliding window to measure average quality and trims accordingly. The default quality parameters are sensible for this step.","title":"Trim poor quality sections"},{"location":"modules/trimmomatic/#trim-short-reads","text":"Once all trimming steps are complete, this function makes sure that the reads are still longer than a specified minimum length. If not, the read is removed from the file and its pair is put into the orphan file. The most appropriate value for this parameter will depend on the FastQC report, specifically the length of the high quality section of the Per Base Sequence Quality graph. Each read library should be trimmed separately with parameters dependent on their own FastQC reports.","title":"Trim short reads"},{"location":"modules/trimmomatic/#run-trimmomatic","text":"Go to Tools NGS Analysis NGS: QC and manipulation Trimmomatic . Paired end data? : Yes Input Type : Pair of datasets Input FASTQ file (R1/first of pair) : mutant_R1.fastq Input FASTQ file (R2/second of pair) : mutant_R2.fastq Perform initial ILLUMINACLIP step : Yes How accurate the match between the two adapter ligated reads must be for PE palindrome read alignment : 40 How accurate the match between any adapter etc. sequence must be against a read : 15 leave the first Trimmomatic Operation as is click on + Insert Trimmomatic Operation Select Trimmomatic operation to perform : Cut bases off the start of a read, if below a threshold quality (LEADING) Minimum quality required to keep a base : 15 click on + Insert Trimmomatic Operation Select Trimmomatic operation to perform : Cut bases off the end of a read, if below a threshold quality (TRAILING) Minimum quality required to keep a base : 15 click Execute","title":"Run Trimmomatic"},{"location":"modules/trimmomatic/#examine-output","text":"Trimmomatic should produce 2 pairs files (1 left and 1 right hand end) and 1 or 2 single \u201corphaned reads\u201d files. The output files are the ones you should use for assembly. There are four output files, still in FASTQ format: R1 reads that have a pair in the R2 file R2 reads that have a pair in the R1 file R1 reads with no pair (R2 match was low quality: deleted) R2 reads with no pair (R1 match was low quality: deleted) Examine each file with the eye icon. Look for: Number of reads orphaned by the trimming / cleanup process. Number of pairs lost totally.","title":"Examine output"},{"location":"modules/trimmomatic/#what-next","text":"Next: use the output FASTQ files for Assembly, e.g. with Spades","title":"What next?"},{"location":"modules/variants/","text":"Variant calling: Humans Introduction Now updated for Galaxy Australia http://melbournebioinformatics.github.io/MelBioInf_docs/tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/ Advanced Not yet updated for Galaxy Australia https://www.melbournebioinformatics.org.au/tutorials/tutorials/var_detect_advanced/var_detect_advanced/","title":"Variant calling - human"},{"location":"modules/variants/#variant-calling-humans","text":"","title":"Variant calling: Humans"},{"location":"modules/variants/#introduction","text":"Now updated for Galaxy Australia http://melbournebioinformatics.github.io/MelBioInf_docs/tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/","title":"Introduction"},{"location":"modules/variants/#advanced","text":"Not yet updated for Galaxy Australia https://www.melbournebioinformatics.org.au/tutorials/tutorials/var_detect_advanced/var_detect_advanced/","title":"Advanced"},{"location":"modules/velvet/","text":"Assembly using Velvet Keywords: de novo assembly, Velvet, Galaxy, Microbial Genomics Virtual Lab Background Velvet is one of a number of de novo assemblers that use short read sets as input (e.g. Illumina Reads), and the assembly method is based on de Bruijn graphs. For information about Velvet see this link . In this activity, we will perform a de novo assembly of a short read set using the Velvet assembler. Learning objectives At the end of this tutorial you should be able to: assemble the reads using Velvet, and examine the output assembly. Import and view data If you have completed the previous tutorial on Quality Control , you should already have the required files in your current Galaxy history. If not, see how to get them here . The data The read set for today is from an imaginary Staphylococcus aureus bacterium with a miniature genome. The whole genome shotgun method used to sequence our mutant strain read set was produced on an Illumina DNA sequencing instrument. The files we need for assembly are the mutant_R1.fastq and mutant_R2.fastq . (We don t need the reference genome sequences for this tutorial). The reads are paired-end. Each read is 150 bases long. The number of bases sequenced is equivalent to 19x the genome sequence of the wildtype strain. (Read coverage 19x - rather low!). Click on the View Data button (the ) next to one of the FASTQ sequence files. Assemble reads with Velvet We will perform a de novo assembly of the mutant FASTQ reads into long contiguous sequences (in FASTA format.) Velvet requires the user to input a value of k for the assembly process. K-mers are fragments of sequence reads. Small k-mers will give greater connectivity, but large k-mers will give better specificity. Go to Tools NGS Analysis NGS: Assembly velvet Set the following parameters (leave other settings as they are): K-mer : Enter the value for k that you have been assigned in the spreadsheet. Input file type : Fastq Single or paired end reads : Paired Select first set of reads : mutant_R1.fastq Select second set of reads : mutant_R2.fastq Your tool interface should look like this (you will most likely have a different value for k): Click Execute Examine the output Galaxy is now running velvet on the reads for you. Press the refresh button in the history pane to see if it has finished. When it is finished, you will have four new files in your history. a Contigs file a Contigs stats file the velvet log file an assembly Last Graph file Click on the View Data button on each of the files. The Contigs file will show each contig with the k-mer length and k-mer coverage listed as part of the header (however, these are just called length and coverage ). K-mer length : For the value of k chosen in the assembly, a measure of how many k-mers overlap (by 1 bp each overlap) to give this length. K-mer coverage : For the value of k chosen in the assembly, a measure of how many k-mers overlap each base position (in the assembly). The Contigs stats file will show a list of these k-mer lengths and k-mer coverages. We will summarise the information in the log file. Go to NGS Common Toolsets FASTA manipulation Fasta statistics For the required input file, choose the velvet Contigs file. Click Execute . A new file will appear called Fasta summary stats Click the eye icon to look at this file. Look at: num_seq : the number of contigs in the FASTA file. num_bp : the number of assembled bases. Roughly proportional to genome size. len_max : the biggest contig. len_N50 : N50 is a contig size. If contigs were ordered from small to large, half of all the nucleotides will be in contigs this size or larger. Now copy the relevant data back into the k-mer spreadsheet on your line. Along with the demonstrator, have a look at the effect of the k-mer size on the output metrics of the assembly. Note that there are local maxima and minima in the charts. Assembly with Velvet Optimiser Now that we have seen the effect of k-mer size on the assembly, we will run the Velvet Optimiser to automatically choose the best k-mer size for us. It will use the n50 to determine the best k-mer value to use. It then performs the further graph cleaning steps and automatically chooses other parameters for velvet. We should get a much better assembly result than we did with our attempts with Velvet alone. Go to Tools NGS Analysis NGS: Assembly Velvet Optimiser Set the following parameters (leave other settings as they are): Start k-mer size : 45 End k-mer size : 73 Input file type : Fastq Single or paired end reads : Paired Select first set of reads : mutant_R1.fastq Select second set of reads : mutant_R2.fastq Click Execute Use the Fasta Statistics tool you used earlier to summarise the Velvet Optimiser Contigs output. Examine the resulting table. What are the main differences?","title":"Home"},{"location":"modules/velvet/#assembly-using-velvet","text":"Keywords: de novo assembly, Velvet, Galaxy, Microbial Genomics Virtual Lab","title":"Assembly using Velvet"},{"location":"modules/velvet/#background","text":"Velvet is one of a number of de novo assemblers that use short read sets as input (e.g. Illumina Reads), and the assembly method is based on de Bruijn graphs. For information about Velvet see this link . In this activity, we will perform a de novo assembly of a short read set using the Velvet assembler.","title":"Background"},{"location":"modules/velvet/#learning-objectives","text":"At the end of this tutorial you should be able to: assemble the reads using Velvet, and examine the output assembly.","title":"Learning objectives"},{"location":"modules/velvet/#import-and-view-data","text":"If you have completed the previous tutorial on Quality Control , you should already have the required files in your current Galaxy history. If not, see how to get them here .","title":"Import and view data"},{"location":"modules/velvet/#the-data","text":"The read set for today is from an imaginary Staphylococcus aureus bacterium with a miniature genome. The whole genome shotgun method used to sequence our mutant strain read set was produced on an Illumina DNA sequencing instrument. The files we need for assembly are the mutant_R1.fastq and mutant_R2.fastq . (We don t need the reference genome sequences for this tutorial). The reads are paired-end. Each read is 150 bases long. The number of bases sequenced is equivalent to 19x the genome sequence of the wildtype strain. (Read coverage 19x - rather low!). Click on the View Data button (the ) next to one of the FASTQ sequence files.","title":"The data"},{"location":"modules/velvet/#assemble-reads-with-velvet","text":"We will perform a de novo assembly of the mutant FASTQ reads into long contiguous sequences (in FASTA format.) Velvet requires the user to input a value of k for the assembly process. K-mers are fragments of sequence reads. Small k-mers will give greater connectivity, but large k-mers will give better specificity. Go to Tools NGS Analysis NGS: Assembly velvet Set the following parameters (leave other settings as they are): K-mer : Enter the value for k that you have been assigned in the spreadsheet. Input file type : Fastq Single or paired end reads : Paired Select first set of reads : mutant_R1.fastq Select second set of reads : mutant_R2.fastq Your tool interface should look like this (you will most likely have a different value for k): Click Execute","title":"Assemble reads with Velvet"},{"location":"modules/velvet/#examine-the-output","text":"Galaxy is now running velvet on the reads for you. Press the refresh button in the history pane to see if it has finished. When it is finished, you will have four new files in your history. a Contigs file a Contigs stats file the velvet log file an assembly Last Graph file Click on the View Data button on each of the files. The Contigs file will show each contig with the k-mer length and k-mer coverage listed as part of the header (however, these are just called length and coverage ). K-mer length : For the value of k chosen in the assembly, a measure of how many k-mers overlap (by 1 bp each overlap) to give this length. K-mer coverage : For the value of k chosen in the assembly, a measure of how many k-mers overlap each base position (in the assembly). The Contigs stats file will show a list of these k-mer lengths and k-mer coverages. We will summarise the information in the log file. Go to NGS Common Toolsets FASTA manipulation Fasta statistics For the required input file, choose the velvet Contigs file. Click Execute . A new file will appear called Fasta summary stats Click the eye icon to look at this file. Look at: num_seq : the number of contigs in the FASTA file. num_bp : the number of assembled bases. Roughly proportional to genome size. len_max : the biggest contig. len_N50 : N50 is a contig size. If contigs were ordered from small to large, half of all the nucleotides will be in contigs this size or larger. Now copy the relevant data back into the k-mer spreadsheet on your line. Along with the demonstrator, have a look at the effect of the k-mer size on the output metrics of the assembly. Note that there are local maxima and minima in the charts.","title":"Examine the output"},{"location":"modules/velvet/#assembly-with-velvet-optimiser","text":"Now that we have seen the effect of k-mer size on the assembly, we will run the Velvet Optimiser to automatically choose the best k-mer size for us. It will use the n50 to determine the best k-mer value to use. It then performs the further graph cleaning steps and automatically chooses other parameters for velvet. We should get a much better assembly result than we did with our attempts with Velvet alone. Go to Tools NGS Analysis NGS: Assembly Velvet Optimiser Set the following parameters (leave other settings as they are): Start k-mer size : 45 End k-mer size : 73 Input file type : Fastq Single or paired end reads : Paired Select first set of reads : mutant_R1.fastq Select second set of reads : mutant_R2.fastq Click Execute Use the Fasta Statistics tool you used earlier to summarise the Velvet Optimiser Contigs output. Examine the resulting table. What are the main differences?","title":"Assembly with Velvet Optimiser"},{"location":"modules/viral_genomes/","text":"Viral Genome Sequencing This tutorial is about determining the genome sequence of a virus, using comparisons to a reference sequence. Background Murray Valley encephalitis virus is classified as part of the Flavivirus genus of viruses, all of which are positive strand RNA viruses. The type species for this genus is yellow fever virus, and the genus includes dengue virus, west nile virus and zika virus. Viruses from this genus have a single-segment genome of about 11 kb. Within this, there is a 10 kb open reading frame that encodes a polyprotein. Following translation, this protein is modified to yield the various, mature structural and non-structural viral proteins as illustrated in Fig. 1. Fig. 1 - An overview of a typical flavivirus genome and the viral proteins that are part of the viral polyprotein (from Viralzone ). There are 14 complete Murray Valley encephalitis virus genome sequences available at NCBI Viral Genomes. Go to NCBI Viral Genomes . Select Browse viral genomes by family and click on the family Flaviviridae : Complete Genomes Note: you may have to widen your screen to see all the columns of viral family names. Look at the row for Murray Valley encephelitis virus. In the Neigbours column, we can see there are 14. In this tutorial we will use the prototype strain 1-151 as the reference genome sequence. This strain was isolated in the early 1950s - see AF161266 . The isolate we are looking at has been sequenced using the Illumina platform. The genomic cDNA was prepared using random hexamers to prime the reverse transcription of the viral genomic RNA. After second strand synthesis, the cDNA was used to prepare an Illumina sequencing library and run on an Illumina MiSeq instrument. In this activity we will use a read mapping approach to determine the sequence of a new Murray Valley encephalitis virus isolate. Import data Section overview: Log in to your Galaxy server Import files required for the activity View imported files Go to the Galaxy Page Web address: http://phln.genome.edu.au/galaxy Remind me how to logon Import files to Galaxy Click on the Analyze Data menu at the top of the page. Click on the History menu button (the on the top right of the history pane) Click Import from File (at the bottom of the list) A new page will appear with a text box for the URL of the history to import. Copy the following URL into the text box: http://phln.genome.edu.au/public/dieter/Galaxy-History-MVEVmapping.tar.gz Click Submit Galaxy will download the data files from the internet and will be available as an additional history (takes about one minute). To make the newly imported history appear as the current history: Click on the View all Histories button (the on the top right of the history pane.) If the history has finished downloading it will appear with the title: \u201cimported from archive: MVEVmapping\u201c Click on the Switch to button above this history and then the Done button. You should now have 4 files in the history pane as follows: Reference sequence files : MVEV.gbk - genbank format MVEV.fna - fasta format Illumina sequence reads (R1 and R2) from the new isolate: MVE_R1.fq - forward reads MVE_R2.fq - reverse reads Snippy Section overview: Find variants in the isolate using the tool Snippy. Snippy is a fast variant caller for haploid genomes. The software is available on GitHub at https://github.com/tseemann/snippy . For this activity, we are using Snippy as installed on Galaxy. Preliminary Activity Run FastQC: How many reads in in each of the fastq files MVE_R1.fq and MVE_R2.fq ? MVEV.fna and MVEV.gbk each contain the genome sequence of Murray Valley encephalitis virus strain 1-151 - how many bases in the genome? (Hint: use Fasta Statistics) Running Snippy Snippy maps reads from the new Murray Valley encephalitis virus isolate (the MVE_R1.fq and MVE_R2.fq reads) onto the genome sequence of Murray Valley encephalitis virus strain 1-151 ( MVEV.fna ). Find Snippy in the tool menu (in NGS: Variant Analysis) Select appropriate files (see screenshot below) and Execute (use default settings). Output Files cataloging SNP differences: 9: snippy on data 4, data 3, and data 2 snps vcf file 10: snippy on data 4, data 3, and data 2 snps gff file 11: snippy on data 4, data 3, and data 2 snps table 12: snippy on data 4, data 3, and data 2 snps summary A log of the progress of the run: 13: snippy on data 4, data 3, and data 2 log file Regions where reads aligned (NNNN , indicate regions where there was low or no read data): 14: snippy on data 4, data 3, and data 2 aligned fasta A consensus genome sequence for the new isolate: 15: snippy on data 4, data 3, and data 2 consensus fasta Summary of the read depth: 16: snippy on data 4, data 3, and data 2 mapping depth A compressed version of the above files (and more that can be downloaded): 17: snippy on data 4, data 3, and data 2 out dir Download these files to your local computer (click on the file name and then the disk icon in the lower left hand corner): 17: snippy on data 4, data 3, and data 2 out dir (and unzip) 2: MVEV.gbk 1: MVEV.fna Also download these bam files from these URLs (open each URL in a new tab and the file should download automatically): http://phln.genome.edu.au/public/dieter/snps.bam http://phln.genome.edu.au/public/dieter/snps.bam.bai http://phln.genome.edu.au/public/dieter/snps.depth.gz http://phln.genome.edu.au/public/dieter/snps.depth.gz.tbi Note: if you have previously downloaded these files, the new downloads may be renamed. Remove any spaces in the names. Artemis Section overview: View the reads from the new isolate mapped against the reference sequence, using the tool Artemis. Artemis is a tool to view genome sequences and mapped reads, including variants (SNPs). If Artemis is not installed, go to http://www.sanger.ac.uk/science/tools/artemis . View the reference sequence Open Artemis. Go to File: Open and select MVEV.gbk . The file will probably have a Galaxy prefix, e.g. Galaxy2-[MVEV.gbk].genbank . The Artemis window: panes 1 and 2 are the same, but can be scaled differently each pane has the double-stranded sequence in the centre, with amino acid translations above and below there is a third lower pane with feature information coding sequences are highlighted in blue other features are highlighted in green clicking on one of these will select it in all panels black vertical lines are stop codons (when zoomed out) move left and right with horizontal scroll bar zoom in and out with right-hand scroll bar Add a plot Go to Graph: Add User Plot , select snps.depth.gz A graph should display at the top of the screen Producing a draft genome sequence Section overview: Produce draft genome sequence for the new viral isolate. Examine the mapped reads What is the minimum read depth used by Snippy to call a SNP? Hint: Go back to Snippy on Galaxy - look at the information below the \u2018Execute\u2019 button Would Snippy call a SNP at positions 1 \u2192 5 ? What is the maximum read depth? What do we know about the sequence of the new isolate in the regions where there is low read coverage? Unzip 17: snippy on data 4, data 3, and data 2 out dir This makes an out folder containing some files including the consensus file. In Artemis, open snps.consensus.fa This is a file that is based on the reference sequence and includes any confirmed SNPs called by Snippy. If we were going to use this sequence to produce the draft sequence of the new isolate, what bases would you have at positions 1\u2192 5? Getting an Overview of the Difference between strain 1-151 and our new isolate Simplest overview: view the bam file with Artemis Open MVEV.gbk in Artemis and load the snps.bam via the File menu to \u201cRead BAM / VCF\u201d Once loaded, differences between reads and the reference sequence can be highlighted by right/command click in the bam view window. Select \u2018Show SNP marks\u2019. Getting more detail: looking at the table of SNPs Located in the out folder there is a html file that contains a table with information about each of the SNPs called by Snippy. Included in the table is a column that provides a prediction about the impact each SNP will have on annotated protein coding regions. The genbank file provides the annotation information used by Snippy to make the predictions. A total of 790 SNP differences were call by Snippy Open snps.html in your web browser Summary: 663/790 SNPs do not result a difference in the encoded polyprotein Is this pattern of variable genome sequence and more conserved protein sequence normal in viruses? What might be the cause?","title":"Home"},{"location":"modules/viral_genomes/#viral-genome-sequencing","text":"This tutorial is about determining the genome sequence of a virus, using comparisons to a reference sequence.","title":"Viral Genome Sequencing"},{"location":"modules/viral_genomes/#background","text":"Murray Valley encephalitis virus is classified as part of the Flavivirus genus of viruses, all of which are positive strand RNA viruses. The type species for this genus is yellow fever virus, and the genus includes dengue virus, west nile virus and zika virus. Viruses from this genus have a single-segment genome of about 11 kb. Within this, there is a 10 kb open reading frame that encodes a polyprotein. Following translation, this protein is modified to yield the various, mature structural and non-structural viral proteins as illustrated in Fig. 1. Fig. 1 - An overview of a typical flavivirus genome and the viral proteins that are part of the viral polyprotein (from Viralzone ). There are 14 complete Murray Valley encephalitis virus genome sequences available at NCBI Viral Genomes. Go to NCBI Viral Genomes . Select Browse viral genomes by family and click on the family Flaviviridae : Complete Genomes Note: you may have to widen your screen to see all the columns of viral family names. Look at the row for Murray Valley encephelitis virus. In the Neigbours column, we can see there are 14. In this tutorial we will use the prototype strain 1-151 as the reference genome sequence. This strain was isolated in the early 1950s - see AF161266 . The isolate we are looking at has been sequenced using the Illumina platform. The genomic cDNA was prepared using random hexamers to prime the reverse transcription of the viral genomic RNA. After second strand synthesis, the cDNA was used to prepare an Illumina sequencing library and run on an Illumina MiSeq instrument. In this activity we will use a read mapping approach to determine the sequence of a new Murray Valley encephalitis virus isolate.","title":"Background"},{"location":"modules/viral_genomes/#import-data","text":"","title":"Import data"},{"location":"modules/viral_genomes/#section-overview","text":"Log in to your Galaxy server Import files required for the activity View imported files","title":"Section overview:"},{"location":"modules/viral_genomes/#go-to-the-galaxy-page","text":"Web address: http://phln.genome.edu.au/galaxy Remind me how to logon","title":"Go to the Galaxy Page"},{"location":"modules/viral_genomes/#import-files-to-galaxy","text":"Click on the Analyze Data menu at the top of the page. Click on the History menu button (the on the top right of the history pane) Click Import from File (at the bottom of the list) A new page will appear with a text box for the URL of the history to import. Copy the following URL into the text box: http://phln.genome.edu.au/public/dieter/Galaxy-History-MVEVmapping.tar.gz Click Submit Galaxy will download the data files from the internet and will be available as an additional history (takes about one minute).","title":"Import files to Galaxy"},{"location":"modules/viral_genomes/#to-make-the-newly-imported-history-appear-as-the-current-history","text":"Click on the View all Histories button (the on the top right of the history pane.) If the history has finished downloading it will appear with the title: \u201cimported from archive: MVEVmapping\u201c Click on the Switch to button above this history and then the Done button. You should now have 4 files in the history pane as follows: Reference sequence files : MVEV.gbk - genbank format MVEV.fna - fasta format Illumina sequence reads (R1 and R2) from the new isolate: MVE_R1.fq - forward reads MVE_R2.fq - reverse reads","title":"To make the newly imported history appear as the current history:"},{"location":"modules/viral_genomes/#snippy","text":"","title":"Snippy"},{"location":"modules/viral_genomes/#section-overview_1","text":"Find variants in the isolate using the tool Snippy. Snippy is a fast variant caller for haploid genomes. The software is available on GitHub at https://github.com/tseemann/snippy . For this activity, we are using Snippy as installed on Galaxy.","title":"Section overview:"},{"location":"modules/viral_genomes/#preliminary-activity","text":"Run FastQC: How many reads in in each of the fastq files MVE_R1.fq and MVE_R2.fq ? MVEV.fna and MVEV.gbk each contain the genome sequence of Murray Valley encephalitis virus strain 1-151 - how many bases in the genome? (Hint: use Fasta Statistics)","title":"Preliminary Activity"},{"location":"modules/viral_genomes/#running-snippy","text":"Snippy maps reads from the new Murray Valley encephalitis virus isolate (the MVE_R1.fq and MVE_R2.fq reads) onto the genome sequence of Murray Valley encephalitis virus strain 1-151 ( MVEV.fna ). Find Snippy in the tool menu (in NGS: Variant Analysis) Select appropriate files (see screenshot below) and Execute (use default settings).","title":"Running Snippy"},{"location":"modules/viral_genomes/#output","text":"Files cataloging SNP differences: 9: snippy on data 4, data 3, and data 2 snps vcf file 10: snippy on data 4, data 3, and data 2 snps gff file 11: snippy on data 4, data 3, and data 2 snps table 12: snippy on data 4, data 3, and data 2 snps summary A log of the progress of the run: 13: snippy on data 4, data 3, and data 2 log file Regions where reads aligned (NNNN , indicate regions where there was low or no read data): 14: snippy on data 4, data 3, and data 2 aligned fasta A consensus genome sequence for the new isolate: 15: snippy on data 4, data 3, and data 2 consensus fasta Summary of the read depth: 16: snippy on data 4, data 3, and data 2 mapping depth A compressed version of the above files (and more that can be downloaded): 17: snippy on data 4, data 3, and data 2 out dir Download these files to your local computer (click on the file name and then the disk icon in the lower left hand corner): 17: snippy on data 4, data 3, and data 2 out dir (and unzip) 2: MVEV.gbk 1: MVEV.fna Also download these bam files from these URLs (open each URL in a new tab and the file should download automatically): http://phln.genome.edu.au/public/dieter/snps.bam http://phln.genome.edu.au/public/dieter/snps.bam.bai http://phln.genome.edu.au/public/dieter/snps.depth.gz http://phln.genome.edu.au/public/dieter/snps.depth.gz.tbi Note: if you have previously downloaded these files, the new downloads may be renamed. Remove any spaces in the names.","title":"Output"},{"location":"modules/viral_genomes/#artemis","text":"","title":"Artemis"},{"location":"modules/viral_genomes/#section-overview_2","text":"View the reads from the new isolate mapped against the reference sequence, using the tool Artemis. Artemis is a tool to view genome sequences and mapped reads, including variants (SNPs). If Artemis is not installed, go to http://www.sanger.ac.uk/science/tools/artemis .","title":"Section overview:"},{"location":"modules/viral_genomes/#view-the-reference-sequence","text":"Open Artemis. Go to File: Open and select MVEV.gbk . The file will probably have a Galaxy prefix, e.g. Galaxy2-[MVEV.gbk].genbank .","title":"View the reference sequence"},{"location":"modules/viral_genomes/#the-artemis-window","text":"panes 1 and 2 are the same, but can be scaled differently each pane has the double-stranded sequence in the centre, with amino acid translations above and below there is a third lower pane with feature information coding sequences are highlighted in blue other features are highlighted in green clicking on one of these will select it in all panels black vertical lines are stop codons (when zoomed out) move left and right with horizontal scroll bar zoom in and out with right-hand scroll bar","title":"The Artemis window:"},{"location":"modules/viral_genomes/#add-a-plot","text":"Go to Graph: Add User Plot , select snps.depth.gz A graph should display at the top of the screen","title":"Add a plot"},{"location":"modules/viral_genomes/#producing-a-draft-genome-sequence","text":"","title":"Producing a draft genome sequence"},{"location":"modules/viral_genomes/#section-overview_3","text":"Produce draft genome sequence for the new viral isolate.","title":"Section overview:"},{"location":"modules/viral_genomes/#examine-the-mapped-reads","text":"What is the minimum read depth used by Snippy to call a SNP? Hint: Go back to Snippy on Galaxy - look at the information below the \u2018Execute\u2019 button Would Snippy call a SNP at positions 1 \u2192 5 ? What is the maximum read depth? What do we know about the sequence of the new isolate in the regions where there is low read coverage? Unzip 17: snippy on data 4, data 3, and data 2 out dir This makes an out folder containing some files including the consensus file. In Artemis, open snps.consensus.fa This is a file that is based on the reference sequence and includes any confirmed SNPs called by Snippy. If we were going to use this sequence to produce the draft sequence of the new isolate, what bases would you have at positions 1\u2192 5?","title":"Examine the mapped reads"},{"location":"modules/viral_genomes/#getting-an-overview-of-the-difference-between-strain-1-151-and-our-new-isolate","text":"Simplest overview: view the bam file with Artemis Open MVEV.gbk in Artemis and load the snps.bam via the File menu to \u201cRead BAM / VCF\u201d Once loaded, differences between reads and the reference sequence can be highlighted by right/command click in the bam view window. Select \u2018Show SNP marks\u2019.","title":"Getting an Overview of the Difference between strain 1-151 and our new isolate"},{"location":"modules/viral_genomes/#getting-more-detail-looking-at-the-table-of-snps","text":"Located in the out folder there is a html file that contains a table with information about each of the SNPs called by Snippy. Included in the table is a column that provides a prediction about the impact each SNP will have on annotated protein coding regions. The genbank file provides the annotation information used by Snippy to make the predictions. A total of 790 SNP differences were call by Snippy Open snps.html in your web browser Summary: 663/790 SNPs do not result a difference in the encoded polyprotein Is this pattern of variable genome sequence and more conserved protein sequence normal in viruses? What might be the cause?","title":"Getting more detail: looking at the table of SNPs"},{"location":"modules/workflow-annotation/","text":"Workflow: Genome Annotation Create a Galaxy workflow Galaxy provides the option to extract a workflow based on the steps you just followed and the tools and parameters used. New to Galaxy? First try the introduction and then learn some key tasks Genome annotation Log in to your Galaxy instance (for example, Galaxy Australia, usegalaxy.org.au ). Before we create a workflow for genome annotation, we need the correct files in our current Galaxy history. We can get these files in two ways: Either follow the tutorial on genome annotation Or, obtain the files from a Shared History (Galaxy Australia only). Go to Shared Data Click Histories Click Completed-annotation-analysis Click Import (at the top right corner) The analysis should now be showing as your current history. When you have completed this tutorial or imported the history, you will have the correct files in your history panel. Extract the Workflow In the history panel, click on the cog icon. Click Extract Workflow If all looks ok, click Create Workflow In the centre panel, click edit This brings up the workflow canvas, a space for graphically viewing and re-arranging your workflow. Drag the panes around until your workflow is clear. In the top right corner click on the cog icon and Save Click on the cog icon again and Run Send results to a new history : Yes Check all input files are correct and change if necessary In the top right corner Run workflow This workflow will now run. In the History panel, click on the View all histories icon Find your workflow history and Switch to it. To see all your workflows, go to the top panel in Galaxy and click Workflow . You can edit or run workflows from here by clicking on the drop-down arrow next to each workflow. What s next? To use the tutorials on this website: see the list in the left hand panel or, click the menu button (three horizontal bars) in the top left of the page You can find more tutorials at the Galaxy Training Network: http://galaxyproject.github.io/training-material/","title":"Workflow - genome annotation"},{"location":"modules/workflow-annotation/#workflow-genome-annotation","text":"","title":"Workflow: Genome Annotation"},{"location":"modules/workflow-annotation/#create-a-galaxy-workflow","text":"Galaxy provides the option to extract a workflow based on the steps you just followed and the tools and parameters used. New to Galaxy? First try the introduction and then learn some key tasks","title":"Create a Galaxy workflow"},{"location":"modules/workflow-annotation/#genome-annotation","text":"Log in to your Galaxy instance (for example, Galaxy Australia, usegalaxy.org.au ). Before we create a workflow for genome annotation, we need the correct files in our current Galaxy history. We can get these files in two ways: Either follow the tutorial on genome annotation Or, obtain the files from a Shared History (Galaxy Australia only). Go to Shared Data Click Histories Click Completed-annotation-analysis Click Import (at the top right corner) The analysis should now be showing as your current history. When you have completed this tutorial or imported the history, you will have the correct files in your history panel.","title":"Genome annotation"},{"location":"modules/workflow-annotation/#extract-the-workflow","text":"In the history panel, click on the cog icon. Click Extract Workflow If all looks ok, click Create Workflow In the centre panel, click edit This brings up the workflow canvas, a space for graphically viewing and re-arranging your workflow. Drag the panes around until your workflow is clear. In the top right corner click on the cog icon and Save Click on the cog icon again and Run Send results to a new history : Yes Check all input files are correct and change if necessary In the top right corner Run workflow This workflow will now run. In the History panel, click on the View all histories icon Find your workflow history and Switch to it. To see all your workflows, go to the top panel in Galaxy and click Workflow . You can edit or run workflows from here by clicking on the drop-down arrow next to each workflow.","title":"Extract the Workflow"},{"location":"modules/workflow-annotation/#whats-next","text":"To use the tutorials on this website: see the list in the left hand panel or, click the menu button (three horizontal bars) in the top left of the page You can find more tutorials at the Galaxy Training Network: http://galaxyproject.github.io/training-material/","title":"What's next?"},{"location":"modules/workflow-assembly/","text":"Workflow: Genome Assembly Create a Galaxy workflow Galaxy provides the option to extract a workflow based on the steps you just followed and the tools and parameters used. New to Galaxy? First try the introduction and then learn some key tasks Genome assembly Log in to your Galaxy instance (for example, Galaxy Australia, usegalaxy.org.au ). Before we create a workflow for genome assembly, we need the correct files in our current Galaxy history. We can get these files in two ways: Either follow the tutorial on genome assembly Or, obtain the files from a Shared History (Galaxy Australia only). Go to Shared Data Click Histories Click Completed-assembly-analysis Click Import (at the top right corner) The analysis should now be showing as your current history. When you have completed this tutorial or imported the history, you will have the correct files in your history panel. Extract the Workflow In the history panel, click on the cog icon. Click Extract Workflow If all looks ok, click Create Workflow In the centre panel, click edit This brings up the workflow canvas, a space for graphically viewing and re-arranging your workflow. Drag the panes around until your workflow is clear. In the top right corner click on the cog icon and Save Click on the cog icon again and Run Send results to a new history : Yes Check all input files are correct and change if necessary In the top right corner Run workflow This workflow will now run. In the History panel, click on the View all histories icon Find your workflow history and Switch to it. To see all your workflows, go to the top panel in Galaxy and click Workflow . You can edit or run workflows from here by clicking on the drop-down arrow next to each workflow. What s next? To use the tutorials on this website: see the list in the left hand panel or, click the menu button (three horizontal bars) in the top left of the page You can find more tutorials at the Galaxy Training Network: http://galaxyproject.github.io/training-material/","title":"Workflow - genome assembly"},{"location":"modules/workflow-assembly/#workflow-genome-assembly","text":"","title":"Workflow: Genome Assembly"},{"location":"modules/workflow-assembly/#create-a-galaxy-workflow","text":"Galaxy provides the option to extract a workflow based on the steps you just followed and the tools and parameters used. New to Galaxy? First try the introduction and then learn some key tasks","title":"Create a Galaxy workflow"},{"location":"modules/workflow-assembly/#genome-assembly","text":"Log in to your Galaxy instance (for example, Galaxy Australia, usegalaxy.org.au ). Before we create a workflow for genome assembly, we need the correct files in our current Galaxy history. We can get these files in two ways: Either follow the tutorial on genome assembly Or, obtain the files from a Shared History (Galaxy Australia only). Go to Shared Data Click Histories Click Completed-assembly-analysis Click Import (at the top right corner) The analysis should now be showing as your current history. When you have completed this tutorial or imported the history, you will have the correct files in your history panel.","title":"Genome assembly"},{"location":"modules/workflow-assembly/#extract-the-workflow","text":"In the history panel, click on the cog icon. Click Extract Workflow If all looks ok, click Create Workflow In the centre panel, click edit This brings up the workflow canvas, a space for graphically viewing and re-arranging your workflow. Drag the panes around until your workflow is clear. In the top right corner click on the cog icon and Save Click on the cog icon again and Run Send results to a new history : Yes Check all input files are correct and change if necessary In the top right corner Run workflow This workflow will now run. In the History panel, click on the View all histories icon Find your workflow history and Switch to it. To see all your workflows, go to the top panel in Galaxy and click Workflow . You can edit or run workflows from here by clicking on the drop-down arrow next to each workflow.","title":"Extract the Workflow"},{"location":"modules/workflow-assembly/#whats-next","text":"To use the tutorials on this website: see the list in the left hand panel or, click the menu button (three horizontal bars) in the top left of the page You can find more tutorials at the Galaxy Training Network: http://galaxyproject.github.io/training-material/","title":"What's next?"},{"location":"modules/workflow-metagenomics/","text":"Workflow: Metagenomics Create a Galaxy workflow Galaxy provides the option to extract a workflow based on the steps you just followed and the tools and parameters used. New to Galaxy? First try the introduction and then learn some key tasks Metagenomics Log in to your Galaxy instance (for example, Galaxy Australia, usegalaxy.org.au ). Before we create a workflow for metagenomics, we need the correct files in our current Galaxy history. We can get these files in two ways: Either follow the tutorial on Metagenomics Or, obtain the files from a Shared History (Galaxy Australia only). Go to Shared Data Click Histories Click Completed-soil-metagenomics-analysis Click Import (at the top right corner) The analysis should now be showing as your current history. When you have completed this tutorial or imported the history, you will have the correct files in your history panel. Extract the Workflow In the history panel, click on the cog icon. Click Extract Workflow If all looks ok, click Create Workflow In the centre panel, click edit This brings up the workflow canvas, a space for graphically viewing and re-arranging your workflow. Drag the panes around until your workflow is clear. Click on the star icon next to any output files that you want to retain. In the top right corner click on the cog icon and Save Workflow part 1 Workflow part 2 Click on the cog icon again and Run Send results to a new history : Yes Check all input files are correct and change if necessary In the top right corner Run workflow This workflow will now run. In the History panel, click on the View all histories icon Find your workflow history and Switch to it. To see all your workflows, go to the top panel in Galaxy and click Workflow . You can edit or run workflows from here by clicking on the drop-down arrow next to each workflow. What s next? To use the tutorials on this website: see the list in the left hand panel or, click the menu button (three horizontal bars) in the top left of the page You can find more tutorials at the Galaxy Training Network: http://galaxyproject.github.io/training-material/","title":"Workflow - Metagenomics"},{"location":"modules/workflow-metagenomics/#workflow-metagenomics","text":"","title":"Workflow: Metagenomics"},{"location":"modules/workflow-metagenomics/#create-a-galaxy-workflow","text":"Galaxy provides the option to extract a workflow based on the steps you just followed and the tools and parameters used. New to Galaxy? First try the introduction and then learn some key tasks","title":"Create a Galaxy workflow"},{"location":"modules/workflow-metagenomics/#metagenomics","text":"Log in to your Galaxy instance (for example, Galaxy Australia, usegalaxy.org.au ). Before we create a workflow for metagenomics, we need the correct files in our current Galaxy history. We can get these files in two ways: Either follow the tutorial on Metagenomics Or, obtain the files from a Shared History (Galaxy Australia only). Go to Shared Data Click Histories Click Completed-soil-metagenomics-analysis Click Import (at the top right corner) The analysis should now be showing as your current history. When you have completed this tutorial or imported the history, you will have the correct files in your history panel.","title":"Metagenomics"},{"location":"modules/workflow-metagenomics/#extract-the-workflow","text":"In the history panel, click on the cog icon. Click Extract Workflow If all looks ok, click Create Workflow In the centre panel, click edit This brings up the workflow canvas, a space for graphically viewing and re-arranging your workflow. Drag the panes around until your workflow is clear. Click on the star icon next to any output files that you want to retain. In the top right corner click on the cog icon and Save Workflow part 1 Workflow part 2 Click on the cog icon again and Run Send results to a new history : Yes Check all input files are correct and change if necessary In the top right corner Run workflow This workflow will now run. In the History panel, click on the View all histories icon Find your workflow history and Switch to it. To see all your workflows, go to the top panel in Galaxy and click Workflow . You can edit or run workflows from here by clicking on the drop-down arrow next to each workflow.","title":"Extract the Workflow"},{"location":"modules/workflow-metagenomics/#whats-next","text":"To use the tutorials on this website: see the list in the left hand panel or, click the menu button (three horizontal bars) in the top left of the page You can find more tutorials at the Galaxy Training Network: http://galaxyproject.github.io/training-material/","title":"What's next?"},{"location":"modules/workflow-pear-trim/","text":"QC and assembly workflow Run FastQC on R1 and R2. Look at results - webpage. Pear Trimmomatic Assembly Trimmomatic Settings: trimmed leading and trailing - qual 15 sliding window 4 - 20 did for F, R, merged all - can see file size dropped so some were trimmed Spades put all these trimmomatic files in as unpaired/single reads Compare to no pear + trimmomatic + spades vs Assembly only","title":"QC and assembly workflow"},{"location":"modules/workflow-pear-trim/#qc-and-assembly-workflow","text":"Run FastQC on R1 and R2. Look at results - webpage. Pear Trimmomatic Assembly","title":"QC and assembly workflow"},{"location":"modules/workflow-pear-trim/#trimmomatic","text":"Settings: trimmed leading and trailing - qual 15 sliding window 4 - 20 did for F, R, merged all - can see file size dropped so some were trimmed","title":"Trimmomatic"},{"location":"modules/workflow-pear-trim/#spades","text":"put all these trimmomatic files in as unpaired/single reads","title":"Spades"},{"location":"modules/workflow-pear-trim/#compare-to-no-pear-trimmomatic-spades","text":"vs Assembly only","title":"Compare to no pear + trimmomatic + spades"},{"location":"modules/workflow-rna-seq/","text":"Workflow: RNA-Seq Create a Galaxy workflow Galaxy provides the option to extract a workflow based on the steps you just followed and the tools and parameters used. New to Galaxy? First try the introduction and then learn some key tasks RNA-Seq Log in to your Galaxy instance (for example, Galaxy Australia, usegalaxy.org.au ). Before we create a workflow for RNA-seq, we need the correct files in our current Galaxy history. We can get these files in two ways: Either follow the tutorial on RNA-seq Or, obtain the files from a Shared History (Galaxy Australia only). Go to Shared Data Click Histories Click Published-RNA-seq-bacteria Click Import (at the top right corner) The analysis should now be showing as your current history. When you have completed this tutorial or imported the history, you will have the correct files in your history panel. Extract the Workflow In the history panel, click on the cog icon. Click Extract Workflow If all looks ok, click Create Workflow In the centre panel, click edit This brings up the workflow canvas, a space for graphically viewing and re-arranging your workflow. Drag the panes around until your workflow is clear. Click on the star icon next to any output files that you want to retain, such as the JBrowse output and the voom output. In the top right corner click on the cog icon and Save Click on the cog icon again and Run Send results to a new history : Yes Check all input files are correct and change if necessary In the top right corner Run workflow This workflow will now run. In the History panel, click on the View all histories icon Find your workflow history and Switch to it. To see all your workflows, go to the top panel in Galaxy and click Workflow . You can edit or run workflows from here by clicking on the drop-down arrow next to each workflow. What s next? To use the tutorials on this website: see the list in the left hand panel or, click the menu button (three horizontal bars) in the top left of the page You can find more tutorials at the Galaxy Training Network: http://galaxyproject.github.io/training-material/","title":"Workflow - Bacterial RNA-seq"},{"location":"modules/workflow-rna-seq/#workflow-rna-seq","text":"","title":"Workflow: RNA-Seq"},{"location":"modules/workflow-rna-seq/#create-a-galaxy-workflow","text":"Galaxy provides the option to extract a workflow based on the steps you just followed and the tools and parameters used. New to Galaxy? First try the introduction and then learn some key tasks","title":"Create a Galaxy workflow"},{"location":"modules/workflow-rna-seq/#rna-seq","text":"Log in to your Galaxy instance (for example, Galaxy Australia, usegalaxy.org.au ). Before we create a workflow for RNA-seq, we need the correct files in our current Galaxy history. We can get these files in two ways: Either follow the tutorial on RNA-seq Or, obtain the files from a Shared History (Galaxy Australia only). Go to Shared Data Click Histories Click Published-RNA-seq-bacteria Click Import (at the top right corner) The analysis should now be showing as your current history. When you have completed this tutorial or imported the history, you will have the correct files in your history panel.","title":"RNA-Seq"},{"location":"modules/workflow-rna-seq/#extract-the-workflow","text":"In the history panel, click on the cog icon. Click Extract Workflow If all looks ok, click Create Workflow In the centre panel, click edit This brings up the workflow canvas, a space for graphically viewing and re-arranging your workflow. Drag the panes around until your workflow is clear. Click on the star icon next to any output files that you want to retain, such as the JBrowse output and the voom output. In the top right corner click on the cog icon and Save Click on the cog icon again and Run Send results to a new history : Yes Check all input files are correct and change if necessary In the top right corner Run workflow This workflow will now run. In the History panel, click on the View all histories icon Find your workflow history and Switch to it. To see all your workflows, go to the top panel in Galaxy and click Workflow . You can edit or run workflows from here by clicking on the drop-down arrow next to each workflow.","title":"Extract the Workflow"},{"location":"modules/workflow-rna-seq/#whats-next","text":"To use the tutorials on this website: see the list in the left hand panel or, click the menu button (three horizontal bars) in the top left of the page You can find more tutorials at the Galaxy Training Network: http://galaxyproject.github.io/training-material/","title":"What's next?"},{"location":"modules/workflow-variant-calling/","text":"Workflow: Variant Calling Create a Galaxy workflow Galaxy provides the option to extract a workflow based on the steps you just followed and the tools and parameters used. New to Galaxy? First try the introduction and then learn some key tasks Variant calling Log in to your Galaxy instance (for example, Galaxy Australia, usegalaxy.org.au ). Before we create a workflow for variant calling, we need the correct files in our current Galaxy history. We can get these files in two ways: Either follow the tutorial on variant calling Or, obtain the files from a Shared History (Galaxy Australia only). Go to Shared Data Click Histories Click Variant_calling_with_snippy Click Import (at the top right corner) The analysis should now be showing as your current history. When you have completed this tutorial or imported the history, you will have the correct files in your history panel. Extract the Workflow In the history panel, click on the cog icon. Click Extract Workflow If all looks ok, click Create Workflow In the centre panel, click edit This brings up the workflow canvas, a space for graphically viewing and re-arranging your workflow. Drag the panes around until your workflow is clear. In the top right corner click on the cog icon and Save Click on the cog icon again and Run Send results to a new history : Yes Check all input files are correct and change if necessary In the top right corner Run workflow This workflow will now run. In the History panel, click on the View all histories icon Find your workflow history and Switch to it. To see all your workflows, go to the top panel in Galaxy and click Workflow . You can edit or run workflows from here by clicking on the drop-down arrow next to each workflow. What s next? To use the tutorials on this website: see the list in the left hand panel or, click the menu button (three horizontal bars) in the top left of the page You can find more tutorials at the Galaxy Training Network: http://galaxyproject.github.io/training-material/","title":"Workflow - variant calling, bacteria"},{"location":"modules/workflow-variant-calling/#workflow-variant-calling","text":"","title":"Workflow: Variant Calling"},{"location":"modules/workflow-variant-calling/#create-a-galaxy-workflow","text":"Galaxy provides the option to extract a workflow based on the steps you just followed and the tools and parameters used. New to Galaxy? First try the introduction and then learn some key tasks","title":"Create a Galaxy workflow"},{"location":"modules/workflow-variant-calling/#variant-calling","text":"Log in to your Galaxy instance (for example, Galaxy Australia, usegalaxy.org.au ). Before we create a workflow for variant calling, we need the correct files in our current Galaxy history. We can get these files in two ways: Either follow the tutorial on variant calling Or, obtain the files from a Shared History (Galaxy Australia only). Go to Shared Data Click Histories Click Variant_calling_with_snippy Click Import (at the top right corner) The analysis should now be showing as your current history. When you have completed this tutorial or imported the history, you will have the correct files in your history panel.","title":"Variant calling"},{"location":"modules/workflow-variant-calling/#extract-the-workflow","text":"In the history panel, click on the cog icon. Click Extract Workflow If all looks ok, click Create Workflow In the centre panel, click edit This brings up the workflow canvas, a space for graphically viewing and re-arranging your workflow. Drag the panes around until your workflow is clear. In the top right corner click on the cog icon and Save Click on the cog icon again and Run Send results to a new history : Yes Check all input files are correct and change if necessary In the top right corner Run workflow This workflow will now run. In the History panel, click on the View all histories icon Find your workflow history and Switch to it. To see all your workflows, go to the top panel in Galaxy and click Workflow . You can edit or run workflows from here by clicking on the drop-down arrow next to each workflow.","title":"Extract the Workflow"},{"location":"modules/workflow-variant-calling/#whats-next","text":"To use the tutorials on this website: see the list in the left hand panel or, click the menu button (three horizontal bars) in the top left of the page You can find more tutorials at the Galaxy Training Network: http://galaxyproject.github.io/training-material/","title":"What's next?"},{"location":"modules/workflows/","text":"Galaxy workflows A workflow is a chain of analysis steps. In Galaxy, we can create a workflow from an existing analysis history, or we can create one visually by adding tools to a canvas. This tutorial covers building a workflow to analyse a bacterial genome, from input FASTQ sequencing reads to assembly, annotation, and visualization. New to Galaxy? First try the introduction and then learn some key tasks Start Log in to your Galaxy instance (for example, Galaxy Australia, usegalaxy.org.au ). Import a history of data files: Click on the History cog Select Import from File In the box called Archived History URL , paste in this link address to the Galaxy history of input files: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Microbial_tutorials/Galaxy_history_input_files.tar.gz Click Import History Wait a few seconds. Click on the View all histories button See if the Galaxy history has been imported: it will be called imported from archive: Data Above that pane, click on the Switch to button. Then click Analyze Data (in the top menu bar). You should now have a list of five files in your current history. Re-name this history Workflows . Build a workflow We will first write a workflow for genome assembly. In the top menu bar in Galaxy, click on Workflow . Click on the plus button. Under Workflow Name: put in Reads to Annotation . Click Save This will bring up the Workflow Canvas , a grid where you can arrange the workflow. Add inputs In the Tools panel, click Inputs: Input datset twice (at the very top of the list). A box will appear: drag it to the left and there will be another box underneath it. Drag this also to the left. Your workflow canvas should look like this: Click on the first box. Look in the right hand panel (now called Details ). Under Label type in R1.fastq . Press Enter for the change to be saved. Repeat for the second input dataset box, naming that one R2.fastq . Add the tool spades In the tools panel, search for spades and click on the tool name. This puts the spades box onto the workflow canvas. Click on the spades box and look in the Details pane on the right. This shows all the options in spades. Choose: Run only Assembly : Yes [the Yes button should be darker grey] Kmers to use separated by commas: 33,55,91 [note: no spaces] Coverage cutoff: auto Join inputs to the tool Now tell spades which input files to use. Look at the input dataset box called R1.fastq and find the small arrow: Click on this and drag the arrow over to the spades box input arrow next to Libraries 1 Files 1 Forward reads . Repeat for the dataset box R2.fastq , joining to the spades box next to Libraries 1 Files 1 Reverse reads . Save it and run Click on the cog at the top right of the workflow canvas and Save . Click the cog again and choose Run . This brings up a window where you specify the input datasets to use in the workflow. Under Step1: Input dataset choose mutant_R1.fastq . Under Step2: Input dataset choose mutant_R2.fastq . Click Run workflow . This will run the workflow (spades) and save the output to the top of your current history in the right hand panel. View some of the output files with the eye icon to check that the workflow (in this case, just spades) ran correctly. Add to the worfklow We will add another tool to the workflow. Go to the top Galaxy panel and click Workflow . Your workflow Reads to Annotation should be in the list. Click on the drop-down arrow next to this workflow and choose Edit . This will bring up the Workflow Canvas where we can add more inputs and tools. In the Tools panel search for Prokka and click on the tool name. This will add a Prokka box to the workflow canvas. We need to tell Prokka which genome assembly) to annotate. Join the spades output called out_contigs(fasta) to the Prokka input called Contigs to annotate . Click on the Prokka box and change some of the settings in the right hand Details panel: Set the following parameters (leave everything else unchanged): Locus tag prefix ( locustag) : P Force GenBank/ENA/DDJB compliance ( compliant) : No Sequencing Centre ID ( centre) : V Use genus-specific BLAST database No Click on the cog to the top right of the workflow canvas to save. Click on the cog again to run. Again, choose the input files: mutant_R1.fastq and mutant_R2.fastq , and then click Run workflow . The output from the workflow (files from spades and prokka) will appear at the top of the History panel. Click on the eye icon for some files to verify the workflow ran correctly. Add more to the workflow We will add a visualization tool to view the genome annotation. Go to the top Galaxy panel and click Workflow . Your workflow Reads to Annotation should be in the list. Click on the drop-down arrow next to this workflow and choose Edit . This will bring up the Workflow Canvas where we can add more inputs and tools. In the Tools panel, search for JBrowse and click on JBrowse genome browser . This will add a JBrowse box to the workflow canvas. Click on the JBrowse box. In the Details pane: Under Reference genome to display choose Use a genome from history . For Produce a Standalone Instance select Yes . For Genetic Code choose 11: The Bacterial, Archaeal and Plant Plastid Code . Under JBrowse-in-Galaxy Action choose New JBrowse Instance . Click Insert Track Group Under Track Category type in gene annotations . Click Insert Annotation Track For Track Type choose GFF/GFF3/BED/GBK Features Under JBrowse Track Type[Advanced] select Canvas Features . Under Track Visibility choose On for new users . Now we need to tell JBrowse the input files to use. Join the Prokka output out_fna (fasta) to the JBrowse input Select the reference genome Join the Prokka output out_gff (gff) to the JBrowse input Track Group 1 Click on the cog to save; again to run; choose input files; Run workflow ; examine output files in current history. The workflow will now assemble and annotate the genome, and create a JBrowse view of the annotations. JBrowse will produce one output file. Click on the eye icon to view. In the centre drop down box, choose contig 1. Under Available Tracks on the left, tick the boxes. Zoom in and out with the plus and minus icons. The blue blocks are the genome annotations. Summary Our workflow is now: FASTQ sequence reads to Spades for assembly Spades contigs fasta file to Prokka for annotation Prokka fasta file and .gff file to JBrowse for visualisation. We can re-run this workflow with different input FASTQ files. Other workflow options Saving outputs To save only some output files: Go to the workflow canvas. Find the star next to the outputs. Click on the star for any outputs you want to save. To save these starred files from the workflow output as a new history: Before you click Run workflow , tick the box above to Send results to a new history . Import a workflow To import an existing Galaxy Workflow: Go to the Workflow tab in the top panel. At the top right, click on Upload or import workflow . Extract a workflow You can extract a workflow from an existing Galaxy history. Go to your Galaxy history Click on the History cog icon and choose Extract Workflow . Give it a name and click Create Workflow . To edit, go to the Workflow tab, select the workflow, and choose Edit from the drop down menu. You can then edit the steps on the Workflow Canvas. A note on workflow tabs We have been using the top Workflow tab. There is another tab at the bottom of the tool panel called Workflows. Click on Workflows: All Workflows . This gives a similar view with a list of workflows. To return to the main Galaxy window click on the Analyze Data tab in the top panel. What s next? To use the tutorials on this website: see the list in the left hand panel or, click the menu button (three horizontal bars) in the top left of the page You can find more tutorials at the Galaxy Training Network: http://galaxyproject.github.io/training-material/","title":"Galaxy workflows - introduction"},{"location":"modules/workflows/#galaxy-workflows","text":"A workflow is a chain of analysis steps. In Galaxy, we can create a workflow from an existing analysis history, or we can create one visually by adding tools to a canvas. This tutorial covers building a workflow to analyse a bacterial genome, from input FASTQ sequencing reads to assembly, annotation, and visualization. New to Galaxy? First try the introduction and then learn some key tasks","title":"Galaxy workflows"},{"location":"modules/workflows/#start","text":"Log in to your Galaxy instance (for example, Galaxy Australia, usegalaxy.org.au ). Import a history of data files: Click on the History cog Select Import from File In the box called Archived History URL , paste in this link address to the Galaxy history of input files: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Microbial_tutorials/Galaxy_history_input_files.tar.gz Click Import History Wait a few seconds. Click on the View all histories button See if the Galaxy history has been imported: it will be called imported from archive: Data Above that pane, click on the Switch to button. Then click Analyze Data (in the top menu bar). You should now have a list of five files in your current history. Re-name this history Workflows .","title":"Start"},{"location":"modules/workflows/#build-a-workflow","text":"We will first write a workflow for genome assembly. In the top menu bar in Galaxy, click on Workflow . Click on the plus button. Under Workflow Name: put in Reads to Annotation . Click Save This will bring up the Workflow Canvas , a grid where you can arrange the workflow.","title":"Build a workflow"},{"location":"modules/workflows/#add-inputs","text":"In the Tools panel, click Inputs: Input datset twice (at the very top of the list). A box will appear: drag it to the left and there will be another box underneath it. Drag this also to the left. Your workflow canvas should look like this: Click on the first box. Look in the right hand panel (now called Details ). Under Label type in R1.fastq . Press Enter for the change to be saved. Repeat for the second input dataset box, naming that one R2.fastq .","title":"Add inputs"},{"location":"modules/workflows/#add-the-tool-spades","text":"In the tools panel, search for spades and click on the tool name. This puts the spades box onto the workflow canvas. Click on the spades box and look in the Details pane on the right. This shows all the options in spades. Choose: Run only Assembly : Yes [the Yes button should be darker grey] Kmers to use separated by commas: 33,55,91 [note: no spaces] Coverage cutoff: auto","title":"Add the tool \"spades\""},{"location":"modules/workflows/#join-inputs-to-the-tool","text":"Now tell spades which input files to use. Look at the input dataset box called R1.fastq and find the small arrow: Click on this and drag the arrow over to the spades box input arrow next to Libraries 1 Files 1 Forward reads . Repeat for the dataset box R2.fastq , joining to the spades box next to Libraries 1 Files 1 Reverse reads .","title":"Join inputs to the tool"},{"location":"modules/workflows/#save-it-and-run","text":"Click on the cog at the top right of the workflow canvas and Save . Click the cog again and choose Run . This brings up a window where you specify the input datasets to use in the workflow. Under Step1: Input dataset choose mutant_R1.fastq . Under Step2: Input dataset choose mutant_R2.fastq . Click Run workflow . This will run the workflow (spades) and save the output to the top of your current history in the right hand panel. View some of the output files with the eye icon to check that the workflow (in this case, just spades) ran correctly.","title":"Save it and run"},{"location":"modules/workflows/#add-to-the-worfklow","text":"We will add another tool to the workflow. Go to the top Galaxy panel and click Workflow . Your workflow Reads to Annotation should be in the list. Click on the drop-down arrow next to this workflow and choose Edit . This will bring up the Workflow Canvas where we can add more inputs and tools. In the Tools panel search for Prokka and click on the tool name. This will add a Prokka box to the workflow canvas. We need to tell Prokka which genome assembly) to annotate. Join the spades output called out_contigs(fasta) to the Prokka input called Contigs to annotate . Click on the Prokka box and change some of the settings in the right hand Details panel: Set the following parameters (leave everything else unchanged): Locus tag prefix ( locustag) : P Force GenBank/ENA/DDJB compliance ( compliant) : No Sequencing Centre ID ( centre) : V Use genus-specific BLAST database No Click on the cog to the top right of the workflow canvas to save. Click on the cog again to run. Again, choose the input files: mutant_R1.fastq and mutant_R2.fastq , and then click Run workflow . The output from the workflow (files from spades and prokka) will appear at the top of the History panel. Click on the eye icon for some files to verify the workflow ran correctly.","title":"Add to the worfklow"},{"location":"modules/workflows/#add-more-to-the-workflow","text":"We will add a visualization tool to view the genome annotation. Go to the top Galaxy panel and click Workflow . Your workflow Reads to Annotation should be in the list. Click on the drop-down arrow next to this workflow and choose Edit . This will bring up the Workflow Canvas where we can add more inputs and tools. In the Tools panel, search for JBrowse and click on JBrowse genome browser . This will add a JBrowse box to the workflow canvas. Click on the JBrowse box. In the Details pane: Under Reference genome to display choose Use a genome from history . For Produce a Standalone Instance select Yes . For Genetic Code choose 11: The Bacterial, Archaeal and Plant Plastid Code . Under JBrowse-in-Galaxy Action choose New JBrowse Instance . Click Insert Track Group Under Track Category type in gene annotations . Click Insert Annotation Track For Track Type choose GFF/GFF3/BED/GBK Features Under JBrowse Track Type[Advanced] select Canvas Features . Under Track Visibility choose On for new users . Now we need to tell JBrowse the input files to use. Join the Prokka output out_fna (fasta) to the JBrowse input Select the reference genome Join the Prokka output out_gff (gff) to the JBrowse input Track Group 1 Click on the cog to save; again to run; choose input files; Run workflow ; examine output files in current history. The workflow will now assemble and annotate the genome, and create a JBrowse view of the annotations. JBrowse will produce one output file. Click on the eye icon to view. In the centre drop down box, choose contig 1. Under Available Tracks on the left, tick the boxes. Zoom in and out with the plus and minus icons. The blue blocks are the genome annotations.","title":"Add more to the workflow"},{"location":"modules/workflows/#summary","text":"Our workflow is now: FASTQ sequence reads to Spades for assembly Spades contigs fasta file to Prokka for annotation Prokka fasta file and .gff file to JBrowse for visualisation. We can re-run this workflow with different input FASTQ files.","title":"Summary"},{"location":"modules/workflows/#other-workflow-options","text":"","title":"Other workflow options"},{"location":"modules/workflows/#saving-outputs","text":"To save only some output files: Go to the workflow canvas. Find the star next to the outputs. Click on the star for any outputs you want to save. To save these starred files from the workflow output as a new history: Before you click Run workflow , tick the box above to Send results to a new history .","title":"Saving outputs"},{"location":"modules/workflows/#import-a-workflow","text":"To import an existing Galaxy Workflow: Go to the Workflow tab in the top panel. At the top right, click on Upload or import workflow .","title":"Import a workflow"},{"location":"modules/workflows/#extract-a-workflow","text":"You can extract a workflow from an existing Galaxy history. Go to your Galaxy history Click on the History cog icon and choose Extract Workflow . Give it a name and click Create Workflow . To edit, go to the Workflow tab, select the workflow, and choose Edit from the drop down menu. You can then edit the steps on the Workflow Canvas.","title":"Extract a workflow"},{"location":"modules/workflows/#a-note-on-workflow-tabs","text":"We have been using the top Workflow tab. There is another tab at the bottom of the tool panel called Workflows. Click on Workflows: All Workflows . This gives a similar view with a list of workflows. To return to the main Galaxy window click on the Analyze Data tab in the top panel.","title":"A note on workflow tabs"},{"location":"modules/workflows/#whats-next","text":"To use the tutorials on this website: see the list in the left hand panel or, click the menu button (three horizontal bars) in the top left of the page You can find more tutorials at the Galaxy Training Network: http://galaxyproject.github.io/training-material/","title":"What's next?"},{"location":"modules/workshop_overview/","text":"Genomics The tutorials listed here under the Genomics tab include a set that can be run as a Genomics Workshop. Genomics workshop The modules in this workshop cover microbial genomics, from assembly to annotation and variant calling: Starting with Galaxy Training dataset Quality control Genome assembly with two tools - Velvet and Spades Genome annotation Variant finding The analyses are conducted on the Galaxy platform, and links to training data are provided. These modules can be delivered to a group workshop or used online independently. If you have not yet used the Galaxy platform, we recommend following the modules in order. If you are using these tutorials outside of a workshop and need access to Galaxy, you can follow the instructions on the homepage here to obtain your own mGVL instance with Galaxy and command line tools. Other Genomics tutorials There are also additional tutorials listed under the Genomics tab (such as PacBio assembly), but these may require different data sets and tools.","title":"Home"},{"location":"modules/workshop_overview/#genomics","text":"The tutorials listed here under the Genomics tab include a set that can be run as a Genomics Workshop.","title":"Genomics"},{"location":"modules/workshop_overview/#genomics-workshop","text":"The modules in this workshop cover microbial genomics, from assembly to annotation and variant calling: Starting with Galaxy Training dataset Quality control Genome assembly with two tools - Velvet and Spades Genome annotation Variant finding The analyses are conducted on the Galaxy platform, and links to training data are provided. These modules can be delivered to a group workshop or used online independently. If you have not yet used the Galaxy platform, we recommend following the modules in order. If you are using these tutorials outside of a workshop and need access to Galaxy, you can follow the instructions on the homepage here to obtain your own mGVL instance with Galaxy and command line tools.","title":"Genomics workshop"},{"location":"modules/workshop_overview/#other-genomics-tutorials","text":"There are also additional tutorials listed under the Genomics tab (such as PacBio assembly), but these may require different data sets and tools.","title":"Other Genomics tutorials"},{"location":"modules/xcms/","text":"Metabolomics with XCMS Overview Metabolomics is the study of metabolites: small molecules (smaller than proteins) produced by organisms. One technique to identify and quantify metabolites is LC/MS (liquid chromatography-mass spectrometry). In liquid chromatography, metabolites are sent through a column and are separated by various chemical properties. At each time point, the abundance of the particular group of metabolites is measured (the intensity). This is called a chromatogram: image: C Wenger, Wikipedia At each time point in this chromatogram, the group of metabolites is then ionized (charged) and fired through a mass spectrometer. These are separated based on their mass-to-charge ratio. This adds another dimension to the graph: an axis with the mass-to-charge ratio of the metabolites found at that time point, and their intensities. image: Daniel Norena-Caro, Wikipedia. There are multiple mass spectra. Each spectrum is often simplified into a graph of peaks only (local maxima): image: Wikipedia. Each of these peaks is a feature : an ion with a unique m/z and retention time. These masses can then be matched to a database to identify the metabolites. A common aim is to compare metabolites between two samples (e.g. two different bacterial strains), and from there, to understand which biological pathways may be up- or down-regulated. In this tutorial, we will use a platform called XCMS online to analyse metabolite data. Input: raw data from mass spectrometry Output: identified metabolites, and a comparison of their abundances between samples. XCMS Go to: https://xcmsonline.scripps.edu and sign up. Get data The data we will use today is from two bacterial strains of Klebsiella pneumoniae . strain AJ218 - 6 replicates strain KPC2 (antibiotic resistance) - 6 replicates The raw data output from the mass spectrometer are points in a 3D graph: intensity, m/z, time (retention time). Download these files to your local computer. Data format: We will use .mzML format. The machine used to produced this data originally produced .d files. These have been converted into .mzML format using the Proteowizard MSConvert program. To use Proteowizard: (Windows only) Download the program and open the MSConvert program. Browse. Add files. Change output format to .mzML Leave default settings. Click Start in the bottom right hand corner. An alternative is the commercially-available Qualitative Analysis Software. Upload data In the top panel, go to Stored Datasets . We will upload some data here. Drag the .mzML files into the Drop Here box - 6 replicates for each strain. Wait until all files have a green tick (scroll down to check all). Name the datset (e.g. Sample AJ218 or Sample KPC2) and click Save . Click Save Dataset Proceed . Repeat with the second strain. Set up job In the top panel, click on Create Job and select Pairwise Job . On the right hand side, under Job Summary, Job Name: click Edit , enter job name: e.g. Klebsiella, and then click Save . Under Dataset 1 click on Select Dataset . Choose AJ218. Under Dataset 2 click on Select Dataset . Choose KPC2. We now need to set parameters that correspond with the machine on which the data was generated. In a typical analysis, we would look at the raw output files and examine the chromatograms and mass spectra to inform some of the settings. Here we have chosen appropriate settings for this data set. Under Parameters select HPLC / UHD Q-TOF (HILIC, neg. mode). Click View/Edit . This brings up a window to change some settings. General First, click Create New in the bottom right hand corner. Give it a name. e.g. Agilent 6545 (Don t click Save Current yet). Retention time format : seconds Polarity : negative See the tabs along the top: we will change some of these settings. Feature Detection This is to adjust for noise in the centroid data. Method : centWave ppm : 50 minimum peak width : 10 maximum peak width : 50 Retention time correction This is to correct the shift that occurs as the run progresses. Method : obiwarp profStep : 0.1 Alignment This is to align spectra after retention time correction. mzwid : 0.5 minfrac : 0.5 bw : 20 Statistics Set up the statistical test for the metabolites from two strains. Statistical test: Unpaired non-parametric (Mann-Whitney) Annotation Search for : isotopes + adducts m/z absolute error : 0.05 ppm : 50 Identification ppm : 50 adducts : [M-H]- sample biosource : Select biosource. Search: K pneumo. Select the top strain. pathway ppm deviation : 5 Visualization EIC width : 200 Miscellaneous Bypass file sanity check : tick Next Save Current Submit Job This will now bring up the View Results page. The current job will be listed as Processing with a % completion bar. The time taken will depend on server load. View results Click on View . There are six graphs, and options for other results in the left hand panel. Ions detected Look at the top three graphs. Graph 1: Total ion Chromatograms (original): All the ions detected. Their intensity vs retention time. Graph 2: Retention Time Deviation vs. Retention Time: A graph showing the correction curve. Graph 3: Total ion Chromatograms (corrected): A corrected version of graph 1. Sample information Graphs 5 and 6 show MDS (Multi-dimensional Scaling) and PCA (Principal Components Analysis) results. Are the samples separated well? Samples (or conditions) should be separated into two groups. For a more detailed examination, click on iPCA in the left hand panel. Results Table Click on the Results Table in the left hand panel. This is a table of features - a feature is an ion with unique m/z and retention time. Click on a row (a feature) to display associated graphs in the right-hand panel. The METLIN database contains data on metabolites, their mass, and their known and predicted fragment masses. To filter the table, click on the small magnifying glass: Filter by p-value or fold change (or both). e.g. p-value less or equal to 0.01, fold change greater than 30 Investigate these features and the identified matches in the Metlin database. Cloud plot In the results pane on the left, click on Metabolomic Cloud Plot. A cloud plot shows the features (m/z and retention time) as dots/circles. The size of the circles is relative to their fold change. Features are shown as either up- or down-regulated, by their position above or below the 0-axis. Adjust p-value and fold change, and click Regenerate Cloud Plot . Click on a feature to see its associated graphs in the left hand panel. Activity network This shows the pathways that correspond to the identified metabolites. The table under the image shows the Top pathways - these are ordered by their p-values. Links XCMS Online XCMS Documentation Tautenhahn, R. et al. (2012) XCMS Online: A Web-Based Platform to Process Untargeted Metabolomic Data. Analytical Chemistry . DOI: 10.1021/ac300698c Smith, R. et al. (2014) Proteomics, lipidomics, metabolomics: a mass spectrometry tutorial from a computer scientist s point of view. BMC Bioinformatics . DOI: 10.1186/1471-2105-15-S7-S9. See Figure 2 for an excellent explanation of the various graphs produced from MS. Patti, G. J. et al. (2013) A View from Above: Cloud Plots to Visualize Global Metabolomic Data. Analytical Chemistry . DOI: 10.1021/ac3029745","title":"Home"},{"location":"modules/xcms/#metabolomics-with-xcms","text":"","title":"Metabolomics with XCMS"},{"location":"modules/xcms/#overview","text":"Metabolomics is the study of metabolites: small molecules (smaller than proteins) produced by organisms. One technique to identify and quantify metabolites is LC/MS (liquid chromatography-mass spectrometry). In liquid chromatography, metabolites are sent through a column and are separated by various chemical properties. At each time point, the abundance of the particular group of metabolites is measured (the intensity). This is called a chromatogram: image: C Wenger, Wikipedia At each time point in this chromatogram, the group of metabolites is then ionized (charged) and fired through a mass spectrometer. These are separated based on their mass-to-charge ratio. This adds another dimension to the graph: an axis with the mass-to-charge ratio of the metabolites found at that time point, and their intensities. image: Daniel Norena-Caro, Wikipedia. There are multiple mass spectra. Each spectrum is often simplified into a graph of peaks only (local maxima): image: Wikipedia. Each of these peaks is a feature : an ion with a unique m/z and retention time. These masses can then be matched to a database to identify the metabolites. A common aim is to compare metabolites between two samples (e.g. two different bacterial strains), and from there, to understand which biological pathways may be up- or down-regulated. In this tutorial, we will use a platform called XCMS online to analyse metabolite data. Input: raw data from mass spectrometry Output: identified metabolites, and a comparison of their abundances between samples.","title":"Overview"},{"location":"modules/xcms/#xcms","text":"Go to: https://xcmsonline.scripps.edu and sign up.","title":"XCMS"},{"location":"modules/xcms/#get-data","text":"The data we will use today is from two bacterial strains of Klebsiella pneumoniae . strain AJ218 - 6 replicates strain KPC2 (antibiotic resistance) - 6 replicates The raw data output from the mass spectrometer are points in a 3D graph: intensity, m/z, time (retention time). Download these files to your local computer. Data format: We will use .mzML format. The machine used to produced this data originally produced .d files. These have been converted into .mzML format using the Proteowizard MSConvert program. To use Proteowizard: (Windows only) Download the program and open the MSConvert program. Browse. Add files. Change output format to .mzML Leave default settings. Click Start in the bottom right hand corner. An alternative is the commercially-available Qualitative Analysis Software.","title":"Get data"},{"location":"modules/xcms/#upload-data","text":"In the top panel, go to Stored Datasets . We will upload some data here. Drag the .mzML files into the Drop Here box - 6 replicates for each strain. Wait until all files have a green tick (scroll down to check all). Name the datset (e.g. Sample AJ218 or Sample KPC2) and click Save . Click Save Dataset Proceed . Repeat with the second strain.","title":"Upload data"},{"location":"modules/xcms/#set-up-job","text":"In the top panel, click on Create Job and select Pairwise Job . On the right hand side, under Job Summary, Job Name: click Edit , enter job name: e.g. Klebsiella, and then click Save . Under Dataset 1 click on Select Dataset . Choose AJ218. Under Dataset 2 click on Select Dataset . Choose KPC2. We now need to set parameters that correspond with the machine on which the data was generated. In a typical analysis, we would look at the raw output files and examine the chromatograms and mass spectra to inform some of the settings. Here we have chosen appropriate settings for this data set. Under Parameters select HPLC / UHD Q-TOF (HILIC, neg. mode). Click View/Edit . This brings up a window to change some settings.","title":"Set up job"},{"location":"modules/xcms/#general","text":"First, click Create New in the bottom right hand corner. Give it a name. e.g. Agilent 6545 (Don t click Save Current yet). Retention time format : seconds Polarity : negative See the tabs along the top: we will change some of these settings.","title":"General"},{"location":"modules/xcms/#feature-detection","text":"This is to adjust for noise in the centroid data. Method : centWave ppm : 50 minimum peak width : 10 maximum peak width : 50","title":"Feature Detection"},{"location":"modules/xcms/#retention-time-correction","text":"This is to correct the shift that occurs as the run progresses. Method : obiwarp profStep : 0.1","title":"Retention time correction"},{"location":"modules/xcms/#alignment","text":"This is to align spectra after retention time correction. mzwid : 0.5 minfrac : 0.5 bw : 20","title":"Alignment"},{"location":"modules/xcms/#statistics","text":"Set up the statistical test for the metabolites from two strains. Statistical test: Unpaired non-parametric (Mann-Whitney)","title":"Statistics"},{"location":"modules/xcms/#annotation","text":"Search for : isotopes + adducts m/z absolute error : 0.05 ppm : 50","title":"Annotation"},{"location":"modules/xcms/#identification","text":"ppm : 50 adducts : [M-H]- sample biosource : Select biosource. Search: K pneumo. Select the top strain. pathway ppm deviation : 5","title":"Identification"},{"location":"modules/xcms/#visualization","text":"EIC width : 200","title":"Visualization"},{"location":"modules/xcms/#miscellaneous","text":"Bypass file sanity check : tick","title":"Miscellaneous"},{"location":"modules/xcms/#next","text":"Save Current Submit Job This will now bring up the View Results page. The current job will be listed as Processing with a % completion bar. The time taken will depend on server load.","title":"Next"},{"location":"modules/xcms/#view-results","text":"Click on View . There are six graphs, and options for other results in the left hand panel.","title":"View results"},{"location":"modules/xcms/#ions-detected","text":"Look at the top three graphs. Graph 1: Total ion Chromatograms (original): All the ions detected. Their intensity vs retention time. Graph 2: Retention Time Deviation vs. Retention Time: A graph showing the correction curve. Graph 3: Total ion Chromatograms (corrected): A corrected version of graph 1.","title":"Ions detected"},{"location":"modules/xcms/#sample-information","text":"Graphs 5 and 6 show MDS (Multi-dimensional Scaling) and PCA (Principal Components Analysis) results. Are the samples separated well? Samples (or conditions) should be separated into two groups. For a more detailed examination, click on iPCA in the left hand panel.","title":"Sample information"},{"location":"modules/xcms/#results-table","text":"Click on the Results Table in the left hand panel. This is a table of features - a feature is an ion with unique m/z and retention time. Click on a row (a feature) to display associated graphs in the right-hand panel. The METLIN database contains data on metabolites, their mass, and their known and predicted fragment masses. To filter the table, click on the small magnifying glass: Filter by p-value or fold change (or both). e.g. p-value less or equal to 0.01, fold change greater than 30 Investigate these features and the identified matches in the Metlin database.","title":"Results Table"},{"location":"modules/xcms/#cloud-plot","text":"In the results pane on the left, click on Metabolomic Cloud Plot. A cloud plot shows the features (m/z and retention time) as dots/circles. The size of the circles is relative to their fold change. Features are shown as either up- or down-regulated, by their position above or below the 0-axis. Adjust p-value and fold change, and click Regenerate Cloud Plot . Click on a feature to see its associated graphs in the left hand panel.","title":"Cloud plot"},{"location":"modules/xcms/#activity-network","text":"This shows the pathways that correspond to the identified metabolites. The table under the image shows the Top pathways - these are ordered by their p-values.","title":"Activity network"},{"location":"modules/xcms/#links","text":"XCMS Online XCMS Documentation Tautenhahn, R. et al. (2012) XCMS Online: A Web-Based Platform to Process Untargeted Metabolomic Data. Analytical Chemistry . DOI: 10.1021/ac300698c Smith, R. et al. (2014) Proteomics, lipidomics, metabolomics: a mass spectrometry tutorial from a computer scientist s point of view. BMC Bioinformatics . DOI: 10.1186/1471-2105-15-S7-S9. See Figure 2 for an excellent explanation of the various graphs produced from MS. Patti, G. J. et al. (2013) A View from Above: Cloud Plots to Visualize Global Metabolomic Data. Analytical Chemistry . DOI: 10.1021/ac3029745","title":"Links"},{"location":"modules/xtandem/","text":"Protein identification using X!Tandem Introduction The high-throughput nature of proteomics mass spectrometry is enabled by a productive combination of data acquisition protocols and the computational tools used to interpret the resulting spectra (list of peaks). One of the key components in mainstream protocols is the generation of tandem mass (MS/MS) spectra by peptide fragmentation. This approach is currently used in the large majority of proteomics experiments to routinely identify hundreds to thousands of proteins from single mass spectrometry runs. In order to do that multiple tools have to be employed: LC-MS/MS to segregate components of proteomic samples associated with protein identification (see Figure 1 ) softwares, X!Tandem 1 , Mascot or SEQUEST all of which perform protein identification but with different algorithms. Figure 1 General overview of the experimental steps and flow of data in protein identification using shotgun proteomics experiment. 2 . Figure 1 shows a general overview of the experimental steps in protein identification. Sample proteins are first cleaved into peptides, which are then separated using chromatography (e.g. HPLC). The peptides are then ionised and then selected ions are fragmented to produce signature tandem mass spectrometry (MS/MS) spectra. Peptides are identified from the MS/MS spectra by using programs such as X!Tandem, which search against a database of known peptides. Sequences of the identified peptides are used to infer which proteins are present in the original sample. Background LC-MS/MS Analysis Liquid Chromatography (LC) is a technique used to separate molecules in the solvent (mobile phase). Nowadays liquid chromatography utilising high pressure to force the solvent through the columns packed with porous particles (stationary phase) is referred as High Pressure Liquid Chromatography (HPLC) see Figure 2 . After purifying the proteomic cell content, LC is able to separate the different proteins which are injected in the mass spectrometer. Each peak from the results will be analysed by mass spectrometry. Figure 2 Schema of High Pressure Liquid Chromatography (HPLC) 5 . Compounds of the mixture are separated in the HPLC column according to various parameters (polarity, charge, affinity etc). The type of separation depends on the column being used). A detector flow cell is used to detect the separated compounds band. Mass spectrometry (MS) see Figure 3 has been widely used to analyse biological samples and has evolved into an indispensable tool for proteomics research. Fundamentally, MS measures the mass-to-charge ratio (m/z) of gas-phase ions. Mass spectrometers consist of an ion source that converts analyte molecules into gas-phase ions, a mass analyser that separates ionised analytes on the basis of m/z ratio and a detector that records the number of ions at each m/z value. Figure 3 Schema of mass specter . A mass spectrometer consists of three components: an ion source, a mass analyzer, and a detector. The ionizer converts a portion of the sample into ions. There is a wide variety of ionization techniques, depending on the phase (solid, liquid, gas) of the sample and the efficiency of various ionization mechanisms for the unknown species. An extraction system removes ions from the sample, which are then targeted through the mass analyzer and onto the detector. The differences in masses of the fragments allows the mass analyzer to sort the ions by their mass-to-charge ratio. The detector measures the value of an indicator quantity and thus provides data for calculating the abundances of each ion present. 6 Tandem mass spectrometry (MS/MS) see Figure 4 is a key technique for protein or peptide sequencing and post-translational modifications analysis. Collision-induced dissociation (CID) has been the most widely used MS/MS technique in proteomics research. In this method, gas-phase peptide/protein cations are internally heated by multiple collisions with rare gas atoms. The result is fragmented ions that, after the detection phase and reconstitution, reveal the amino-acid chains. Figure 4 Schema of a tandem mass spectrometry associated with liquid chromatography (LC-MS/MS) . This technique combine separation form liquid chromatography and m/z ratio analyse of the tandem mass spectrometry in order to generate spectra. File formats : During a full proteomics analysis, as seen in Figure 5 , many files are created. Every step has its own multiple file formats: Figure 5 Multiple formats during MS treatment 7 . From the sample processing to the final file, various file formats can be used. This schema shows the different processes and file formats that are generated during a full MS experiment. For this tutorial we will focus on the Informatics Analysis part using the following file formats: fasta : fasta files are mainly used for representing either nucleotide sequences or peptide sequences, using single-letter codes. MGF : Mascot Generic Format (MGF) file is the most common format for MS/MS data encoding in the form of a peak list 8 . This file encodes multiple MS/MS spectra in a single file listing the [m/z, intensity] pairs separated by headers see Figure 6 . More precisely each query represents a complete MS/MS spectrum delimited by BEGIN IONS and END IONS statements. Embedded parameters 9 can be found after each BEGIN IONS statement. An example entry is shown in the figure below: Figure 6 Sample of a MGF file . MGF files are used for enconding MS results, multiple spectra can be enconded in one MGF. Every spectrum begins with the BEGINS IONS assessment and finishes with END IONS . MGF files can be divided in 2 parts : The header : containing information about the embedded Search Parameters. Ions information : the first figure is the ion mass, the second is the ion charge. X!Tandem X!Tandem is an open source software that can match tandem mass spectra (usually the experiment) with peptide sequences from a database. This process allows identification of proteins in one or more samples. The X!Tandem search engine calculates a statistical confidence (expectation value) for all of the individual spectrum-to-sequence assignments. Some spectra might map to more than one protein, in this case X!Tandem will pick the best match with the highest confidence score (see Figure 7 ). The output is a lists all of the high confidence assignments. Figure 7 Schema of the X!Tandem analysis 2 . After LC-MS/MS analysis the experimental spectrum is compared to a theoretical spectrum made from a protein database. This comparison leads to a list of peptide match ranked by score. GALAXY GALAXY 3 is an open source, web-based platform for biomedical research. GALAXY allows users to perform multi-omics data analyses: genomics, proteomics, transcriptomics and more. Numerous tools are available to achieve various analyses. This tutorial will focus on proteomics identification: matching the experimental data, spectra from LC-MS/MS analysis against data from a protein database using X!Tandem. Before starting, a quick overview of the GALAXY interface see Figure 8 . The interface is divided into three parts: Left panel : List the tools that are available. A search textbox is at the top of the panel in order to find the tool you want. Right panel : Is the history of all the data outputs from previous steps and subsequently becoming the input for the next step. For example, the figure below shows the data imported ready for the next step. This panel allows the user to follow the different steps of the analysis. Central panel : Is the main screen, showing the details and options of the selected tool. Figure 8 Galaxy interface . Divided in 3 parts Galaxy s interface go from the left selecting the tools to the right where the results are displayed. Tutorial This tutorial describes how to identify a list of proteins from tandem mass spectrometry data. Analyses of this type are a fundamental part of most proteomics studies. The basic idea is to match tandem MS spectra obtained from a sample with equivalent theoretical spectra against a reference protein database. The process is referred to as protein identification , although amino acid sequences are not obtained de novo with this method. Objectives The objective of this tutorial is to perform protein identification from MS/MS spectra data using the X!Tandem tool in GALAXY see Figure 9 . The basic steps involved are: Loading UniProt 4 proteome data in GALAXY (fasta file format) Loading your MS/MS spectra in GALAXY Run X!Tandem proteomics search Sorting and analysing the results The tutorial will finish with an exercise where you repeat the same protocol but with your own proteome as the reference database instead of using UniProt. Figure 9 - General flowchart of this training . The first part is to upload datasets to be investigated. Then the training will cover the key parts of the configuration of a X!Tandem search. Finally, we look at sorting and analysing the results. The aim of the training will be to create a list of all proteins that can be confidently said to be present in the sample. This tutorial uses the following open source tools: X!Tandem search engine Trans Proteomic Pipeline 11 GALAXY platform with tools already installed This tutorial uses an E. Coli MS/MS spectra dataset s1-000.RAW.gz that has already been converted to MGF format for use in the Galaxy. The MGF data file is over 200MB, instead of downloading it to your computer, you can upload this file into Galaxy directly. Copy this link (https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/EColi_K12_MS_Spectra.mgf) and follow the instructions below. Note If you downloaded the RAW data (which can be obtained from here ), you will need to manually convert it to MGF before it can be used in Galaxy. To convert the file, follow the following steps: Download and Install ProteoWizard on your local computer. (Note: version - Windows 64-bit installer(able to convert vendor files except T2D) Start the ProteoWizard software on your local computer with windows operating system. * Upload the raw file and convert to mgf format STEP 1: Data import Before importing data, Name your history. Click on the Unnamed history on the top of the right panel until you get the cursor. Delete and type in Protein Identificaiton E.coli K12 or a more meaningful name. You must hit Enter , otherwise the name will not be saved. Next, import data into GALAXY. On the left panel click on the upload button as shown below: A new window will open, where you can select a method to upload your data: Choose local file , Choose FTP file or Paste/Fetch data . Click on Paste/Fetch data then copy and paste the URL of the mass spectrometer file into the textbox. Tip : You can also use the Get Data Upload file tool to obtain the same result. Here you want to upload your MS/MS spectra. Warning : X!Tandem only accepts mgf files in GALAXY. Other file formats have to be converted beforehand. A useful tool for that is msconvert 12 (download : here . This tool only working on Windows). STEP 2: Import Reference Data We will first use the UniProt Database as our reference data to search against. Select the tool named Protein Database Downloader Choose the database: UniProtKB Select the organism of interest: Escherichia Coli (strain K12) Click on Execute You will see your history update with the new data imports Rename your Protein Database by clicking on pencil icon. Select Edit Attributes In Name , type in EColi_K12_UniProt_Database Click Save STEP 3: X!Tandem MS/MS Search This part of the tutorial is to perform the X!Tandem MS/MS search. The tool can be found in the left panel under the section Proteomics Tools X!Tandem MSMS Search In the central section, you should see the following options. Below the key parameters are explained in detail. X!Tandem proposes many options, the key options of interest are: Uploaded FASTA file : this parameter is to select the fasta file that will be used as the proteins database. MSMS File : select the spectra file to analyse. Variable Modifications : this option considers possible modification on each residue (which impact the MS/MS spectra). Fixed Modifications : this option allows you to specify any known modification. Missed Cleavages Allowed : when a cleavage reagent (either chemical or enzymatic) is used to cleave the peptide backbone of a protein to produce peptides, different sites along the backbone will have different reaction rates and kinetics. Therefore, there may be some sites that should be cleaved by the reagent that are not completely cleaved during the course of the experiment. The value of this parameter represents the maximum number of missed cleavage sites allowed within a peptide. Enzyme : specify the enzyme that has been used to cleave the proteins (affecting the interpretation of the spectrum). Fragment ion tolerance : define the minimum weight (in Da) of the fragmented ions, default value is 0.5. Precursor ion tolerance : define the minimum weight (in Da) of the precursor ions. In this tutorial, we are using the following parameters: Parameters Name Value Default Value Uploaded FASTA file EColi_K12_UniProt_Database MSMS File EColi_K12_MS_Spectra.mgf Variable Modifications Oxidation M Fixed Modifications Carbamidomethyl C Missed Cleavages Allowed 2 2 Enzyme Trypsin Trypsin Fragment ion tolerance 0.5 0.5 Precursor ion tolerance 10 ppm 10 ppm Leave all other parameters as their default settings. Click on Execute The history should update with a new entry, the output file of the X!Tandem Rename the output by clicking on the icon You can view the output by click on the name in the history panel. STEP 4: Convert X!Tandem XML to Table The output of X!Tandem is an XML format, which is not easy to decipher. In order to get a more readable file, we will convert the XML format to a table. This is a two step process: Select Proteomics Tools Tandem to pepXML Select your tandem file in the Input File field Click on Execute The history should update with a new pepXML file. The pepXML file is still a XML file and needs to be converted to a tabular. Select Proteomics Tools PepXML to Table Select your pepXML file in the Input File field This history should update with a new file After the X!Tandem search we obtain a list of proteins present in the sample data from Step 1: Tabular name Tandem file XML designation Definition Protein label Protein name according to the database used for the MS/MS search Peptide seq Peptide sequence Assumed_charge z Parent ion mass (plus a proton) from the spectrum Calc_neutral_pep_mass mh (+mass of a proton) Parent ion mass calculated from the spectrum Neutral_mass mh (+mass of a proton) Calculated peptide mass (plus a proton) Retention_time rt Length of time between injection and position of the target compound peak. 10 Start_scan id id of the group treated (where the analysis starts) End_scan id id of the domain treated (usually the same as start_scan, no consideration of the doted name in the original XML file) Search_engine Name of the search engine used, in our case X!Tandem (associated with the scoring method : k-score ) Raw_score expect Expectation value for the top ranked protein identified with this spectrum Note: You can find all the details on the X!Tandem output file here: The file format for X! series search engines . The tandem XML file contains more information than the tabular format which can also be of further use (e.i. the score for the different ions x, y, z, a, b, c ) References X!Tandem website and X!Tandem documentation . Craig, R., and R. C. Beavis. 2004. \u201cTANDEM: matching proteins with tandem mass spectra.\u201d Bioinformatics 20, no. 9 (June): 1466-67. http://dx.doi.org/10.1093/bioinformatics/bth092. Nesvizhskii, Alexey I. Protein Identification By Tandem Mass Spectrometry And Sequence Database Searching . Mass Spectrometry Data Analysis in Proteomics 87-120. Mass Spectrometry Data Analysis in Proteomics GALAXY platform : https://usegalaxy.org/. You can also find a more extensive documentation here : https://galaxyproject.org/. Afgan, Enis et al. The Galaxy Platform For Accessible, Reproducible And Collaborative Biomedical Analyses: 2016 Update . Nucleic Acids Res 44.W1 (2016): W3-W10. The Galaxy platform for accessible, reproducible and collaborative biomedical analyses: 2016 update UniProt : http://www.uniprot.org/. Apweiler, R. Uniprot: The Universal Protein Knowledgebase . Nucleic Acids Research 32.90001 (2004): 115D-119. UniProt: the Universal Protein knowledgebase High-performance liquid chromatography (HPLC): How Does High Performance Liquid Chromatography Work ? Mant, Colin T. et al. HPLC Analysis And Purification Of Peptides . Peptide Characterization and Application Protocols (2007): 3-55. Aebersold, Ruedi and Matthias Mann. Mass Spectrometry-Based Proteomics . Nature 422.6928 (2003): 198-207. Mass Spectrometry-Based Proteomics Deutsch, E. W. File Formats Commonly Used In Mass Spectrometry Proteomics . Molecular Cellular Proteomics 11.12 (2012): 1612-1621. File Formats Commonly Used in Mass Spectrometry Proteomics Mascot Generic Format (MGF) file is likely the most common text format for MS/MS data encoding in the form of a peak list. This file encodes multiple MS/MS spectra in a single file via m/z, intensity pairs separated by headers. MGF file format The MGF format allows parameters that can be found after the BEGIN IONS statement. Embedded Parameters The retention time of a peak can be used as means of qualitative identification. The length of time between injection and position of the target compound peak. Retention Time Parameters Retention Time explained for GC/MS The Trans-Proteomic Pipeline (TPP) website : http://tools.proteomecenter.org/wiki/index.php?title=Software:TPP. Deutsch, Eric W. et al. A Guided Tour Of The Trans-Proteomic Pipeline . Proteomics 10.6 (2010): 1150-1159. A Guided Tour of the Trans-Proteomic Pipeline ProteoWizard Tools website : http://proteowizard.sourceforge.net/tools.shtml. Holman, Jerry D., David L. Tabb, and Parag Mallick. Employing Proteowizard To Convert Raw Mass Spectrometry Data . Current Protocols in Bioinformatics (2014): 13.24.1-13.24.9. Employing ProteoWizard to Convert Raw Mass Spectrometry Data","title":"Home"},{"location":"modules/xtandem/#protein-identification-using-xtandem","text":"","title":"Protein identification using X!Tandem"},{"location":"modules/xtandem/#introduction","text":"The high-throughput nature of proteomics mass spectrometry is enabled by a productive combination of data acquisition protocols and the computational tools used to interpret the resulting spectra (list of peaks). One of the key components in mainstream protocols is the generation of tandem mass (MS/MS) spectra by peptide fragmentation. This approach is currently used in the large majority of proteomics experiments to routinely identify hundreds to thousands of proteins from single mass spectrometry runs. In order to do that multiple tools have to be employed: LC-MS/MS to segregate components of proteomic samples associated with protein identification (see Figure 1 ) softwares, X!Tandem 1 , Mascot or SEQUEST all of which perform protein identification but with different algorithms. Figure 1 General overview of the experimental steps and flow of data in protein identification using shotgun proteomics experiment. 2 . Figure 1 shows a general overview of the experimental steps in protein identification. Sample proteins are first cleaved into peptides, which are then separated using chromatography (e.g. HPLC). The peptides are then ionised and then selected ions are fragmented to produce signature tandem mass spectrometry (MS/MS) spectra. Peptides are identified from the MS/MS spectra by using programs such as X!Tandem, which search against a database of known peptides. Sequences of the identified peptides are used to infer which proteins are present in the original sample.","title":"Introduction"},{"location":"modules/xtandem/#background","text":"","title":"Background"},{"location":"modules/xtandem/#lc-msms-analysis","text":"Liquid Chromatography (LC) is a technique used to separate molecules in the solvent (mobile phase). Nowadays liquid chromatography utilising high pressure to force the solvent through the columns packed with porous particles (stationary phase) is referred as High Pressure Liquid Chromatography (HPLC) see Figure 2 . After purifying the proteomic cell content, LC is able to separate the different proteins which are injected in the mass spectrometer. Each peak from the results will be analysed by mass spectrometry. Figure 2 Schema of High Pressure Liquid Chromatography (HPLC) 5 . Compounds of the mixture are separated in the HPLC column according to various parameters (polarity, charge, affinity etc). The type of separation depends on the column being used). A detector flow cell is used to detect the separated compounds band. Mass spectrometry (MS) see Figure 3 has been widely used to analyse biological samples and has evolved into an indispensable tool for proteomics research. Fundamentally, MS measures the mass-to-charge ratio (m/z) of gas-phase ions. Mass spectrometers consist of an ion source that converts analyte molecules into gas-phase ions, a mass analyser that separates ionised analytes on the basis of m/z ratio and a detector that records the number of ions at each m/z value. Figure 3 Schema of mass specter . A mass spectrometer consists of three components: an ion source, a mass analyzer, and a detector. The ionizer converts a portion of the sample into ions. There is a wide variety of ionization techniques, depending on the phase (solid, liquid, gas) of the sample and the efficiency of various ionization mechanisms for the unknown species. An extraction system removes ions from the sample, which are then targeted through the mass analyzer and onto the detector. The differences in masses of the fragments allows the mass analyzer to sort the ions by their mass-to-charge ratio. The detector measures the value of an indicator quantity and thus provides data for calculating the abundances of each ion present. 6 Tandem mass spectrometry (MS/MS) see Figure 4 is a key technique for protein or peptide sequencing and post-translational modifications analysis. Collision-induced dissociation (CID) has been the most widely used MS/MS technique in proteomics research. In this method, gas-phase peptide/protein cations are internally heated by multiple collisions with rare gas atoms. The result is fragmented ions that, after the detection phase and reconstitution, reveal the amino-acid chains. Figure 4 Schema of a tandem mass spectrometry associated with liquid chromatography (LC-MS/MS) . This technique combine separation form liquid chromatography and m/z ratio analyse of the tandem mass spectrometry in order to generate spectra. File formats : During a full proteomics analysis, as seen in Figure 5 , many files are created. Every step has its own multiple file formats: Figure 5 Multiple formats during MS treatment 7 . From the sample processing to the final file, various file formats can be used. This schema shows the different processes and file formats that are generated during a full MS experiment. For this tutorial we will focus on the Informatics Analysis part using the following file formats: fasta : fasta files are mainly used for representing either nucleotide sequences or peptide sequences, using single-letter codes. MGF : Mascot Generic Format (MGF) file is the most common format for MS/MS data encoding in the form of a peak list 8 . This file encodes multiple MS/MS spectra in a single file listing the [m/z, intensity] pairs separated by headers see Figure 6 . More precisely each query represents a complete MS/MS spectrum delimited by BEGIN IONS and END IONS statements. Embedded parameters 9 can be found after each BEGIN IONS statement. An example entry is shown in the figure below: Figure 6 Sample of a MGF file . MGF files are used for enconding MS results, multiple spectra can be enconded in one MGF. Every spectrum begins with the BEGINS IONS assessment and finishes with END IONS . MGF files can be divided in 2 parts : The header : containing information about the embedded Search Parameters. Ions information : the first figure is the ion mass, the second is the ion charge.","title":"LC-MS/MS Analysis"},{"location":"modules/xtandem/#xtandem","text":"X!Tandem is an open source software that can match tandem mass spectra (usually the experiment) with peptide sequences from a database. This process allows identification of proteins in one or more samples. The X!Tandem search engine calculates a statistical confidence (expectation value) for all of the individual spectrum-to-sequence assignments. Some spectra might map to more than one protein, in this case X!Tandem will pick the best match with the highest confidence score (see Figure 7 ). The output is a lists all of the high confidence assignments. Figure 7 Schema of the X!Tandem analysis 2 . After LC-MS/MS analysis the experimental spectrum is compared to a theoretical spectrum made from a protein database. This comparison leads to a list of peptide match ranked by score.","title":"X!Tandem"},{"location":"modules/xtandem/#galaxy","text":"GALAXY 3 is an open source, web-based platform for biomedical research. GALAXY allows users to perform multi-omics data analyses: genomics, proteomics, transcriptomics and more. Numerous tools are available to achieve various analyses. This tutorial will focus on proteomics identification: matching the experimental data, spectra from LC-MS/MS analysis against data from a protein database using X!Tandem. Before starting, a quick overview of the GALAXY interface see Figure 8 . The interface is divided into three parts: Left panel : List the tools that are available. A search textbox is at the top of the panel in order to find the tool you want. Right panel : Is the history of all the data outputs from previous steps and subsequently becoming the input for the next step. For example, the figure below shows the data imported ready for the next step. This panel allows the user to follow the different steps of the analysis. Central panel : Is the main screen, showing the details and options of the selected tool. Figure 8 Galaxy interface . Divided in 3 parts Galaxy s interface go from the left selecting the tools to the right where the results are displayed.","title":"GALAXY"},{"location":"modules/xtandem/#tutorial","text":"This tutorial describes how to identify a list of proteins from tandem mass spectrometry data. Analyses of this type are a fundamental part of most proteomics studies. The basic idea is to match tandem MS spectra obtained from a sample with equivalent theoretical spectra against a reference protein database. The process is referred to as protein identification , although amino acid sequences are not obtained de novo with this method.","title":"Tutorial"},{"location":"modules/xtandem/#objectives","text":"The objective of this tutorial is to perform protein identification from MS/MS spectra data using the X!Tandem tool in GALAXY see Figure 9 . The basic steps involved are: Loading UniProt 4 proteome data in GALAXY (fasta file format) Loading your MS/MS spectra in GALAXY Run X!Tandem proteomics search Sorting and analysing the results The tutorial will finish with an exercise where you repeat the same protocol but with your own proteome as the reference database instead of using UniProt. Figure 9 - General flowchart of this training . The first part is to upload datasets to be investigated. Then the training will cover the key parts of the configuration of a X!Tandem search. Finally, we look at sorting and analysing the results. The aim of the training will be to create a list of all proteins that can be confidently said to be present in the sample. This tutorial uses the following open source tools: X!Tandem search engine Trans Proteomic Pipeline 11 GALAXY platform with tools already installed This tutorial uses an E. Coli MS/MS spectra dataset s1-000.RAW.gz that has already been converted to MGF format for use in the Galaxy. The MGF data file is over 200MB, instead of downloading it to your computer, you can upload this file into Galaxy directly. Copy this link (https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/EColi_K12_MS_Spectra.mgf) and follow the instructions below. Note If you downloaded the RAW data (which can be obtained from here ), you will need to manually convert it to MGF before it can be used in Galaxy. To convert the file, follow the following steps: Download and Install ProteoWizard on your local computer. (Note: version - Windows 64-bit installer(able to convert vendor files except T2D) Start the ProteoWizard software on your local computer with windows operating system. * Upload the raw file and convert to mgf format","title":"Objectives"},{"location":"modules/xtandem/#step-1-data-import","text":"Before importing data, Name your history. Click on the Unnamed history on the top of the right panel until you get the cursor. Delete and type in Protein Identificaiton E.coli K12 or a more meaningful name. You must hit Enter , otherwise the name will not be saved. Next, import data into GALAXY. On the left panel click on the upload button as shown below: A new window will open, where you can select a method to upload your data: Choose local file , Choose FTP file or Paste/Fetch data . Click on Paste/Fetch data then copy and paste the URL of the mass spectrometer file into the textbox. Tip : You can also use the Get Data Upload file tool to obtain the same result. Here you want to upload your MS/MS spectra. Warning : X!Tandem only accepts mgf files in GALAXY. Other file formats have to be converted beforehand. A useful tool for that is msconvert 12 (download : here . This tool only working on Windows).","title":"STEP 1: Data import"},{"location":"modules/xtandem/#step-2-import-reference-data","text":"We will first use the UniProt Database as our reference data to search against. Select the tool named Protein Database Downloader Choose the database: UniProtKB Select the organism of interest: Escherichia Coli (strain K12) Click on Execute You will see your history update with the new data imports Rename your Protein Database by clicking on pencil icon. Select Edit Attributes In Name , type in EColi_K12_UniProt_Database Click Save","title":"STEP 2: Import Reference Data"},{"location":"modules/xtandem/#step-3-xtandem-msms-search","text":"This part of the tutorial is to perform the X!Tandem MS/MS search. The tool can be found in the left panel under the section Proteomics Tools X!Tandem MSMS Search In the central section, you should see the following options. Below the key parameters are explained in detail. X!Tandem proposes many options, the key options of interest are: Uploaded FASTA file : this parameter is to select the fasta file that will be used as the proteins database. MSMS File : select the spectra file to analyse. Variable Modifications : this option considers possible modification on each residue (which impact the MS/MS spectra). Fixed Modifications : this option allows you to specify any known modification. Missed Cleavages Allowed : when a cleavage reagent (either chemical or enzymatic) is used to cleave the peptide backbone of a protein to produce peptides, different sites along the backbone will have different reaction rates and kinetics. Therefore, there may be some sites that should be cleaved by the reagent that are not completely cleaved during the course of the experiment. The value of this parameter represents the maximum number of missed cleavage sites allowed within a peptide. Enzyme : specify the enzyme that has been used to cleave the proteins (affecting the interpretation of the spectrum). Fragment ion tolerance : define the minimum weight (in Da) of the fragmented ions, default value is 0.5. Precursor ion tolerance : define the minimum weight (in Da) of the precursor ions. In this tutorial, we are using the following parameters: Parameters Name Value Default Value Uploaded FASTA file EColi_K12_UniProt_Database MSMS File EColi_K12_MS_Spectra.mgf Variable Modifications Oxidation M Fixed Modifications Carbamidomethyl C Missed Cleavages Allowed 2 2 Enzyme Trypsin Trypsin Fragment ion tolerance 0.5 0.5 Precursor ion tolerance 10 ppm 10 ppm Leave all other parameters as their default settings. Click on Execute The history should update with a new entry, the output file of the X!Tandem Rename the output by clicking on the icon You can view the output by click on the name in the history panel.","title":"STEP 3: X!Tandem MS/MS Search"},{"location":"modules/xtandem/#step-4-convert-xtandem-xml-to-table","text":"The output of X!Tandem is an XML format, which is not easy to decipher. In order to get a more readable file, we will convert the XML format to a table. This is a two step process: Select Proteomics Tools Tandem to pepXML Select your tandem file in the Input File field Click on Execute The history should update with a new pepXML file. The pepXML file is still a XML file and needs to be converted to a tabular. Select Proteomics Tools PepXML to Table Select your pepXML file in the Input File field This history should update with a new file After the X!Tandem search we obtain a list of proteins present in the sample data from Step 1: Tabular name Tandem file XML designation Definition Protein label Protein name according to the database used for the MS/MS search Peptide seq Peptide sequence Assumed_charge z Parent ion mass (plus a proton) from the spectrum Calc_neutral_pep_mass mh (+mass of a proton) Parent ion mass calculated from the spectrum Neutral_mass mh (+mass of a proton) Calculated peptide mass (plus a proton) Retention_time rt Length of time between injection and position of the target compound peak. 10 Start_scan id id of the group treated (where the analysis starts) End_scan id id of the domain treated (usually the same as start_scan, no consideration of the doted name in the original XML file) Search_engine Name of the search engine used, in our case X!Tandem (associated with the scoring method : k-score ) Raw_score expect Expectation value for the top ranked protein identified with this spectrum Note: You can find all the details on the X!Tandem output file here: The file format for X! series search engines . The tandem XML file contains more information than the tabular format which can also be of further use (e.i. the score for the different ions x, y, z, a, b, c )","title":"STEP 4: Convert X!Tandem XML to Table"},{"location":"modules/xtandem/#references","text":"X!Tandem website and X!Tandem documentation . Craig, R., and R. C. Beavis. 2004. \u201cTANDEM: matching proteins with tandem mass spectra.\u201d Bioinformatics 20, no. 9 (June): 1466-67. http://dx.doi.org/10.1093/bioinformatics/bth092. Nesvizhskii, Alexey I. Protein Identification By Tandem Mass Spectrometry And Sequence Database Searching . Mass Spectrometry Data Analysis in Proteomics 87-120. Mass Spectrometry Data Analysis in Proteomics GALAXY platform : https://usegalaxy.org/. You can also find a more extensive documentation here : https://galaxyproject.org/. Afgan, Enis et al. The Galaxy Platform For Accessible, Reproducible And Collaborative Biomedical Analyses: 2016 Update . Nucleic Acids Res 44.W1 (2016): W3-W10. The Galaxy platform for accessible, reproducible and collaborative biomedical analyses: 2016 update UniProt : http://www.uniprot.org/. Apweiler, R. Uniprot: The Universal Protein Knowledgebase . Nucleic Acids Research 32.90001 (2004): 115D-119. UniProt: the Universal Protein knowledgebase High-performance liquid chromatography (HPLC): How Does High Performance Liquid Chromatography Work ? Mant, Colin T. et al. HPLC Analysis And Purification Of Peptides . Peptide Characterization and Application Protocols (2007): 3-55. Aebersold, Ruedi and Matthias Mann. Mass Spectrometry-Based Proteomics . Nature 422.6928 (2003): 198-207. Mass Spectrometry-Based Proteomics Deutsch, E. W. File Formats Commonly Used In Mass Spectrometry Proteomics . Molecular Cellular Proteomics 11.12 (2012): 1612-1621. File Formats Commonly Used in Mass Spectrometry Proteomics Mascot Generic Format (MGF) file is likely the most common text format for MS/MS data encoding in the form of a peak list. This file encodes multiple MS/MS spectra in a single file via m/z, intensity pairs separated by headers. MGF file format The MGF format allows parameters that can be found after the BEGIN IONS statement. Embedded Parameters The retention time of a peak can be used as means of qualitative identification. The length of time between injection and position of the target compound peak. Retention Time Parameters Retention Time explained for GC/MS The Trans-Proteomic Pipeline (TPP) website : http://tools.proteomecenter.org/wiki/index.php?title=Software:TPP. Deutsch, Eric W. et al. A Guided Tour Of The Trans-Proteomic Pipeline . Proteomics 10.6 (2010): 1150-1159. A Guided Tour of the Trans-Proteomic Pipeline ProteoWizard Tools website : http://proteowizard.sourceforge.net/tools.shtml. Holman, Jerry D., David L. Tabb, and Parag Mallick. Employing Proteowizard To Convert Raw Mass Spectrometry Data . Current Protocols in Bioinformatics (2014): 13.24.1-13.24.9. Employing ProteoWizard to Convert Raw Mass Spectrometry Data","title":"References"},{"location":"scrapbook/data/data_access/","text":"Data: access, using, moving how to get data from x to y Introduction FIXME Learning Objectives At the end of this tutorial you should be able to: FIXME FIXME FIXME Main Tutorial FIXME Note Please take note of this because we care about you. Hint This is a useful tip for you because we like you. Warning Please be careful here, you could really mess things up. What next? FIXME","title":"Data: access, using, moving"},{"location":"scrapbook/data/data_access/#data-access-using-moving","text":"how to get data from x to y","title":"Data: access, using, moving"},{"location":"scrapbook/data/data_access/#introduction","text":"FIXME","title":"Introduction"},{"location":"scrapbook/data/data_access/#learning-objectives","text":"At the end of this tutorial you should be able to: FIXME FIXME FIXME","title":"Learning Objectives"},{"location":"scrapbook/data/data_access/#main-tutorial","text":"FIXME Note Please take note of this because we care about you. Hint This is a useful tip for you because we like you. Warning Please be careful here, you could really mess things up.","title":"Main Tutorial"},{"location":"scrapbook/data/data_access/#what-next","text":"FIXME","title":"What next?"},{"location":"scrapbook/data/formats/bam/","text":"BAM File [from LSCC docs] SAM is a text format which is not space efficient. Binary Sequence Alignment is a compressed version of SAM. Data in a BAM file is binary and therefore can t be visualised as text. If you try and visualise in Galaxy, it will default to downloading the file. FIXME link","title":"BAM File"},{"location":"scrapbook/data/formats/bam/#bam-file","text":"[from LSCC docs] SAM is a text format which is not space efficient. Binary Sequence Alignment is a compressed version of SAM. Data in a BAM file is binary and therefore can t be visualised as text. If you try and visualise in Galaxy, it will default to downloading the file. FIXME link","title":"BAM File"},{"location":"scrapbook/data/formats/file_formats/","text":"File formats FIXME: link to images? File format File extensions Description FASTA SequenceID sequence More information. .fasta .fas .fa generic FASTA file .fna nucleotides .ffn nucleotides for coding regions only? .faa amino acids .frn nucleotides? for non-coding RNA regions FASTQ .fastq @SequenceID sequence + nucleotide quality scores SAM .sam tab-delimited text file of reads aligned to a reference. e.g. mapped position, sequence, quality scores. More [FIXME: can t link] BAM .bam Compressed version of SAM file. More [FIXME: can t link] GBK .gbk Genbank format. Sequence information, features, protein translations, DNA sequence. GFF .gff General Feature Format. Tab-delimited. Each line is a feature. also known as GTF? mention gff3? e.g. it needs to be version 3? More information. VCF .vcf Variant Call Format. Tab-delimited. Header, then one line per identified variant. pileup Each line is a nucleotide in the sequence, with information on how all the reads are mapped to that position. More [FIXME: can t link] there are some nice images etc in here https://docs.google.com/document/pub?id=1fouC29Lq0CXxQQCpuojrR5RXbdzMdxRf8ZID01XYNqI#h.18e90b8fc68f","title":"File formats"},{"location":"scrapbook/data/formats/file_formats/#file-formats","text":"FIXME: link to images? File format File extensions Description FASTA SequenceID sequence More information. .fasta .fas .fa generic FASTA file .fna nucleotides .ffn nucleotides for coding regions only? .faa amino acids .frn nucleotides? for non-coding RNA regions FASTQ .fastq @SequenceID sequence + nucleotide quality scores SAM .sam tab-delimited text file of reads aligned to a reference. e.g. mapped position, sequence, quality scores. More [FIXME: can t link] BAM .bam Compressed version of SAM file. More [FIXME: can t link] GBK .gbk Genbank format. Sequence information, features, protein translations, DNA sequence. GFF .gff General Feature Format. Tab-delimited. Each line is a feature. also known as GTF? mention gff3? e.g. it needs to be version 3? More information. VCF .vcf Variant Call Format. Tab-delimited. Header, then one line per identified variant. pileup Each line is a nucleotide in the sequence, with information on how all the reads are mapped to that position. More [FIXME: can t link] there are some nice images etc in here https://docs.google.com/document/pub?id=1fouC29Lq0CXxQQCpuojrR5RXbdzMdxRf8ZID01XYNqI#h.18e90b8fc68f","title":"File formats"},{"location":"scrapbook/data/formats/pileup/","text":"Pileup format [from LSCC docs] Pileup Format A pileup file has as many lines as there are bases in the reference sequence that are aligned with reads in the SAM/BAM file. Each line contains information about every base found in the sequence reads that corresponds to the reference base on that line. The format of a pileup record is: ReferenceSeq [string] - name of the reference sequence Coordinate [integer] - position in the reference sequence ReferenceBase [A/C/G/T/N] - reference base at that position Num. Reads [integer] - number of reads aligning to that base ReadBases [variable length string, see below] BaseQualities [variable length string, Phred encoded] ReadBases: Each separate read that covers the base is represented here. The more reads that cover this base, the longer this string . = match on forward strand for that base , = match on reverse strand ACGTN = mismatch on forward acgtn = mismatch on reverse +[0-9]+[ACGTNacgtn]+ = insertion between this reference position and the next [0-9]+[ACGTNacgtn]+ = deletion between this reference position and the next ^ = start of read $ = end of read BaseQualities = one character per base in ReadBases, ASCII encoded Phred scores Example: chr1 272 T 24 ,.$.....,,.,., ,,,.,..^+. +; = ; ;7 chr1 273 T 23 ,.....,,.,., ,,,.,..A ; 3 = ; + chr1 274 T 23 ,.$....,,.,., ,,,., 7 7; ; = ; ; 6 chr1 275 A 23 ,$....,,.,., ,,,., ^l. +;9* = :; chr1 276 G 22 T,,.,., ,,,.,.... 33;+ 7=7 7 1; 6 In this example there are 5 chromosomal positions represented, with between 22 and 24 reads aligning to each of the positions. There are two mismatches: an A in position 273 and a T in position 276. That is, only a single read contained that mismatch in either case. More information on pileup format here: http://samtools.sourceforge.net/pileup.shtml Pileup file in Galaxy","title":"Pileup format"},{"location":"scrapbook/data/formats/pileup/#pileup-format","text":"[from LSCC docs]","title":"Pileup format"},{"location":"scrapbook/data/formats/pileup/#pileup-format_1","text":"A pileup file has as many lines as there are bases in the reference sequence that are aligned with reads in the SAM/BAM file. Each line contains information about every base found in the sequence reads that corresponds to the reference base on that line. The format of a pileup record is: ReferenceSeq [string] - name of the reference sequence Coordinate [integer] - position in the reference sequence ReferenceBase [A/C/G/T/N] - reference base at that position Num. Reads [integer] - number of reads aligning to that base ReadBases [variable length string, see below] BaseQualities [variable length string, Phred encoded] ReadBases: Each separate read that covers the base is represented here. The more reads that cover this base, the longer this string . = match on forward strand for that base , = match on reverse strand ACGTN = mismatch on forward acgtn = mismatch on reverse +[0-9]+[ACGTNacgtn]+ = insertion between this reference position and the next [0-9]+[ACGTNacgtn]+ = deletion between this reference position and the next ^ = start of read $ = end of read BaseQualities = one character per base in ReadBases, ASCII encoded Phred scores Example: chr1 272 T 24 ,.$.....,,.,., ,,,.,..^+. +; = ; ;7 chr1 273 T 23 ,.....,,.,., ,,,.,..A ; 3 = ; + chr1 274 T 23 ,.$....,,.,., ,,,., 7 7; ; = ; ; 6 chr1 275 A 23 ,$....,,.,., ,,,., ^l. +;9* = :; chr1 276 G 22 T,,.,., ,,,.,.... 33;+ 7=7 7 1; 6 In this example there are 5 chromosomal positions represented, with between 22 and 24 reads aligning to each of the positions. There are two mismatches: an A in position 273 and a T in position 276. That is, only a single read contained that mismatch in either case. More information on pileup format here: http://samtools.sourceforge.net/pileup.shtml","title":"Pileup Format"},{"location":"scrapbook/data/formats/pileup/#pileup-file-in-galaxy","text":"","title":"Pileup file in Galaxy"},{"location":"scrapbook/data/formats/sam/","text":"SAM File [from LSCC docs] Sequence Alignment/Map (SAM) format records all information relevant to how a set of reads aligns to a reference genome. A SAM file has an optional set of header lines describing the context of the alignment, then one line per read, with the following format: 11 mandatory fields (+ variable number of optional fields) 1 QNAME: Query name of the read 2 FLAG 3 RNAME: Reference sequence name 4 POS: Position of alignment in reference sequence 5 MAPQ: Mapping quality (Phred-scaled) 6 CIGAR: String that describes the specifics of the alignment against the reference 7 MRNM 8 MPOS 9 ISIZE 10 SEQQuery: Sequence on the same strand as the reference 11 QUAL: Query quality (ASCII-33=Phred base quality) SAM example SRR017937.312 16 chr20 43108717 37 76M * 0 0 TGAGCCTCCGGGCTATGTGTGCTCACTGACAGAAGACCTGGTCACCAAAGCCCGGGAAGAGCTGCAGGAAAAGCCG ?,@A=A @ABBB@@9BB@:@5 BBB2 BBB@BBB?;;BABBBBBBB@ For this example: QNAME = SRR017937.312 - this is the name of this read FLAG = 16 - see the format description below RNAME = chr20 - this read aligns to chromosome 20 POS = 43108717 - this read aligns the sequence on chr20 at position 43108717 MAPQ = 37 - this is quite a high quality score for the alignment (b/w 0 and 90) CIGAR = 76M - this read aligns to the reference segment across all bases (76 Matches means no deletions or insertions. Note that aligns can mean aligns with mismatches - mismatches that don t affect the alignment are not recorded in this field) MRNM = * - see the format description below MPOS = 0 as there is no mate for this read - the sequenced DNA library was single ended, not mate paired*. ISIZE = 0 as there is no mate for this read SEQQuery = the 76bp sequence of the reference segment QUAL = per-base quality scores for each position on the alignment. This is just a copy of what is in the FASTQ file NOTE: reads are shown mapped to the sense strand of the reference, and bases are listed in 5 - 3 order. This is important because an actual read might be from the other strand of DNA. The alignment tool will try to map the read as it is, and also the reverse compliment. If it was on the other strand then the reverse compliment is shown in the SAM file, rather than the original read itself More information. [FIXME link]","title":"SAM File"},{"location":"scrapbook/data/formats/sam/#sam-file","text":"[from LSCC docs] Sequence Alignment/Map (SAM) format records all information relevant to how a set of reads aligns to a reference genome. A SAM file has an optional set of header lines describing the context of the alignment, then one line per read, with the following format: 11 mandatory fields (+ variable number of optional fields) 1 QNAME: Query name of the read 2 FLAG 3 RNAME: Reference sequence name 4 POS: Position of alignment in reference sequence 5 MAPQ: Mapping quality (Phred-scaled) 6 CIGAR: String that describes the specifics of the alignment against the reference 7 MRNM 8 MPOS 9 ISIZE 10 SEQQuery: Sequence on the same strand as the reference 11 QUAL: Query quality (ASCII-33=Phred base quality)","title":"SAM File"},{"location":"scrapbook/data/formats/sam/#sam-example","text":"SRR017937.312 16 chr20 43108717 37 76M * 0 0 TGAGCCTCCGGGCTATGTGTGCTCACTGACAGAAGACCTGGTCACCAAAGCCCGGGAAGAGCTGCAGGAAAAGCCG ?,@A=A @ABBB@@9BB@:@5 BBB2 BBB@BBB?;;BABBBBBBB@ For this example: QNAME = SRR017937.312 - this is the name of this read FLAG = 16 - see the format description below RNAME = chr20 - this read aligns to chromosome 20 POS = 43108717 - this read aligns the sequence on chr20 at position 43108717 MAPQ = 37 - this is quite a high quality score for the alignment (b/w 0 and 90) CIGAR = 76M - this read aligns to the reference segment across all bases (76 Matches means no deletions or insertions. Note that aligns can mean aligns with mismatches - mismatches that don t affect the alignment are not recorded in this field) MRNM = * - see the format description below MPOS = 0 as there is no mate for this read - the sequenced DNA library was single ended, not mate paired*. ISIZE = 0 as there is no mate for this read SEQQuery = the 76bp sequence of the reference segment QUAL = per-base quality scores for each position on the alignment. This is just a copy of what is in the FASTQ file NOTE: reads are shown mapped to the sense strand of the reference, and bases are listed in 5 - 3 order. This is important because an actual read might be from the other strand of DNA. The alignment tool will try to map the read as it is, and also the reverse compliment. If it was on the other strand then the reverse compliment is shown in the SAM file, rather than the original read itself More information. [FIXME link]","title":"SAM example"},{"location":"scrapbook/dge/","text":"Differential Gene Expression Background Differential Gene Expression (DGE) is the process of determining whether any genes were expressed at a different level between two conditions. For example, the conditions could be wildtype versus mutant, or two growth conditions. Usually multiple biological replicates are done for each condition - these are needed to separate variation within the condition from that between the conditions. Learning Objectives At the end of this tutorial you should be able to: Align RNA-Seq data to a reference genome Count transcripts for each sample Perform statistical analysis to obtain a list of differentially expressed genes Interpret the DGE list Visualize the results in Degust Input data A typical experiment will have 2 conditions each with 3 replicates, for a total of 6 samples. Each sample will be RNA-Seq data , either as one file per sample (single-end reads / SE) or two files (paired-end reads / PE). Condition 1 Condition 2 Replicate 1 1 4 Replicate 2 2 5 Replicate 3 3 6 FIXME: load reads Prepare reference FIXME: tricky need it in correct format for htseq-count GFF2 ? Align reads FIXME: use BWA MEM with defaults? for each sample, should be able to use the 6 at once feature of Galaxy? Count reads FIXME: htseq-count, use the CDS and RNA features to align to, get count table DGE Analysis Within Galaxy FIXME: Need to use Voom/Limma here Within Degust FIXME: need to combine each of the results in count-reads section into a single table (using galaxy table tools?) but need to munge in the annotation as well, so i or simon will need to add new tools to toolshed to do this What next? FIXME","title":"Differential Gene Expression"},{"location":"scrapbook/dge/#differential-gene-expression","text":"","title":"Differential Gene Expression"},{"location":"scrapbook/dge/#background","text":"Differential Gene Expression (DGE) is the process of determining whether any genes were expressed at a different level between two conditions. For example, the conditions could be wildtype versus mutant, or two growth conditions. Usually multiple biological replicates are done for each condition - these are needed to separate variation within the condition from that between the conditions.","title":"Background"},{"location":"scrapbook/dge/#learning-objectives","text":"At the end of this tutorial you should be able to: Align RNA-Seq data to a reference genome Count transcripts for each sample Perform statistical analysis to obtain a list of differentially expressed genes Interpret the DGE list Visualize the results in Degust","title":"Learning Objectives"},{"location":"scrapbook/dge/#input-data","text":"A typical experiment will have 2 conditions each with 3 replicates, for a total of 6 samples. Each sample will be RNA-Seq data , either as one file per sample (single-end reads / SE) or two files (paired-end reads / PE). Condition 1 Condition 2 Replicate 1 1 4 Replicate 2 2 5 Replicate 3 3 6 FIXME: load reads","title":"Input data"},{"location":"scrapbook/dge/#prepare-reference","text":"FIXME: tricky need it in correct format for htseq-count GFF2 ?","title":"Prepare reference"},{"location":"scrapbook/dge/#align-reads","text":"FIXME: use BWA MEM with defaults? for each sample, should be able to use the 6 at once feature of Galaxy?","title":"Align reads"},{"location":"scrapbook/dge/#count-reads","text":"FIXME: htseq-count, use the CDS and RNA features to align to, get count table","title":"Count reads"},{"location":"scrapbook/dge/#dge-analysis","text":"","title":"DGE Analysis"},{"location":"scrapbook/dge/#within-galaxy","text":"FIXME: Need to use Voom/Limma here","title":"Within Galaxy"},{"location":"scrapbook/dge/#within-degust","text":"FIXME: need to combine each of the results in count-reads section into a single table (using galaxy table tools?) but need to munge in the annotation as well, so i or simon will need to add new tools to toolshed to do this","title":"Within Degust"},{"location":"scrapbook/dge/#what-next","text":"FIXME","title":"What next?"},{"location":"scrapbook/dna/anno/","text":"Genome Annotation Genome annotation is the process of labelling genomic features onto a genomic sequence, such as the location of coding genes and RNAs. Command Line Prokka Galaxy Prokka Web RAST GENIX","title":"Genome Annotation"},{"location":"scrapbook/dna/anno/#genome-annotation","text":"Genome annotation is the process of labelling genomic features onto a genomic sequence, such as the location of coding genes and RNAs.","title":"Genome Annotation"},{"location":"scrapbook/dna/anno/#command-line","text":"Prokka","title":"Command Line"},{"location":"scrapbook/dna/anno/#galaxy","text":"Prokka","title":"Galaxy"},{"location":"scrapbook/dna/anno/#web","text":"RAST GENIX","title":"Web"},{"location":"scrapbook/dna/data/","text":"Genomics Data DNA-Seq FASTQ","title":"Genomics Data"},{"location":"scrapbook/dna/data/#genomics-data","text":"DNA-Seq FASTQ","title":"Genomics Data"},{"location":"scrapbook/dna/denovo/","text":"De novo genome assembly Illumina data Command line Velvet Spades MegaHIT Galaxy Spades Pacbio data Command line CANU SMRT Suite HGAP","title":"*De novo* genome assembly"},{"location":"scrapbook/dna/denovo/#de-novo-genome-assembly","text":"","title":"De novo genome assembly"},{"location":"scrapbook/dna/denovo/#illumina-data","text":"","title":"Illumina data"},{"location":"scrapbook/dna/denovo/#command-line","text":"Velvet Spades MegaHIT","title":"Command line"},{"location":"scrapbook/dna/denovo/#galaxy","text":"Spades","title":"Galaxy"},{"location":"scrapbook/dna/denovo/#pacbio-data","text":"","title":"Pacbio data"},{"location":"scrapbook/dna/denovo/#command-line_1","text":"CANU","title":"Command line"},{"location":"scrapbook/dna/denovo/#smrt-suite","text":"HGAP","title":"SMRT Suite"},{"location":"scrapbook/dna/snps/","text":"Variant Finding Variant finding the the process of identifying small differences between two genomes. Typically this refers to the small scale, such as SNPs and small insertions and deletions, as opposed to larger structural variation. Command Line Snippy Galaxy Varscan","title":"Variant Finding"},{"location":"scrapbook/dna/snps/#variant-finding","text":"Variant finding the the process of identifying small differences between two genomes. Typically this refers to the small scale, such as SNPs and small insertions and deletions, as opposed to larger structural variation.","title":"Variant Finding"},{"location":"scrapbook/dna/snps/#command-line","text":"Snippy","title":"Command Line"},{"location":"scrapbook/dna/snps/#galaxy","text":"Varscan","title":"Galaxy"},{"location":"scrapbook/dna/anno/eugene/","text":"EuGene-PP link: http://bioinformatics.oxfordjournals.org/content/30/18/2659.full can annotate using oriented RNA-seq data, so that the resulting annotation may include non-coding regions, including antisense ones. Pre-requisites mGVL Start Input data assembled genome oriented RNA-seq How it works -","title":"EuGene-PP"},{"location":"scrapbook/dna/anno/eugene/#eugene-pp","text":"link: http://bioinformatics.oxfordjournals.org/content/30/18/2659.full can annotate using oriented RNA-seq data, so that the resulting annotation may include non-coding regions, including antisense ones.","title":"EuGene-PP"},{"location":"scrapbook/dna/anno/eugene/#pre-requisites","text":"mGVL","title":"Pre-requisites"},{"location":"scrapbook/dna/anno/eugene/#start","text":"","title":"Start"},{"location":"scrapbook/dna/anno/eugene/#input-data","text":"assembled genome oriented RNA-seq","title":"Input data"},{"location":"scrapbook/dna/anno/eugene/#how-it-works","text":"-","title":"How it works"},{"location":"scrapbook/dna/anno/genix/","text":"GENIX this is a new website not galaxy","title":"GENIX"},{"location":"scrapbook/dna/anno/genix/#genix","text":"this is a new website not galaxy","title":"GENIX"},{"location":"scrapbook/dna/anno/megannotator/","text":"MEGAnnotator http://femsle.oxfordjournals.org/content/363/7/fnw049","title":"Megannotator"},{"location":"scrapbook/dna/anno/megannotator/#megannotator","text":"http://femsle.oxfordjournals.org/content/363/7/fnw049","title":"MEGAnnotator"},{"location":"scrapbook/dna/anno/prokka/","text":"Prokka on Galaxy Background Genome annotation involves finding and describing particular features, such as genes, tRNAs and rRNAs. This tutorial will demonstrate how to annotate an assembled bacterial genome using the tool Prokka. Link to Prokka on github ; link to Prokka citation . Learning objectives At the end of this tutorial you should be able to : input files into Prokka change settings run Prokka, and examine the output: annotated genome. Pre-requisites a mGVL and galaxy instance Start open your galaxy instance in your mGVL Input data assembled contigs, e.g. SPAdes_contigs.fasta How it works Prokka compares the input contigs with various databases to identify coding sequences, rRNA genes, tRNA genes, non-coding RNA, and signal leader peptides. These databases are maintained by different organisations, and include information about known genomic features and their locations. Prokka includes a local copy. Run Prokka In Galaxy, go to Tools NGS Analysis NGS: Annotation Prokka Set the following parameters (leave everything else unchanged): Contigs to annotate : SPAdes contigs (fasta) Locus tag prefix ( locustag) : P FIXME: actually we want to have a different locus tag for each sample for later use in Roary. Is there some problem here with the length of the locus tag that can be used? Force GenBank/ENA/DDJB compliance ( compliant) : Yes Sequencing Centre ID ( centre) : V Genus Name : Staphylococcus Species Name : aureus Use genus-specific BLAST database No Click Execute . This may take x minutes. Examine the output Once Prokka has finished, examine each of its output files. The gff and gbk files contains all of the information about all of the features annotated (in different formats.) summary.gff : a list of all the features found, listed in order of their location (starting at the start of contig number 1). Each row is a genomic feature and its location. Column 2 is the source - the database used to find the feature. Column 3 is the feature - e.g. CDS, tRNA. summary.gbk : the contigs listed in order. For each contig, the features are listed (e.g. CDS name and translation), followed by the sequence of the whole contig. The txt file contains a summary of the number of features annotated. The faa file contains the protein sequences of the genes annotated. The ffn file contains the nucleotide sequences of the genes annotated. Download the gff file to your local computer: click on the file name with the .gff extension, and then click on the disk icon . Annotated features Now that we have annotated the draft genome sequence, we would like to view the sequence in the Artemis genome viewer. Open Artemis and load the downloaded .gff file. The top panel shows an overview - here we can see annotated genes and other features. The middle panel shows the DNA sequence and amino acid translations in 6 frames. The bottom panel shows a text summary of the features. Scroll left and right with the horizontal bars under each panel. Zoom with the vertical bars to the right. What Next? Determine core and pan genomes using Roary .","title":"Prokka on Galaxy"},{"location":"scrapbook/dna/anno/prokka/#prokka-on-galaxy","text":"","title":"Prokka on Galaxy"},{"location":"scrapbook/dna/anno/prokka/#background","text":"Genome annotation involves finding and describing particular features, such as genes, tRNAs and rRNAs. This tutorial will demonstrate how to annotate an assembled bacterial genome using the tool Prokka. Link to Prokka on github ; link to Prokka citation .","title":"Background"},{"location":"scrapbook/dna/anno/prokka/#learning-objectives","text":"At the end of this tutorial you should be able to : input files into Prokka change settings run Prokka, and examine the output: annotated genome.","title":"Learning objectives"},{"location":"scrapbook/dna/anno/prokka/#pre-requisites","text":"a mGVL and galaxy instance","title":"Pre-requisites"},{"location":"scrapbook/dna/anno/prokka/#start","text":"open your galaxy instance in your mGVL","title":"Start"},{"location":"scrapbook/dna/anno/prokka/#input-data","text":"assembled contigs, e.g. SPAdes_contigs.fasta","title":"Input data"},{"location":"scrapbook/dna/anno/prokka/#how-it-works","text":"Prokka compares the input contigs with various databases to identify coding sequences, rRNA genes, tRNA genes, non-coding RNA, and signal leader peptides. These databases are maintained by different organisations, and include information about known genomic features and their locations. Prokka includes a local copy.","title":"How it works"},{"location":"scrapbook/dna/anno/prokka/#run-prokka","text":"In Galaxy, go to Tools NGS Analysis NGS: Annotation Prokka Set the following parameters (leave everything else unchanged): Contigs to annotate : SPAdes contigs (fasta) Locus tag prefix ( locustag) : P FIXME: actually we want to have a different locus tag for each sample for later use in Roary. Is there some problem here with the length of the locus tag that can be used? Force GenBank/ENA/DDJB compliance ( compliant) : Yes Sequencing Centre ID ( centre) : V Genus Name : Staphylococcus Species Name : aureus Use genus-specific BLAST database No Click Execute . This may take x minutes.","title":"Run Prokka"},{"location":"scrapbook/dna/anno/prokka/#examine-the-output","text":"Once Prokka has finished, examine each of its output files. The gff and gbk files contains all of the information about all of the features annotated (in different formats.) summary.gff : a list of all the features found, listed in order of their location (starting at the start of contig number 1). Each row is a genomic feature and its location. Column 2 is the source - the database used to find the feature. Column 3 is the feature - e.g. CDS, tRNA. summary.gbk : the contigs listed in order. For each contig, the features are listed (e.g. CDS name and translation), followed by the sequence of the whole contig. The txt file contains a summary of the number of features annotated. The faa file contains the protein sequences of the genes annotated. The ffn file contains the nucleotide sequences of the genes annotated. Download the gff file to your local computer: click on the file name with the .gff extension, and then click on the disk icon .","title":"Examine the output"},{"location":"scrapbook/dna/anno/prokka/#annotated-features","text":"Now that we have annotated the draft genome sequence, we would like to view the sequence in the Artemis genome viewer. Open Artemis and load the downloaded .gff file. The top panel shows an overview - here we can see annotated genes and other features. The middle panel shows the DNA sequence and amino acid translations in 6 frames. The bottom panel shows a text summary of the features. Scroll left and right with the horizontal bars under each panel. Zoom with the vertical bars to the right.","title":"Annotated features"},{"location":"scrapbook/dna/anno/prokka/#what-next","text":"Determine core and pan genomes using Roary .","title":"What Next?"},{"location":"scrapbook/dna/anno/prokka_cmdline/","text":"Prokka on commandline ssh to your mGVL wget data - scaffolds of .fna for several species (alternatively - have already put data on your mGVL) type in: prokka -- outdir [ name of output folder for 1 sample ] -- locustag [ tag eg sample number ] [ fna filename ] make a new directory for the gff files e.g. gff_files mv -v / .gff gff_files/ (means move any .gff files into that folder) FIXME: prokka options see manual","title":"Prokka on commandline"},{"location":"scrapbook/dna/anno/prokka_cmdline/#prokka-on-commandline","text":"ssh to your mGVL wget data - scaffolds of .fna for several species (alternatively - have already put data on your mGVL) type in: prokka -- outdir [ name of output folder for 1 sample ] -- locustag [ tag eg sample number ] [ fna filename ] make a new directory for the gff files e.g. gff_files mv -v / .gff gff_files/ (means move any .gff files into that folder) FIXME: prokka options see manual","title":"Prokka on commandline"},{"location":"scrapbook/dna/anno/rast/","text":"RAST website","title":"RAST"},{"location":"scrapbook/dna/anno/rast/#rast","text":"website","title":"RAST"},{"location":"scrapbook/dna/denovo/canu/","text":"CANU pacbio cmdline","title":"CANU"},{"location":"scrapbook/dna/denovo/canu/#canu","text":"pacbio cmdline","title":"CANU"},{"location":"scrapbook/dna/denovo/galaxy-spades/","text":"Assembly with Spades in Galaxy FIXME: This tutorial includes the Workshop 2a Assembly with Spades but also some extra info: section: Pre-requisites section: How does Spades work more detail on output files section: Filter output section: questions Background Spades is one of a number of de novo assemblers that use short read sets as input (e.g. Illumina Reads), and the assembly method is based on de Bruijn graphs. For information about Spades see this link . A protocol for assembling with Velvet (another de novo assembler) is available here . In this activity, we will perform a de novo assembly of a short read set (from an Illumina sequencer) using the Spades assembler. The output from Spades that we are interested in is a multifasta file that contains the draft genome sequence. The read set for today is from an imaginary Staphylococcus aureus bacterium with a miniature genome. We have a closed, annotated genome sequence for a closely related wildtype strain. The whole genome shotgun method used to sequence our mutant strain read set was produced on an Illumina DNA sequencing instrument. The reads are paired-end Each read is 150 bases (before trimming) The number of bases sequenced is equivalent to 19x the genome sequence of the wildtype strain. (Read coverage 19x - rather low!). Learning objectives At the end of this tutorial you should be able to: import data into Galaxy view the files evaluate the read quality assemble the reads using Spades, and examine the output assembly. Pre-requisites Galaxy de novo assembly QC Trimming Login to Galaxy Go to this Galaxy address: http://43.240.98.1/galaxy (FIXME: or alternative) Remind me how to logon. FIXME: note this contains the same galaxy address as above - change? Import data Click on the Analyze Data menu at the top of the page. Click on the History options button the on the top right of the history pane. Click Import from File (at the bottom of the list). A new page will appear with a text box for the URL of the history to import. Copy the following URL into the text box: http://43.240.98.1/public/dieter/Galaxy-History-Colombiaworkshopstart.tar.gz Click Submit . Galaxy will download the data files from the internet and will be available as an additional history (takes about one minute). To view this new history, click the View all histories button (top right of the history pane). If the history has finished downloading it will appear as imported from archive: Colombia_workshop_start Click on the button above the imported from archive:Colombia_workshop_start then the button. You should now have four files in the history pane as follows: View files All the files are text files. mutant_R1.fastq and mutant_R2.fastq : a paired-end read set wildtype.fna : a file that contains the genome sequence of the wildtype strain in fasta format (a header line, then the nucleotide sequence of the genome) wildtype.gff : a file that contains the genome sequence of the wildtype strain in general feature format. (a list of features - one feature per line, then the nucleotide sequence of the genome) Look at the contents of these files Click on the View Data button (the ) next to each of the files in turn. Brief Discussion about the GFF format (FIXME: add?) Evaluate the input reads Questions you might ask about your input reads include: How good is my read set? Do I need to ask for a new sequencing run? Is it suitable for the analysis I need to do? We will evaluate the input reads using the FastQC tool. This runs a standard series of tests on your read set and returns a relatively easy to interpret report. We will use the FASTQC tool in Galaxy to evaluate the quality of one of our fastq files. Go to Tools NGS:Analysis NGS: QC and Manipulation FastQC Select mutant_R1.fastq Execute Once finished, examine the output called FastQC on data1:webpage (Hint: ). It has a summary at the top of the page and a number of graphs. Some of the important outputs of FastQC for our purposes are: Basic Statistics: Sequence length : will be important in setting maximum k-mer size value for assembly Basic Statistics: Encoding : Quality encoding type: important for quality trimming software Basic Statistics: % GC : high GC organisms don\u2019t tend to assemble well and may have an uneven read coverage distribution. Basic Statistics: Total sequences : Total number of reads: gives you an idea of coverage. Per base sequence quality : Dips in quality near the beginning, middle or end of the reads: determines possible trimming/cleanup methods and parameters and may indicate technical problems with the sequencing process/machine run. Per base N content : Presence of large numbers of Ns in reads: may point to poor quality sequencing run. You would need to trim these reads to remove Ns. Kmer content : Presence of highly recurring k-mers: may point to contamination of reads with barcodes, adapter sequences etc. Although we have warnings for two outputs (per base sequence content; Kmer content), we can ignore these for now. For a fuller discussion of FastQC outputs and warnings, see the FastQC website link , including the section on each of the output reports , and examples of good and bad Illumina data. We won\u2019t be doing anything to these data to clean it up as there isn\u2019t much need. Therefore we will get on with the assembly! How does Spades work? As with several other de novo assembly programs (e.g. Velvet) Spades uses an algorithm based on de Bruijn graphs. Such graphs use sub-lengths of sequence reads to build an overall genome assembly. The span of the sub-length is called a k-mer, where k is the number of nucleotides (e.g. k=21). The user chooses three values of k and Spades makes three assemblies based on these. For the first value of k, each read is broken into as many fragments as possible. For example, if the input read is 22 nucleotides long, and the chosen value of k is 21, then there are two possible fragments (positions 1-21 and 2-22). One randomly-chosen fragment becomes the first node on the de Bruijn graph. A second fragment is connected to this node if it overlaps. Repeat until all fragments are connected. Output de Bruijn graph. Find a connected pathway through this graph. Output a pathway (sequence) known as a contig. Because of poor or incorrect sequencing, not all the fragments can be joined together. There will be several de Bruijn graphs and so several contigs, usually of different sizes. Repeat these steps for a further two values of k (e.g. k = 33, k = 55). Output Three (sets of) contigs. Merge the three (sets of) contigs to get one. Output one set of contigs. For paired-end reads (as in this tutorial), the two reads are sequenced from each end of a longer DNA fragment. The middle part of the fragment is not sequenced, but information about the distance between the reads can be used by Spades to join contigs into larger sequences, called scaffolds. Output one set of scaffolds. To fix any errors map the original sequence reads onto the scaffolds with the program BWA. Output assembled genome. Assemble reads with Spades We will perform a de novo assembly of the mutant fastq reads into long contiguous sequences (in fasta format.) Spades produces both contigs and scaffolds. Ask your demonstrator if you would like to know the difference between contigs and scaffolds. Go to Tools NGS Analysis NGS: Assembly spades Set the following parameters: Run only Assembly : Yes Kmers to use separated by commas: 33,55,91 no spaces Coverage cutoff: auto Files Forward reads: mutant_R1.fastq Files Reverse reads: mutant_R2.fastq Your tool interface should look like this: Click Execute Examine the output Galaxy is now running Spades on the reads for you. When it is finished, you will have five new files in your history. contig stats : There are x contigs. Look at the variation in length and coverage. A short contig with high coverage could be a result of contamination, a collapsed repeat, or a plasmid. contigs : Each contig is listed, followed by its sequence in fasta format. scaffold stats : There are x scaffolds. scaffolds : Each scaffold is listed, followed by its sequence in fasta format. log : The specific actions performed in the analysis. Click on the View Data button on each of the files. Note that the short reads have been assembled into much longer contigs. (However, in this case, the contigs have not been assembled into larger scaffolds.) The stats files will give you the length of each of the contigs. Filter output Go to Tools NGS Analysis NGS: Assembly Filter SPAdes output . This is a quick way to discard contigs that are too short (e.g., they might be contamination) or contigs that do not have enough coverage (e.g., they might be too unreliable). Under Sequences , choose the contigs fasta file. Under Contig stats choose the contigs stats file. Change the cut-off values for length and coverage or leave them as they are. For Save filtered-out sequences? click Yes . Click Execute . A new fasta file with only the filtered sequences will be saved in the right-side history pane. Questions How does SPAdes differ from other genome assembly programs? It uses multiple values of k in de Bruijn graphs. Larger fragment sizes will more accurately position sections of duplicated DNA (repeats), but these larger fragments will only overlap well in densely-sequenced (high-coverage) areas of the genome. Because bacterial genomes may have low-coverage regions, using smaller fragments can increase the potential for overlaps (joins) in these low-coverage regions. Using a range of fragment sizes will therefore get the benefit from both approaches. More information [here](http://thegenomefactory.blogspot.com.au/2013/08/how-spades-differs-from-velvet.html>). How do I choose values of k? The k values need to be odd numbers, and shorter than the read lengths. A good strategy could be to choose some that are small, medium and large. e.g. if the read is 150 nucleotides, k values could be 33, 55, 91. There is no absolute rule; rather, the aim is to get a good spread of k values across the read length. What can I do with my assembled genome? This tutorial used a subset of a real dataset, so is not a complete genome (is it?). You could re-try it with short reads from a whole genome, at NCBI SRA. You can [annotate] (describe) the genomic features such as genes or [compare] it to other genomes to see variation in structure. What Next? Annotate the genome, e.g. with Prokka .","title":"Assembly with Spades in Galaxy"},{"location":"scrapbook/dna/denovo/galaxy-spades/#assembly-with-spades-in-galaxy","text":"FIXME: This tutorial includes the Workshop 2a Assembly with Spades but also some extra info: section: Pre-requisites section: How does Spades work more detail on output files section: Filter output section: questions","title":"Assembly with Spades in Galaxy"},{"location":"scrapbook/dna/denovo/galaxy-spades/#background","text":"Spades is one of a number of de novo assemblers that use short read sets as input (e.g. Illumina Reads), and the assembly method is based on de Bruijn graphs. For information about Spades see this link . A protocol for assembling with Velvet (another de novo assembler) is available here . In this activity, we will perform a de novo assembly of a short read set (from an Illumina sequencer) using the Spades assembler. The output from Spades that we are interested in is a multifasta file that contains the draft genome sequence. The read set for today is from an imaginary Staphylococcus aureus bacterium with a miniature genome. We have a closed, annotated genome sequence for a closely related wildtype strain. The whole genome shotgun method used to sequence our mutant strain read set was produced on an Illumina DNA sequencing instrument. The reads are paired-end Each read is 150 bases (before trimming) The number of bases sequenced is equivalent to 19x the genome sequence of the wildtype strain. (Read coverage 19x - rather low!).","title":"Background"},{"location":"scrapbook/dna/denovo/galaxy-spades/#learning-objectives","text":"At the end of this tutorial you should be able to: import data into Galaxy view the files evaluate the read quality assemble the reads using Spades, and examine the output assembly.","title":"Learning objectives"},{"location":"scrapbook/dna/denovo/galaxy-spades/#pre-requisites","text":"Galaxy de novo assembly QC Trimming","title":"Pre-requisites"},{"location":"scrapbook/dna/denovo/galaxy-spades/#login-to-galaxy","text":"Go to this Galaxy address: http://43.240.98.1/galaxy (FIXME: or alternative) Remind me how to logon. FIXME: note this contains the same galaxy address as above - change?","title":"Login to Galaxy"},{"location":"scrapbook/dna/denovo/galaxy-spades/#import-data","text":"Click on the Analyze Data menu at the top of the page. Click on the History options button the on the top right of the history pane. Click Import from File (at the bottom of the list). A new page will appear with a text box for the URL of the history to import. Copy the following URL into the text box: http://43.240.98.1/public/dieter/Galaxy-History-Colombiaworkshopstart.tar.gz Click Submit . Galaxy will download the data files from the internet and will be available as an additional history (takes about one minute). To view this new history, click the View all histories button (top right of the history pane). If the history has finished downloading it will appear as imported from archive: Colombia_workshop_start Click on the button above the imported from archive:Colombia_workshop_start then the button. You should now have four files in the history pane as follows:","title":"Import data"},{"location":"scrapbook/dna/denovo/galaxy-spades/#view-files","text":"All the files are text files. mutant_R1.fastq and mutant_R2.fastq : a paired-end read set wildtype.fna : a file that contains the genome sequence of the wildtype strain in fasta format (a header line, then the nucleotide sequence of the genome) wildtype.gff : a file that contains the genome sequence of the wildtype strain in general feature format. (a list of features - one feature per line, then the nucleotide sequence of the genome) Look at the contents of these files Click on the View Data button (the ) next to each of the files in turn. Brief Discussion about the GFF format (FIXME: add?)","title":"View files"},{"location":"scrapbook/dna/denovo/galaxy-spades/#evaluate-the-input-reads","text":"Questions you might ask about your input reads include: How good is my read set? Do I need to ask for a new sequencing run? Is it suitable for the analysis I need to do? We will evaluate the input reads using the FastQC tool. This runs a standard series of tests on your read set and returns a relatively easy to interpret report. We will use the FASTQC tool in Galaxy to evaluate the quality of one of our fastq files. Go to Tools NGS:Analysis NGS: QC and Manipulation FastQC Select mutant_R1.fastq Execute Once finished, examine the output called FastQC on data1:webpage (Hint: ). It has a summary at the top of the page and a number of graphs. Some of the important outputs of FastQC for our purposes are: Basic Statistics: Sequence length : will be important in setting maximum k-mer size value for assembly Basic Statistics: Encoding : Quality encoding type: important for quality trimming software Basic Statistics: % GC : high GC organisms don\u2019t tend to assemble well and may have an uneven read coverage distribution. Basic Statistics: Total sequences : Total number of reads: gives you an idea of coverage. Per base sequence quality : Dips in quality near the beginning, middle or end of the reads: determines possible trimming/cleanup methods and parameters and may indicate technical problems with the sequencing process/machine run. Per base N content : Presence of large numbers of Ns in reads: may point to poor quality sequencing run. You would need to trim these reads to remove Ns. Kmer content : Presence of highly recurring k-mers: may point to contamination of reads with barcodes, adapter sequences etc. Although we have warnings for two outputs (per base sequence content; Kmer content), we can ignore these for now. For a fuller discussion of FastQC outputs and warnings, see the FastQC website link , including the section on each of the output reports , and examples of good and bad Illumina data. We won\u2019t be doing anything to these data to clean it up as there isn\u2019t much need. Therefore we will get on with the assembly!","title":"Evaluate the input reads"},{"location":"scrapbook/dna/denovo/galaxy-spades/#how-does-spades-work","text":"As with several other de novo assembly programs (e.g. Velvet) Spades uses an algorithm based on de Bruijn graphs. Such graphs use sub-lengths of sequence reads to build an overall genome assembly. The span of the sub-length is called a k-mer, where k is the number of nucleotides (e.g. k=21). The user chooses three values of k and Spades makes three assemblies based on these. For the first value of k, each read is broken into as many fragments as possible. For example, if the input read is 22 nucleotides long, and the chosen value of k is 21, then there are two possible fragments (positions 1-21 and 2-22). One randomly-chosen fragment becomes the first node on the de Bruijn graph. A second fragment is connected to this node if it overlaps. Repeat until all fragments are connected. Output de Bruijn graph. Find a connected pathway through this graph. Output a pathway (sequence) known as a contig. Because of poor or incorrect sequencing, not all the fragments can be joined together. There will be several de Bruijn graphs and so several contigs, usually of different sizes. Repeat these steps for a further two values of k (e.g. k = 33, k = 55). Output Three (sets of) contigs. Merge the three (sets of) contigs to get one. Output one set of contigs. For paired-end reads (as in this tutorial), the two reads are sequenced from each end of a longer DNA fragment. The middle part of the fragment is not sequenced, but information about the distance between the reads can be used by Spades to join contigs into larger sequences, called scaffolds. Output one set of scaffolds. To fix any errors map the original sequence reads onto the scaffolds with the program BWA. Output assembled genome.","title":"How does Spades work?"},{"location":"scrapbook/dna/denovo/galaxy-spades/#assemble-reads-with-spades","text":"We will perform a de novo assembly of the mutant fastq reads into long contiguous sequences (in fasta format.) Spades produces both contigs and scaffolds. Ask your demonstrator if you would like to know the difference between contigs and scaffolds. Go to Tools NGS Analysis NGS: Assembly spades Set the following parameters: Run only Assembly : Yes Kmers to use separated by commas: 33,55,91 no spaces Coverage cutoff: auto Files Forward reads: mutant_R1.fastq Files Reverse reads: mutant_R2.fastq Your tool interface should look like this: Click Execute","title":"Assemble reads with Spades"},{"location":"scrapbook/dna/denovo/galaxy-spades/#examine-the-output","text":"Galaxy is now running Spades on the reads for you. When it is finished, you will have five new files in your history. contig stats : There are x contigs. Look at the variation in length and coverage. A short contig with high coverage could be a result of contamination, a collapsed repeat, or a plasmid. contigs : Each contig is listed, followed by its sequence in fasta format. scaffold stats : There are x scaffolds. scaffolds : Each scaffold is listed, followed by its sequence in fasta format. log : The specific actions performed in the analysis. Click on the View Data button on each of the files. Note that the short reads have been assembled into much longer contigs. (However, in this case, the contigs have not been assembled into larger scaffolds.) The stats files will give you the length of each of the contigs.","title":"Examine the output"},{"location":"scrapbook/dna/denovo/galaxy-spades/#filter-output","text":"Go to Tools NGS Analysis NGS: Assembly Filter SPAdes output . This is a quick way to discard contigs that are too short (e.g., they might be contamination) or contigs that do not have enough coverage (e.g., they might be too unreliable). Under Sequences , choose the contigs fasta file. Under Contig stats choose the contigs stats file. Change the cut-off values for length and coverage or leave them as they are. For Save filtered-out sequences? click Yes . Click Execute . A new fasta file with only the filtered sequences will be saved in the right-side history pane.","title":"Filter output"},{"location":"scrapbook/dna/denovo/galaxy-spades/#questions","text":"How does SPAdes differ from other genome assembly programs? It uses multiple values of k in de Bruijn graphs. Larger fragment sizes will more accurately position sections of duplicated DNA (repeats), but these larger fragments will only overlap well in densely-sequenced (high-coverage) areas of the genome. Because bacterial genomes may have low-coverage regions, using smaller fragments can increase the potential for overlaps (joins) in these low-coverage regions. Using a range of fragment sizes will therefore get the benefit from both approaches. More information [here](http://thegenomefactory.blogspot.com.au/2013/08/how-spades-differs-from-velvet.html>). How do I choose values of k? The k values need to be odd numbers, and shorter than the read lengths. A good strategy could be to choose some that are small, medium and large. e.g. if the read is 150 nucleotides, k values could be 33, 55, 91. There is no absolute rule; rather, the aim is to get a good spread of k values across the read length. What can I do with my assembled genome? This tutorial used a subset of a real dataset, so is not a complete genome (is it?). You could re-try it with short reads from a whole genome, at NCBI SRA. You can [annotate] (describe) the genomic features such as genes or [compare] it to other genomes to see variation in structure.","title":"Questions"},{"location":"scrapbook/dna/denovo/galaxy-spades/#what-next","text":"Annotate the genome, e.g. with Prokka .","title":"What Next?"},{"location":"scrapbook/dna/denovo/hgap/","text":"HGAP this means SMRT Suite Assembly with PacBio data PacBio is a company that uses a technology called single molecule real time (SMRT) sequencing. This produces very long sequencing reads (up to xx?). These raw sequencing reads can be assembled into genomes by using the software from PacBio, called the SMRT Portal. link: http://www.pacb.com/smrt-science/ link: http://www.pacb.com/products-and-services/analytical-software/smrt-analysis/ info: https://github.com/PacificBiosciences/Bioinformatics-Training/wiki/Large-Genome-Assembly-with-PacBio-Long-Reads Background: assembly galaxy cmdline? Start Open your GVL dashboard. Go to Admin. There is a list of packages. Find SMRT Analysis. On the right, click on Install . (Note: you will need 16 cores in your GVL). open up the link and log on (create a username and password I think) FIXME Sepsis Data e.g. https://downloads-qcif.bioplatforms.com/bpa/sepsis/pacbio/ e.g. one sample it will be split into three files ? for each sample: need the bas file? and the xml but not the subreads bas.h5 Reference Guide (PDF): Describes the main output files produced by the primary analysis pipeline: bas.h5,.1.bax.h5, .2.bax.h5, and .3.bax.h5. The bax.h5 files contain base call information from the sequencing run. The bas.h5 file is essentially a pointer to the three bax.h5 files. Metadata Output Guide (PDF): Describes the file metadata.xml, which contains top-level information about the data, including what sequencing enzyme and chemistry were used, sample name, and other metadata. which files to put over into sepsis gvl Input data from here [link] Import and Manage Import SMRT cells: SMRT Cells shows file paths that are searched add (so should have made folder, put smrt cells data in there, and then tell it this path using add ) How it works The PacBio SMRT analysis software: de novo assembly. link: http://www.pacb.com/products-and-services/analytical-software/smrt-analysis/analysis-applications/de-novo-assembly/ Several options, but for example HGAP + BridgeMapper: RS_HGAP Assembly.3 pre-assembly de novo assembly with AssembleUnitig finalise assembly with Quiver RS_Bridgemapper assesses assembly quality by comparing to a reference genome. Run give job name and comments choose a protocol click on the data you want (how to get in) and then arrow to transfer it to right pane run click on the monitor tab as it runs, new items will appear on the left hand side under reports, e.g. starting with filtering graphs will appear for some items, e.g. mapped subread length, what does it mean new data sets will appear under on the left hand side under data eg polished assembly fastq. click on top right corner - log (to check for anything?) Output when finished, go to view data tab; click on job name, open (what is SMRT view? another option) shows all reports and data files e.g. assembled genome download any or leave in here? main assembly parameters /reports - what to check how do you know assembly is good https://github.com/PacificBiosciences/Bioinformatics-Training/wiki/Finishing-Bacterial-Genomes good detail in here Next Links to more information","title":"HGAP"},{"location":"scrapbook/dna/denovo/hgap/#hgap","text":"this means SMRT Suite Assembly with PacBio data PacBio is a company that uses a technology called single molecule real time (SMRT) sequencing. This produces very long sequencing reads (up to xx?). These raw sequencing reads can be assembled into genomes by using the software from PacBio, called the SMRT Portal. link: http://www.pacb.com/smrt-science/ link: http://www.pacb.com/products-and-services/analytical-software/smrt-analysis/ info: https://github.com/PacificBiosciences/Bioinformatics-Training/wiki/Large-Genome-Assembly-with-PacBio-Long-Reads Background: assembly galaxy cmdline? Start Open your GVL dashboard. Go to Admin. There is a list of packages. Find SMRT Analysis. On the right, click on Install . (Note: you will need 16 cores in your GVL). open up the link and log on (create a username and password I think) FIXME Sepsis Data e.g. https://downloads-qcif.bioplatforms.com/bpa/sepsis/pacbio/ e.g. one sample it will be split into three files ? for each sample: need the bas file? and the xml but not the subreads bas.h5 Reference Guide (PDF): Describes the main output files produced by the primary analysis pipeline: bas.h5,.1.bax.h5, .2.bax.h5, and .3.bax.h5. The bax.h5 files contain base call information from the sequencing run. The bas.h5 file is essentially a pointer to the three bax.h5 files. Metadata Output Guide (PDF): Describes the file metadata.xml, which contains top-level information about the data, including what sequencing enzyme and chemistry were used, sample name, and other metadata. which files to put over into sepsis gvl Input data from here [link] Import and Manage Import SMRT cells: SMRT Cells shows file paths that are searched add (so should have made folder, put smrt cells data in there, and then tell it this path using add ) How it works The PacBio SMRT analysis software: de novo assembly. link: http://www.pacb.com/products-and-services/analytical-software/smrt-analysis/analysis-applications/de-novo-assembly/ Several options, but for example HGAP + BridgeMapper: RS_HGAP Assembly.3 pre-assembly de novo assembly with AssembleUnitig finalise assembly with Quiver RS_Bridgemapper assesses assembly quality by comparing to a reference genome. Run give job name and comments choose a protocol click on the data you want (how to get in) and then arrow to transfer it to right pane run click on the monitor tab as it runs, new items will appear on the left hand side under reports, e.g. starting with filtering graphs will appear for some items, e.g. mapped subread length, what does it mean new data sets will appear under on the left hand side under data eg polished assembly fastq. click on top right corner - log (to check for anything?) Output when finished, go to view data tab; click on job name, open (what is SMRT view? another option) shows all reports and data files e.g. assembled genome download any or leave in here? main assembly parameters /reports - what to check how do you know assembly is good https://github.com/PacificBiosciences/Bioinformatics-Training/wiki/Finishing-Bacterial-Genomes good detail in here Next Links to more information","title":"HGAP"},{"location":"scrapbook/dna/denovo/megahit/","text":"Megahit cmdline This tutorial demonstrates how to assemble genomic reads from bacteria found in environmental samples. [FIXME: is that correct?] Megahit on Github Pre-requisites mGVL instance knowledge: cmdline knowledge: metagenomics Start open your mGVL in terminal make a folder for the Megahit analsyses: mkdir Megahit move into this folder: cd Megahit Input data We will use Illumina paired-end reads from several species [FIXME?] Run Megahit megahit [ options ] R1reads.fastq R2reads.fastq Output Next More information","title":"Megahit"},{"location":"scrapbook/dna/denovo/megahit/#megahit","text":"cmdline This tutorial demonstrates how to assemble genomic reads from bacteria found in environmental samples. [FIXME: is that correct?] Megahit on Github","title":"Megahit"},{"location":"scrapbook/dna/denovo/megahit/#pre-requisites","text":"mGVL instance knowledge: cmdline knowledge: metagenomics","title":"Pre-requisites"},{"location":"scrapbook/dna/denovo/megahit/#start","text":"open your mGVL in terminal make a folder for the Megahit analsyses: mkdir Megahit move into this folder: cd Megahit","title":"Start"},{"location":"scrapbook/dna/denovo/megahit/#input-data","text":"We will use Illumina paired-end reads from several species [FIXME?]","title":"Input data"},{"location":"scrapbook/dna/denovo/megahit/#run-megahit","text":"megahit [ options ] R1reads.fastq R2reads.fastq","title":"Run Megahit"},{"location":"scrapbook/dna/denovo/megahit/#output","text":"","title":"Output"},{"location":"scrapbook/dna/denovo/megahit/#next","text":"","title":"Next"},{"location":"scrapbook/dna/denovo/megahit/#more-information","text":"","title":"More information"},{"location":"scrapbook/dna/denovo/pacbio/","text":"PacBio SMRT Portal This tutorial will show you how to assemble a bacterial genome de novo, using the PacBio SMRT Portal. Link to PacBio analysis software Pre-requisites mGVL instance with 16 cores (FIXME: will people have this - if not, note how they can request it) knowledge: de novo assembly Start Open your mGVL dashboard. Go to Admin. There is a list of packages. Find SMRT Analysis. On the right, click on Install . You should see SMRT Portal as one of the instance services on your GVL dashboard. Open up the SMRT portal web link (to the right) and register/log on. How it works RS_HGAP_Assembly.3 Protocol Filters short and poor-quality reads. Large insert (fragment) sizes = single pass long reads (but these have lower quality) and/or small insert (fragment) sizes = cut adapters and assemble subreads = Circular Consensus Sequence (CSS) reads (higher quality) Pre-assembly e.g. with Canu Polishes assembly e.g. with Quiver (joins contigs/scaffolds?) Corrections Input Choose your data. (FIXME: e.g. on GenomeSpace?) Get the data you want to use onto your mGVL. (FIXME: explain how?) In the SMRT Portal, go to Design Job , the top left tab. Go to Import and Manage: Import SMRT cells: SMRT Cells . Work out where you put the data on your GVL, and make sure the file path is showing. If not, click Add and list the file path to the data. Click on the file path and then Scan to check for new data. Run Go back to the top tab Design Job . Go to Create New . An Analysis window should appear. Check the box next to De novo assembly , then Next . Under Job Name enter a name. Under Protocols choose RS_HGAP_Assembly.3 . There is an ellipsis underneath Protocols - click on the ellipsis. This brings up the settings. Leave everything as is, except for: Click on Assembly . Change the Genome Size to an approximately correct size for the sample. Click Ok . In the SMRT Cells Available window, select the file to be used. Click on the arrow to transfer these files to the SMRT Cells in Job window. Click Save . Next to Save , click Start . The Monitor Jobs window should open. As each step proceeds, new items will appear under the Reports and Data tabs on the left. Click on each of these items to see the details and graphs available, which will appear in the main pane. The default display in the main pane is Overview . FIXME: how long will it take for this example data. Output The current running jobs will be under the Monitor Jobs tab. Click on the job to see the reports and data. The finished jobs will be under the View Data tab. A full ist of reports and terminology is here Reports: General: Filtering : look at the table showing the pre-filter and post-filter information. FIXME: what are other graphs showing / why are there mapping/coverage graphs if a ref genome wasn t supplied? FIXME: what are the files under Data for - further analyses later? where would these be saved if we want to use later. BAM and BAI files: view reads aligned to assembly (IGV) Next Check assembly quality: for example, align to the same species in Mauve. Links to more information: Finishing bacterial genomes","title":"PacBio SMRT Portal"},{"location":"scrapbook/dna/denovo/pacbio/#pacbio-smrt-portal","text":"This tutorial will show you how to assemble a bacterial genome de novo, using the PacBio SMRT Portal. Link to PacBio analysis software","title":"PacBio SMRT Portal"},{"location":"scrapbook/dna/denovo/pacbio/#pre-requisites","text":"mGVL instance with 16 cores (FIXME: will people have this - if not, note how they can request it) knowledge: de novo assembly","title":"Pre-requisites"},{"location":"scrapbook/dna/denovo/pacbio/#start","text":"Open your mGVL dashboard. Go to Admin. There is a list of packages. Find SMRT Analysis. On the right, click on Install . You should see SMRT Portal as one of the instance services on your GVL dashboard. Open up the SMRT portal web link (to the right) and register/log on.","title":"Start"},{"location":"scrapbook/dna/denovo/pacbio/#how-it-works","text":"RS_HGAP_Assembly.3 Protocol Filters short and poor-quality reads. Large insert (fragment) sizes = single pass long reads (but these have lower quality) and/or small insert (fragment) sizes = cut adapters and assemble subreads = Circular Consensus Sequence (CSS) reads (higher quality) Pre-assembly e.g. with Canu Polishes assembly e.g. with Quiver (joins contigs/scaffolds?) Corrections","title":"How it works"},{"location":"scrapbook/dna/denovo/pacbio/#input","text":"Choose your data. (FIXME: e.g. on GenomeSpace?) Get the data you want to use onto your mGVL. (FIXME: explain how?) In the SMRT Portal, go to Design Job , the top left tab. Go to Import and Manage: Import SMRT cells: SMRT Cells . Work out where you put the data on your GVL, and make sure the file path is showing. If not, click Add and list the file path to the data. Click on the file path and then Scan to check for new data.","title":"Input"},{"location":"scrapbook/dna/denovo/pacbio/#run","text":"Go back to the top tab Design Job . Go to Create New . An Analysis window should appear. Check the box next to De novo assembly , then Next . Under Job Name enter a name. Under Protocols choose RS_HGAP_Assembly.3 . There is an ellipsis underneath Protocols - click on the ellipsis. This brings up the settings. Leave everything as is, except for: Click on Assembly . Change the Genome Size to an approximately correct size for the sample. Click Ok . In the SMRT Cells Available window, select the file to be used. Click on the arrow to transfer these files to the SMRT Cells in Job window. Click Save . Next to Save , click Start . The Monitor Jobs window should open. As each step proceeds, new items will appear under the Reports and Data tabs on the left. Click on each of these items to see the details and graphs available, which will appear in the main pane. The default display in the main pane is Overview . FIXME: how long will it take for this example data.","title":"Run"},{"location":"scrapbook/dna/denovo/pacbio/#output","text":"The current running jobs will be under the Monitor Jobs tab. Click on the job to see the reports and data. The finished jobs will be under the View Data tab. A full ist of reports and terminology is here Reports: General: Filtering : look at the table showing the pre-filter and post-filter information. FIXME: what are other graphs showing / why are there mapping/coverage graphs if a ref genome wasn t supplied? FIXME: what are the files under Data for - further analyses later? where would these be saved if we want to use later. BAM and BAI files: view reads aligned to assembly (IGV)","title":"Output"},{"location":"scrapbook/dna/denovo/pacbio/#next","text":"Check assembly quality: for example, align to the same species in Mauve.","title":"Next"},{"location":"scrapbook/dna/denovo/pacbio/#links-to-more-information","text":"Finishing bacterial genomes","title":"Links to more information:"},{"location":"scrapbook/dna/denovo/spades/","text":"Spades - command line FIXME: add general info about spades (copy from galaxy-spades) Start open your mGVL (e.g. via ssh in terminal) check spades is installed: which spades the path to spades should appear FIXME: it s already loaded? seems to work make a new directory for these analyses: mkdir spades copy the read files into your mGVL e.g. R1.fastq and R2.fastq (e.g. download from the galaxy workshops) in terminal, navigate to the folder containing these files scp file names user@mGVL:/path to put data Run spades spades.py -1 R1.fastq -2 R2.fastq -k 33 ,55,77 -o output Output Go to the folder called output contigs.fasta scaffolds.fasta assembly_graph.fastg subfolder: corrected reads [FIXME explain] What next? view assembly_graph.fasta in Bandage generate summary statistics with Quast","title":"Spades - command line"},{"location":"scrapbook/dna/denovo/spades/#spades-command-line","text":"FIXME: add general info about spades (copy from galaxy-spades)","title":"Spades - command line"},{"location":"scrapbook/dna/denovo/spades/#start","text":"open your mGVL (e.g. via ssh in terminal) check spades is installed: which spades the path to spades should appear FIXME: it s already loaded? seems to work make a new directory for these analyses: mkdir spades copy the read files into your mGVL e.g. R1.fastq and R2.fastq (e.g. download from the galaxy workshops) in terminal, navigate to the folder containing these files scp file names user@mGVL:/path to put data","title":"Start"},{"location":"scrapbook/dna/denovo/spades/#run-spades","text":"spades.py -1 R1.fastq -2 R2.fastq -k 33 ,55,77 -o output","title":"Run spades"},{"location":"scrapbook/dna/denovo/spades/#output","text":"Go to the folder called output contigs.fasta scaffolds.fasta assembly_graph.fastg subfolder: corrected reads [FIXME explain]","title":"Output"},{"location":"scrapbook/dna/denovo/spades/#what-next","text":"view assembly_graph.fasta in Bandage generate summary statistics with Quast","title":"What next?"},{"location":"scrapbook/dna/denovo/velvet/","text":"Velvet cmdline","title":"Velvet"},{"location":"scrapbook/dna/denovo/velvet/#velvet","text":"cmdline","title":"Velvet"},{"location":"scrapbook/dna/denovo/vopt/","text":"Velvet cmdline","title":"Velvet"},{"location":"scrapbook/dna/denovo/vopt/#velvet","text":"cmdline","title":"Velvet"},{"location":"scrapbook/dna/denovo/galaxy-velvet/galaxy-velvet/","text":"Galaxy - Velvet Modified from LSCC tutorial by Simon Gladman - VLSCI Tutorial Overview In this tutorial we cover the concepts of Microbial de novo assembly using a very small synthetic dataset from a well studied organism. Background Where is the data in this tutorial from? The data for this tutorial is from a whole genome sequencing experiment of a multi-drug resistant strain of the bacterium Staphylococcus aureus . The DNA was sequenced using an Illumina GAII sequencing machine. The data we are going to use consists of about 4 million x 75 base-pair, paired end reads (two FASTQ read files, one for each end of a DNA fragment.) The data was downloaded from the NCBI Short Read Archive (SRA) (http://www.ncbi.nlm.nih.gov/sra/). The specific sample is a public dataset published in April 2012 with SRA accession number ERR048396. We will also use a FASTA file containing the sequences of the Illumina adapters used in the sequencing process. It is desirable to remove these as they are artificial sequences and not part of the bacterium that was sequenced. We will use software called Velvet (Zerbino et al 2008) for the main de novo assembly, as well as some other peripheral software for pre- and post-processing of the data. Details of these can be found in the background document linked above. The protocol: We are performing a de novo assembly of the read data into contigs and then into scaffolds (appropriately positioned contigs loosely linked together). We firstly need to check the quality of the input data as this will help us choose the most appropriate range of input parameters for the assembly and will guide us on an appropriate quality trimming/cleanup strategy. We will then use an iterative method to assemble the reads using the Velvet Optimiser (a program that performs lots of Velvet assemblies searching for an optimum outcome.) Once this is complete we will obtain summary statistics on the final results (contigs) of the assembly. More information about this protocol at the end of this tutorial. The protocol in a nutshell: Input: Raw reads from sequencer run on microbial DNA sample. Output: File of assembled scaffolds/contigs and associated information. Input data On the Galaxy tools panel, click on Get data - Upload File . Click on the Paste/Fetch Data button. Paste the URL: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Assembly/ERR048396_1.fastq.gz into the text box. Change the type to fastqsanger (Not fastqcsanger ). Click on the Paste/Fetch Data button again. Paste the URL: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Assembly/ERR048396_2.fastq.gz into the text box and change it s type to fastqsanger as well. Repeat the process for the last URL: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Assembly/illumina_adapters.fna , but make it s type fasta Click on the Start button. Once all of the uploads are at 100%, click on the Close button. When the files have finished uploading, rename them to \u2018ERR048396_1.fastq\u2019, \u2018ERR048396_2.fastq\u2019 and \u2018illumina_adapters.fna\u2019 respectively by clicking on the icon to the top right of the file name in the right hand Galaxy panel (the history panel) You should now have the following files in your Galaxy history: ERR048396_1.fastq - forward reads in fastq format ERR048396_2.fastq - reverse reads in fastq format illumina_adapters.fa - Illumina adapter sequences in fasta format Click on the icon to the top right of each fastq file to view the first part of the file Section 1: Quality control The basic process here is to collect statistics about the quality of the reads in the sample FASTQ readsets. We will then evaluate their quality and choose an appropriate regime for quality filtering using Trimmomatic (a FASTQ read quality trimmer.) Run FastQC on both input read files From the tools menu in the left hand panel of Galaxy, select NGS QC and manipulation FastQC: Comprehensive QC (down the bottom of this category) and run with these parameters: FASTQ reads : ERR048396_1.fastq Use default for other fields Click Execute Now repeat the above process on the second read file: ERR048396_2.fastq It is important to do both read files as the quality can be very different between them. Figure 1: Screenshot of FastQC interface in Galaxy Examine the FastQC output You should have two output objects from the first step: FastQC_ERR048396_1.fastqc.html FastQC_ERR048396_2.fastqc.html These are a html outputs which show the results of all of the tests FastQC performed on the read files. Click on the icon of each of these objects in turn to see the FastQC output. The main parts of the output to evaluate are: Basic statistics. This section tells us that the ASCII quality encoding format used was Sanger/Illumina 1.9 and the reads are length 75 and the percent GC content of the entire file is 35%. Per base sequence quality. In the plot you should see that most of the early bases are up around the 32 mark and then increase to 38-40, which is very high quality; The spread of quality values for the last few bases increases and some of the outliers have quality scores of less than 30. This is a very good quality dataset. 20 is often used as a cutoff for reliable quality. Figure 2: Screenshot of FastQC output in Galaxy Quality trim the reads using Trimmomatic. From the tools menu in the left hand panel of Galaxy, select NGS QC and manipulation Trimmomatic and run with these parameters (only the non-default selections are listed here): Input FASTQ file (R1/first of pair) : ERR048396_1.fastq Input FASTQ file (R2/second of pair) : ERR048396_2.fastq Perform initial ILLUMINACLIP step? : Yes Adapter sequences to use : TruSeq3 (additional seqs) (paired end, for MiSeq and HiSeq) How accurate read alignment : 40 How accurate against a read : 15 We will use the default settings for the SLIDING_WINDOW operation but we need to add a few more Trimmomatic operations. Click Insert Trimmomatic Operation Add Cut bases (LEADING) Minimum quality required to keep a base : 15 Repeat the Insert Trimmomatic Operation for: Trim trailing bases, minimum quality: 15 Minimum length read: 35 Click Execute Figure 3: Screenshot of Trimmomatic inputs in Galaxy Examine the Trimmomatic output FastQ files. You should have 4 new objects in your history from the output of Trimmomatic: Trimmomatic on data 2 and data 1 (R1 Paired) Trimmomatic on data 2 and data 1 (R1 Unpaired) Trimmomatic on data 2 and data 1 (R2 Paired) Trimmomatic on data 2 and data 1 (R2 Unpaired) Click on the on one of the objects to look at its contents. You\u2019ll notice that not all of the reads are the same length now, as they have had the illumina adapters cut out of them and they\u2019ve been quality trimmed. Section 2: Assemble reads into contigs with Velvet and the Velvet Optimiser The aim here is to assemble the trimmed reads into contigs/scaffolds using Velvet and the Velvet Optimiser. We will use a single tool, Velvet Optimiser, which takes the trimmed reads from Trimmomatic and performs numerous Velvet assemblies to find the best one. We need to add the reads in two separate libraries. One for the still paired reads and the other for the singleton reads orphaned from their pairs by the trimming process. Click here for a more detailed explanation of Velvet assemblies and the Velvet Optimiser De novo assembly of the reads into contigs From the tools menu in the left hand panel of Galaxy, select NGS: Assembly - Velvet Optimiser and run with these parameters (only the non-default selections are listed here): Start k-mer value : 55 End k-mer value : 69 In the input files section: Select first set of reads : Trimmomatic on data 2 and data 1 (R1 paired) Select second set of reads : Trimmomatic on data 2 and data 1 (R2 paired) Click the Insert Input Files button and add the following: Single or paired end reads : Single Select the reads : Trimmomatic on data 2 and data 1 (R1 unpaired) Repeat the above process to add the other unpaired read set Trimmomatic on data 2 and data 1 (R2 unpaired) as well. Click Execute . Figure 4: Screenshot of Velvet Optimiser inputs in Galaxy Examine assembly output Once step 1 is complete, you should now have 2 new objects in your history: VelvetOptimiser on data 9, data 7, and others: Contigs VelvetOptimiser on data 9, data 7, and others: Contig Stats Click on the icon of the various objects. Contigs: You\u2019ll see the first MB of the file. Note that the contigs are named NODE_XX_length_XXXX_cov_XXX.XXX. This information tells you how long (in k-mer length) each contig is and what it\u2019s average k-mer coverage is. (See detailed explanation of Velvet and Velvet Optimiser for explanation of k-mer coverage and k-mer length.) Contig stats: This shows a table of the contigs and their k-mer coverages and which read library contributed to the coverage. It is interesting to note that some of them have much higher coverage than the average. These are most likely to be repeated contigs. (Things like ribosomal RNA and IS elements.) Figure 5: Screenshot of assembled contigs (a) and contig stats (b) a b Calculate some statistics on the assembled contigs From the tools menu in the left hand panel of Galaxy, select FASTA Manipulation - Fasta Statistics and run with these parameters: Fasta or multifasta file : Velvet Optimiser Contigs Click Execute Examine the Fasta Stats output You should now have one more object in your history: Fasta Statistics on data 10: Fasta summary stats Click on the icon next to this object and have a look at the output. You\u2019ll see a statistical summary of the contigs including various length stats, the % GC content, the n50 as well as the number of contigs and the number of N bases contained in them. Section 3: Extension. Examine the contig coverage depth and blast a high coverage contig against a protein database. Examine the contig coverage depth. Look at the Contig Stats data (Velvet Optimiser vlsci on data 8, data 9, and data 7: Contig stats) by clicking on the icon. Note that column 2 contig length (lgth), shows a number of very short contigs (some are length 1). We can easily filter out these short contigs from this information list by using the Filter and Sort - Filter tool. Set the following: Filter : Velvet Optimiser on data 8, data 7 and others: Contig stats With the following condition : c2 100 Click Execute The new data object in the history is called: Filter on data 11 . Click on its icon to view it. Look through the list taking note of the coverages. Note that the average of the coverages (column 6) seems to be somewhere between 16 and 32. There are a lot of contigs with coverage 16. We could say that these contigs only appear once in the genome of the bacteria. Therefore, contigs with double this coverage would appear twice. Note that some of the coverages are 400! These contigs will appear in the genome more than 20 times! Lets have a look at one of these contigs and see if we can find out what it is. Extract a single sequence from the contigs file. Note the contig number (column 1 in the Contig stats file) of a contig with a coverage of over 300. There should be a few of them. We need to extract the fasta sequence of this contig from the contigs multifasta so we can see it more easily. To do this we will use the tool: Fasta manipulation - Fasta Extract Sequence Set the following: Fasta or multifasta file : Velvet Optimiser : Contigs Sequence ID (or partial): NODE_1_ (for example) Click Execute The new data object in the history is called: Fasta Extract Sequence on data 10: Fasta . Click on its icon to view it. It is a single sequence in fasta format. Blast sequence to determine what it contains. We want to find out what this contig is or what kind of coding sequence (if any) it contains. So we will blast the sequence using the NCBI blast website. (External to Galaxy). To do this: Bring up the sequence of the contig into the main window of the browser by clicking on the icon if it isn\u2019t already. Select the entire sequence by clicking and dragging with the mouse or by pressing ctrl-a in the browser. Copy the selected sequence to the clipboard. Open a new tab of your browser and point it to: http://blast.ncbi.nlm.nih.gov/Blast.cgi Under the BASIC BLAST section, click \u201cblastx\u201d. Paste the sequence into the large text box labelled: Enter Accession number(s), gi(s) or FASTA sequence(s). Change the Genetic code to: Bacteria and Archaea (11) Click the button labelled: BLAST After a while the website will present a report of the blast run. Note that the sequence we blasted (if you chose NODE_1) is identical to part of a transposase gene (IS256) from a similar Staphylococcus aureus bacteria. These transposases occur frequently as repeats in bacterial genomes and so we shouldn\u2019t be surprised at its very high coverage. Figure 6: Screenshot of the output from the NCBI Blast website De novo assembly with Velvet and the Velvet Optimiser. Velvet Velvet is software to perform dna assembly from short reads by manipulating de Bruijn graphs. It is capable of forming long contigs (n50 of in excess of 150kb) from paired end short reads. It has several input parameters for controlling the structure of the de Bruijn graph and these must be set optimally to get the best assembly possible. Velvet can read Fasta, FastQ, sam or bam files. However, it ignores any quality scores and simply relies on sequencing depth to resolve errors. The Velvet Optimiser software performs many Velvet assemblies with various parameter sets and searches for the optimal assembly automatically. de Bruijn graphs A de Bruijn graph is a directed graph which represents overlaps between sequences of symbols. The size of the sequence contained in the nodes of the graph is called the word-length or k-mer size. In Figure 2, the word length is 3. The two symbols are 1 and 0. Each node in the graph has the last two symbols of the previous node and 1 new symbol. Sequences of symbols can be produced by traversing the graph and adding the \u201cnew\u201d symbol to the growing sequence. Figure 2: A de Bruijn graph of word length 3 for the symbols 1 and 0. From: https://cameroncounts.wordpress.com/2015/02/28/1247/ Velvet constructs a de Bruijn graph of the reads. It has 4 symbols (A, C, G and T - N\u2019s are converted to A\u2019s) The word length (or k-mer size) is one of Velvet\u2019s prime parameters. Velvet is not the only assembly software that works in this manner. Euler, Edena and SOAP de novo are examples of others. The Velvet algorithm Step 1: Hashing the reads. Velvet breaks up each read into k-mers of length k. A k-mer is a k length subsequence of the read. A 36 base pair long read would have 6 different 31-mers. The k-mers and their reverse complements are added to a hash table to categorize them. Each k-mer is stored once but the number of times it appears is also recorded. This step is performed by \u201cvelveth\u201d - one of the programs in the Velvet suite. Step 2: Constructing the de Bruijn graph. Velvet adds the k-mers one-by-one to the graph. Adjacent k-mers overlap by k-1 nucleotides. A k-mer which has no k-1 overlaps with any k-mer already on the graph starts a new node. Each node stores the average number of times its k-mers appear in the hash table. Figure 3 shows a section of a de Bruijn graph constructed by Velvet for k=5. Different sequences can be read off the graph by following a different path through it. (Figure 3) Figure 3: Section of a simple de Bruijn graph of reads with k-mer size 5. Coloured sequences are constructed by following the appropriately coloured line through the graph. (Base figure Zerbino et al 2008.) Step 3: Simplification of the graph. Chain merging: When there are two connected nodes in the graph without a divergence, merge the two nodes. Tip clipping: Tips are short (typically) chains of nodes that are disconnected on one end. They will be clipped if their length is 2 x k or their average k-mer depth is much less than the continuing path. Bubble removal: Bubbles are redundant paths that start and end at the same nodes (Figure 4.) They are created by sequencing errors, biological variants or slightly varying repeat sequences. Velvet compares the paths using dynamic programming. If they are highly similar, the paths are merged. Error removal: Erroneous connections are removed by using a \u201ccoverage cutoff\u201d. Genuine short nodes which cannot be simplified should have a high coverage. An attempt is made to resolve repeats using the \u201cexpected coverage\u201d of the graph nodes. Paired end read information: Velvet uses algorithms called \u201cPebble\u201d and \u201cRock Band\u201d (Zerbino et al 2009) to order the nodes with respect to one another in order to scaffold them into longer contigs. Figure 4: Representation of \u201cbubbles\u201d in a Velvet de Bruijn graph. (Base figure Zerbino et al 2008.) Step 4: Read off the contigs. Follow the chains of nodes through the graph and \u201cread off\u201d the bases to create the contigs. Where there is an ambiguous divergence/convergence, stop the current contig and start a new one. K-mer size and coverage cutoff values The size of the k-mers that construct the graph is very important and has a large effect on the outcome of the assembly. Generally, small k-mers create a graph with increased connectivity, more ambiguity (more divergences) and less clear \u201cpaths\u201d through the graph. Large k-mers produce graphs with less connectivity but higher specificity. The paths through the graph are clearer but they are less connected and prone to breaking down. The coverage cutoff c used during the error correction step of Velvet also has a significant effect on the output of the assembly process. If c is too low, the assembly will contain nodes of the graph that are the product of sequencing errors and misconnections. If c is too high, it can create mis-assemblies in the contigs and destroys lots of useful data. Each dataset has its own optimum values for the k-mer size and the coverage cutoff used in the error removal step. Choosing them appropriately is one of the challenges faced by new users of the Velvet software. Velvet Optimiser The Velvet Optimiser chooses the optimal values for k and c automatically by performing many runs of Velvet (partially in parallel) and interrogating the subsequent assemblies. It uses different optimisation functions for k and c and these can be user controlled. It requires the user to input a range of k values to search (to cut down on running time). References http://en.wikipedia.org/wiki/Sequence_assembly Zerbino DR, Birney E, Velvet: algorithms for de novo short read assembly using de Bruijn graphs, Genome Research, 2008, 18:821-829 Zerbino DR, McEwen GK, Margulies EH, Birney E, Pebble and rock band: heuristic resolution of repeats and scaffolding in the velvet short-read de novo assembler. PLoS One. 2009; 4(12):e8407. Gladman SL, Seemann T, Velvet Optimiser, http://www.vicbioinformatics.com/software.shtml 2009.","title":"Galaxy - Velvet"},{"location":"scrapbook/dna/denovo/galaxy-velvet/galaxy-velvet/#galaxy-velvet","text":"Modified from LSCC tutorial by Simon Gladman - VLSCI","title":"Galaxy - Velvet"},{"location":"scrapbook/dna/denovo/galaxy-velvet/galaxy-velvet/#tutorial-overview","text":"In this tutorial we cover the concepts of Microbial de novo assembly using a very small synthetic dataset from a well studied organism.","title":"Tutorial Overview"},{"location":"scrapbook/dna/denovo/galaxy-velvet/galaxy-velvet/#background","text":"Where is the data in this tutorial from? The data for this tutorial is from a whole genome sequencing experiment of a multi-drug resistant strain of the bacterium Staphylococcus aureus . The DNA was sequenced using an Illumina GAII sequencing machine. The data we are going to use consists of about 4 million x 75 base-pair, paired end reads (two FASTQ read files, one for each end of a DNA fragment.) The data was downloaded from the NCBI Short Read Archive (SRA) (http://www.ncbi.nlm.nih.gov/sra/). The specific sample is a public dataset published in April 2012 with SRA accession number ERR048396. We will also use a FASTA file containing the sequences of the Illumina adapters used in the sequencing process. It is desirable to remove these as they are artificial sequences and not part of the bacterium that was sequenced. We will use software called Velvet (Zerbino et al 2008) for the main de novo assembly, as well as some other peripheral software for pre- and post-processing of the data. Details of these can be found in the background document linked above. The protocol: We are performing a de novo assembly of the read data into contigs and then into scaffolds (appropriately positioned contigs loosely linked together). We firstly need to check the quality of the input data as this will help us choose the most appropriate range of input parameters for the assembly and will guide us on an appropriate quality trimming/cleanup strategy. We will then use an iterative method to assemble the reads using the Velvet Optimiser (a program that performs lots of Velvet assemblies searching for an optimum outcome.) Once this is complete we will obtain summary statistics on the final results (contigs) of the assembly. More information about this protocol at the end of this tutorial. The protocol in a nutshell: Input: Raw reads from sequencer run on microbial DNA sample. Output: File of assembled scaffolds/contigs and associated information.","title":"Background"},{"location":"scrapbook/dna/denovo/galaxy-velvet/galaxy-velvet/#input-data","text":"On the Galaxy tools panel, click on Get data - Upload File . Click on the Paste/Fetch Data button. Paste the URL: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Assembly/ERR048396_1.fastq.gz into the text box. Change the type to fastqsanger (Not fastqcsanger ). Click on the Paste/Fetch Data button again. Paste the URL: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Assembly/ERR048396_2.fastq.gz into the text box and change it s type to fastqsanger as well. Repeat the process for the last URL: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Assembly/illumina_adapters.fna , but make it s type fasta Click on the Start button. Once all of the uploads are at 100%, click on the Close button. When the files have finished uploading, rename them to \u2018ERR048396_1.fastq\u2019, \u2018ERR048396_2.fastq\u2019 and \u2018illumina_adapters.fna\u2019 respectively by clicking on the icon to the top right of the file name in the right hand Galaxy panel (the history panel) You should now have the following files in your Galaxy history: ERR048396_1.fastq - forward reads in fastq format ERR048396_2.fastq - reverse reads in fastq format illumina_adapters.fa - Illumina adapter sequences in fasta format Click on the icon to the top right of each fastq file to view the first part of the file","title":"Input data"},{"location":"scrapbook/dna/denovo/galaxy-velvet/galaxy-velvet/#section-1-quality-control","text":"The basic process here is to collect statistics about the quality of the reads in the sample FASTQ readsets. We will then evaluate their quality and choose an appropriate regime for quality filtering using Trimmomatic (a FASTQ read quality trimmer.)","title":"Section 1: Quality control"},{"location":"scrapbook/dna/denovo/galaxy-velvet/galaxy-velvet/#run-fastqc-on-both-input-read-files","text":"From the tools menu in the left hand panel of Galaxy, select NGS QC and manipulation FastQC: Comprehensive QC (down the bottom of this category) and run with these parameters: FASTQ reads : ERR048396_1.fastq Use default for other fields Click Execute Now repeat the above process on the second read file: ERR048396_2.fastq It is important to do both read files as the quality can be very different between them.","title":"Run FastQC on both input read files"},{"location":"scrapbook/dna/denovo/galaxy-velvet/galaxy-velvet/#figure-1-screenshot-of-fastqc-interface-in-galaxy","text":"","title":"Figure 1: Screenshot of FastQC interface in Galaxy"},{"location":"scrapbook/dna/denovo/galaxy-velvet/galaxy-velvet/#examine-the-fastqc-output","text":"You should have two output objects from the first step: FastQC_ERR048396_1.fastqc.html FastQC_ERR048396_2.fastqc.html These are a html outputs which show the results of all of the tests FastQC performed on the read files. Click on the icon of each of these objects in turn to see the FastQC output. The main parts of the output to evaluate are: Basic statistics. This section tells us that the ASCII quality encoding format used was Sanger/Illumina 1.9 and the reads are length 75 and the percent GC content of the entire file is 35%. Per base sequence quality. In the plot you should see that most of the early bases are up around the 32 mark and then increase to 38-40, which is very high quality; The spread of quality values for the last few bases increases and some of the outliers have quality scores of less than 30. This is a very good quality dataset. 20 is often used as a cutoff for reliable quality.","title":"Examine the FastQC output"},{"location":"scrapbook/dna/denovo/galaxy-velvet/galaxy-velvet/#figure-2-screenshot-of-fastqc-output-in-galaxy","text":"","title":"Figure 2: Screenshot of FastQC output in Galaxy"},{"location":"scrapbook/dna/denovo/galaxy-velvet/galaxy-velvet/#quality-trim-the-reads-using-trimmomatic","text":"From the tools menu in the left hand panel of Galaxy, select NGS QC and manipulation Trimmomatic and run with these parameters (only the non-default selections are listed here): Input FASTQ file (R1/first of pair) : ERR048396_1.fastq Input FASTQ file (R2/second of pair) : ERR048396_2.fastq Perform initial ILLUMINACLIP step? : Yes Adapter sequences to use : TruSeq3 (additional seqs) (paired end, for MiSeq and HiSeq) How accurate read alignment : 40 How accurate against a read : 15 We will use the default settings for the SLIDING_WINDOW operation but we need to add a few more Trimmomatic operations. Click Insert Trimmomatic Operation Add Cut bases (LEADING) Minimum quality required to keep a base : 15 Repeat the Insert Trimmomatic Operation for: Trim trailing bases, minimum quality: 15 Minimum length read: 35 Click Execute","title":"Quality trim the reads using Trimmomatic."},{"location":"scrapbook/dna/denovo/galaxy-velvet/galaxy-velvet/#figure-3-screenshot-of-trimmomatic-inputs-in-galaxy","text":"","title":"Figure 3: Screenshot of Trimmomatic inputs in Galaxy"},{"location":"scrapbook/dna/denovo/galaxy-velvet/galaxy-velvet/#examine-the-trimmomatic-output-fastq-files","text":"You should have 4 new objects in your history from the output of Trimmomatic: Trimmomatic on data 2 and data 1 (R1 Paired) Trimmomatic on data 2 and data 1 (R1 Unpaired) Trimmomatic on data 2 and data 1 (R2 Paired) Trimmomatic on data 2 and data 1 (R2 Unpaired) Click on the on one of the objects to look at its contents. You\u2019ll notice that not all of the reads are the same length now, as they have had the illumina adapters cut out of them and they\u2019ve been quality trimmed.","title":"Examine the Trimmomatic output FastQ files."},{"location":"scrapbook/dna/denovo/galaxy-velvet/galaxy-velvet/#section-2-assemble-reads-into-contigs-with-velvet-and-the-velvet-optimiser","text":"The aim here is to assemble the trimmed reads into contigs/scaffolds using Velvet and the Velvet Optimiser. We will use a single tool, Velvet Optimiser, which takes the trimmed reads from Trimmomatic and performs numerous Velvet assemblies to find the best one. We need to add the reads in two separate libraries. One for the still paired reads and the other for the singleton reads orphaned from their pairs by the trimming process. Click here for a more detailed explanation of Velvet assemblies and the Velvet Optimiser","title":"Section 2: Assemble reads into contigs with Velvet and the Velvet Optimiser"},{"location":"scrapbook/dna/denovo/galaxy-velvet/galaxy-velvet/#de-novo-assembly-of-the-reads-into-contigs","text":"From the tools menu in the left hand panel of Galaxy, select NGS: Assembly - Velvet Optimiser and run with these parameters (only the non-default selections are listed here): Start k-mer value : 55 End k-mer value : 69 In the input files section: Select first set of reads : Trimmomatic on data 2 and data 1 (R1 paired) Select second set of reads : Trimmomatic on data 2 and data 1 (R2 paired) Click the Insert Input Files button and add the following: Single or paired end reads : Single Select the reads : Trimmomatic on data 2 and data 1 (R1 unpaired) Repeat the above process to add the other unpaired read set Trimmomatic on data 2 and data 1 (R2 unpaired) as well. Click Execute .","title":"De novo assembly of the reads into contigs"},{"location":"scrapbook/dna/denovo/galaxy-velvet/galaxy-velvet/#figure-4-screenshot-of-velvet-optimiser-inputs-in-galaxy","text":"","title":"Figure 4: Screenshot of Velvet Optimiser inputs in Galaxy"},{"location":"scrapbook/dna/denovo/galaxy-velvet/galaxy-velvet/#examine-assembly-output","text":"Once step 1 is complete, you should now have 2 new objects in your history: VelvetOptimiser on data 9, data 7, and others: Contigs VelvetOptimiser on data 9, data 7, and others: Contig Stats Click on the icon of the various objects. Contigs: You\u2019ll see the first MB of the file. Note that the contigs are named NODE_XX_length_XXXX_cov_XXX.XXX. This information tells you how long (in k-mer length) each contig is and what it\u2019s average k-mer coverage is. (See detailed explanation of Velvet and Velvet Optimiser for explanation of k-mer coverage and k-mer length.) Contig stats: This shows a table of the contigs and their k-mer coverages and which read library contributed to the coverage. It is interesting to note that some of them have much higher coverage than the average. These are most likely to be repeated contigs. (Things like ribosomal RNA and IS elements.)","title":"Examine assembly output"},{"location":"scrapbook/dna/denovo/galaxy-velvet/galaxy-velvet/#figure-5-screenshot-of-assembled-contigs-a-and-contig-stats-b","text":"","title":"Figure 5: Screenshot of assembled contigs (a) and contig stats (b)"},{"location":"scrapbook/dna/denovo/galaxy-velvet/galaxy-velvet/#a","text":"","title":"a"},{"location":"scrapbook/dna/denovo/galaxy-velvet/galaxy-velvet/#b","text":"","title":"b"},{"location":"scrapbook/dna/denovo/galaxy-velvet/galaxy-velvet/#calculate-some-statistics-on-the-assembled-contigs","text":"From the tools menu in the left hand panel of Galaxy, select FASTA Manipulation - Fasta Statistics and run with these parameters: Fasta or multifasta file : Velvet Optimiser Contigs Click Execute Examine the Fasta Stats output You should now have one more object in your history: Fasta Statistics on data 10: Fasta summary stats Click on the icon next to this object and have a look at the output. You\u2019ll see a statistical summary of the contigs including various length stats, the % GC content, the n50 as well as the number of contigs and the number of N bases contained in them.","title":"Calculate some statistics on the assembled contigs"},{"location":"scrapbook/dna/denovo/galaxy-velvet/galaxy-velvet/#section-3-extension","text":"Examine the contig coverage depth and blast a high coverage contig against a protein database.","title":"Section 3: Extension."},{"location":"scrapbook/dna/denovo/galaxy-velvet/galaxy-velvet/#examine-the-contig-coverage-depth","text":"Look at the Contig Stats data (Velvet Optimiser vlsci on data 8, data 9, and data 7: Contig stats) by clicking on the icon. Note that column 2 contig length (lgth), shows a number of very short contigs (some are length 1). We can easily filter out these short contigs from this information list by using the Filter and Sort - Filter tool. Set the following: Filter : Velvet Optimiser on data 8, data 7 and others: Contig stats With the following condition : c2 100 Click Execute The new data object in the history is called: Filter on data 11 . Click on its icon to view it. Look through the list taking note of the coverages. Note that the average of the coverages (column 6) seems to be somewhere between 16 and 32. There are a lot of contigs with coverage 16. We could say that these contigs only appear once in the genome of the bacteria. Therefore, contigs with double this coverage would appear twice. Note that some of the coverages are 400! These contigs will appear in the genome more than 20 times! Lets have a look at one of these contigs and see if we can find out what it is.","title":"Examine the contig coverage depth."},{"location":"scrapbook/dna/denovo/galaxy-velvet/galaxy-velvet/#extract-a-single-sequence-from-the-contigs-file","text":"Note the contig number (column 1 in the Contig stats file) of a contig with a coverage of over 300. There should be a few of them. We need to extract the fasta sequence of this contig from the contigs multifasta so we can see it more easily. To do this we will use the tool: Fasta manipulation - Fasta Extract Sequence Set the following: Fasta or multifasta file : Velvet Optimiser : Contigs Sequence ID (or partial): NODE_1_ (for example) Click Execute The new data object in the history is called: Fasta Extract Sequence on data 10: Fasta . Click on its icon to view it. It is a single sequence in fasta format.","title":"Extract a single sequence from the contigs file."},{"location":"scrapbook/dna/denovo/galaxy-velvet/galaxy-velvet/#blast-sequence-to-determine-what-it-contains","text":"We want to find out what this contig is or what kind of coding sequence (if any) it contains. So we will blast the sequence using the NCBI blast website. (External to Galaxy). To do this: Bring up the sequence of the contig into the main window of the browser by clicking on the icon if it isn\u2019t already. Select the entire sequence by clicking and dragging with the mouse or by pressing ctrl-a in the browser. Copy the selected sequence to the clipboard. Open a new tab of your browser and point it to: http://blast.ncbi.nlm.nih.gov/Blast.cgi Under the BASIC BLAST section, click \u201cblastx\u201d. Paste the sequence into the large text box labelled: Enter Accession number(s), gi(s) or FASTA sequence(s). Change the Genetic code to: Bacteria and Archaea (11) Click the button labelled: BLAST After a while the website will present a report of the blast run. Note that the sequence we blasted (if you chose NODE_1) is identical to part of a transposase gene (IS256) from a similar Staphylococcus aureus bacteria. These transposases occur frequently as repeats in bacterial genomes and so we shouldn\u2019t be surprised at its very high coverage.","title":"Blast sequence to determine what it contains."},{"location":"scrapbook/dna/denovo/galaxy-velvet/galaxy-velvet/#figure-6-screenshot-of-the-output-from-the-ncbi-blast-website","text":"","title":"Figure 6: Screenshot of the output from the NCBI Blast website"},{"location":"scrapbook/dna/denovo/galaxy-velvet/galaxy-velvet/#de-novo-assembly-with-velvet-and-the-velvet-optimiser","text":"","title":"De novo assembly with Velvet and the Velvet Optimiser."},{"location":"scrapbook/dna/denovo/galaxy-velvet/galaxy-velvet/#velvet","text":"Velvet is software to perform dna assembly from short reads by manipulating de Bruijn graphs. It is capable of forming long contigs (n50 of in excess of 150kb) from paired end short reads. It has several input parameters for controlling the structure of the de Bruijn graph and these must be set optimally to get the best assembly possible. Velvet can read Fasta, FastQ, sam or bam files. However, it ignores any quality scores and simply relies on sequencing depth to resolve errors. The Velvet Optimiser software performs many Velvet assemblies with various parameter sets and searches for the optimal assembly automatically.","title":"Velvet"},{"location":"scrapbook/dna/denovo/galaxy-velvet/galaxy-velvet/#de-bruijn-graphs","text":"A de Bruijn graph is a directed graph which represents overlaps between sequences of symbols. The size of the sequence contained in the nodes of the graph is called the word-length or k-mer size. In Figure 2, the word length is 3. The two symbols are 1 and 0. Each node in the graph has the last two symbols of the previous node and 1 new symbol. Sequences of symbols can be produced by traversing the graph and adding the \u201cnew\u201d symbol to the growing sequence. Figure 2: A de Bruijn graph of word length 3 for the symbols 1 and 0. From: https://cameroncounts.wordpress.com/2015/02/28/1247/ Velvet constructs a de Bruijn graph of the reads. It has 4 symbols (A, C, G and T - N\u2019s are converted to A\u2019s) The word length (or k-mer size) is one of Velvet\u2019s prime parameters. Velvet is not the only assembly software that works in this manner. Euler, Edena and SOAP de novo are examples of others.","title":"de Bruijn graphs"},{"location":"scrapbook/dna/denovo/galaxy-velvet/galaxy-velvet/#the-velvet-algorithm","text":"","title":"The Velvet algorithm"},{"location":"scrapbook/dna/denovo/galaxy-velvet/galaxy-velvet/#step-1-hashing-the-reads","text":"Velvet breaks up each read into k-mers of length k. A k-mer is a k length subsequence of the read. A 36 base pair long read would have 6 different 31-mers. The k-mers and their reverse complements are added to a hash table to categorize them. Each k-mer is stored once but the number of times it appears is also recorded. This step is performed by \u201cvelveth\u201d - one of the programs in the Velvet suite.","title":"Step 1: Hashing the reads."},{"location":"scrapbook/dna/denovo/galaxy-velvet/galaxy-velvet/#step-2-constructing-the-de-bruijn-graph","text":"Velvet adds the k-mers one-by-one to the graph. Adjacent k-mers overlap by k-1 nucleotides. A k-mer which has no k-1 overlaps with any k-mer already on the graph starts a new node. Each node stores the average number of times its k-mers appear in the hash table. Figure 3 shows a section of a de Bruijn graph constructed by Velvet for k=5. Different sequences can be read off the graph by following a different path through it. (Figure 3) Figure 3: Section of a simple de Bruijn graph of reads with k-mer size 5. Coloured sequences are constructed by following the appropriately coloured line through the graph. (Base figure Zerbino et al 2008.)","title":"Step 2: Constructing the de Bruijn graph."},{"location":"scrapbook/dna/denovo/galaxy-velvet/galaxy-velvet/#step-3-simplification-of-the-graph","text":"Chain merging: When there are two connected nodes in the graph without a divergence, merge the two nodes. Tip clipping: Tips are short (typically) chains of nodes that are disconnected on one end. They will be clipped if their length is 2 x k or their average k-mer depth is much less than the continuing path. Bubble removal: Bubbles are redundant paths that start and end at the same nodes (Figure 4.) They are created by sequencing errors, biological variants or slightly varying repeat sequences. Velvet compares the paths using dynamic programming. If they are highly similar, the paths are merged. Error removal: Erroneous connections are removed by using a \u201ccoverage cutoff\u201d. Genuine short nodes which cannot be simplified should have a high coverage. An attempt is made to resolve repeats using the \u201cexpected coverage\u201d of the graph nodes. Paired end read information: Velvet uses algorithms called \u201cPebble\u201d and \u201cRock Band\u201d (Zerbino et al 2009) to order the nodes with respect to one another in order to scaffold them into longer contigs. Figure 4: Representation of \u201cbubbles\u201d in a Velvet de Bruijn graph. (Base figure Zerbino et al 2008.)","title":"Step 3: Simplification of the graph."},{"location":"scrapbook/dna/denovo/galaxy-velvet/galaxy-velvet/#step-4-read-off-the-contigs","text":"Follow the chains of nodes through the graph and \u201cread off\u201d the bases to create the contigs. Where there is an ambiguous divergence/convergence, stop the current contig and start a new one.","title":"Step 4: Read off the contigs."},{"location":"scrapbook/dna/denovo/galaxy-velvet/galaxy-velvet/#k-mer-size-and-coverage-cutoff-values","text":"The size of the k-mers that construct the graph is very important and has a large effect on the outcome of the assembly. Generally, small k-mers create a graph with increased connectivity, more ambiguity (more divergences) and less clear \u201cpaths\u201d through the graph. Large k-mers produce graphs with less connectivity but higher specificity. The paths through the graph are clearer but they are less connected and prone to breaking down. The coverage cutoff c used during the error correction step of Velvet also has a significant effect on the output of the assembly process. If c is too low, the assembly will contain nodes of the graph that are the product of sequencing errors and misconnections. If c is too high, it can create mis-assemblies in the contigs and destroys lots of useful data. Each dataset has its own optimum values for the k-mer size and the coverage cutoff used in the error removal step. Choosing them appropriately is one of the challenges faced by new users of the Velvet software.","title":"K-mer size and coverage cutoff values"},{"location":"scrapbook/dna/denovo/galaxy-velvet/galaxy-velvet/#velvet-optimiser","text":"The Velvet Optimiser chooses the optimal values for k and c automatically by performing many runs of Velvet (partially in parallel) and interrogating the subsequent assemblies. It uses different optimisation functions for k and c and these can be user controlled. It requires the user to input a range of k values to search (to cut down on running time).","title":"Velvet Optimiser"},{"location":"scrapbook/dna/denovo/galaxy-velvet/galaxy-velvet/#references","text":"http://en.wikipedia.org/wiki/Sequence_assembly Zerbino DR, Birney E, Velvet: algorithms for de novo short read assembly using de Bruijn graphs, Genome Research, 2008, 18:821-829 Zerbino DR, McEwen GK, Margulies EH, Birney E, Pebble and rock band: heuristic resolution of repeats and scaffolding in the velvet short-read de novo assembler. PLoS One. 2009; 4(12):e8407. Gladman SL, Seemann T, Velvet Optimiser, http://www.vicbioinformatics.com/software.shtml 2009.","title":"References"},{"location":"scrapbook/dna/pan/mauve/","text":"Mauve This tutorial demonstrates how to use the Mauve software to align genomes. For example, a de novo assembly can be compared against a reference genome to check the assembly. Or, changes between two or more genomes can be examined. Link to Mauve Pre-requisites mGVL instance if using Lubuntu desktop Start FIXME: use linux version in Lubuntu desktop, or local version e.g. Mac ? or you can use in the cmd line Input 2+ genomes in FASTA(.fasta or .fna)/gbk format FIXME: choose data How it works Mauve finds structural variants in genomes. It aligns genoms and finds homologous regions, either from a common ancestor (orthologs) or lateral transfer (xenologs). Run File: Align with progressiveMauve Add sequences : select 2+ genome FASTA files Output : provide a name for the output folder Align the Mauve Console will appear while Mauve is running, showing the job status. Output FIXME: screenshots one genome per line genomic regions are coloured blocks: locally co-linear blocks (LCB): a block of genome that is unchanged inside, but as a block might have moved/inverted etc. you can export a list of annotated homologous features Next","title":"Mauve"},{"location":"scrapbook/dna/pan/mauve/#mauve","text":"This tutorial demonstrates how to use the Mauve software to align genomes. For example, a de novo assembly can be compared against a reference genome to check the assembly. Or, changes between two or more genomes can be examined. Link to Mauve","title":"Mauve"},{"location":"scrapbook/dna/pan/mauve/#pre-requisites","text":"mGVL instance if using Lubuntu desktop","title":"Pre-requisites"},{"location":"scrapbook/dna/pan/mauve/#start","text":"FIXME: use linux version in Lubuntu desktop, or local version e.g. Mac ? or you can use in the cmd line","title":"Start"},{"location":"scrapbook/dna/pan/mauve/#input","text":"2+ genomes in FASTA(.fasta or .fna)/gbk format FIXME: choose data","title":"Input"},{"location":"scrapbook/dna/pan/mauve/#how-it-works","text":"Mauve finds structural variants in genomes. It aligns genoms and finds homologous regions, either from a common ancestor (orthologs) or lateral transfer (xenologs).","title":"How it works"},{"location":"scrapbook/dna/pan/mauve/#run","text":"File: Align with progressiveMauve Add sequences : select 2+ genome FASTA files Output : provide a name for the output folder Align the Mauve Console will appear while Mauve is running, showing the job status.","title":"Run"},{"location":"scrapbook/dna/pan/mauve/#output","text":"FIXME: screenshots one genome per line genomic regions are coloured blocks: locally co-linear blocks (LCB): a block of genome that is unchanged inside, but as a block might have moved/inverted etc. you can export a list of annotated homologous features","title":"Output"},{"location":"scrapbook/dna/pan/mauve/#next","text":"","title":"Next"},{"location":"scrapbook/dna/pan/orthomcl/","text":"OrthoMCL cmdline - probably not worth bothering","title":"OrthoMCL"},{"location":"scrapbook/dna/pan/orthomcl/#orthomcl","text":"cmdline - probably not worth bothering","title":"OrthoMCL"},{"location":"scrapbook/dna/pan/roary/","text":"Roary cmdline This tutorial demonstrates how to calculate the pan and core genomes of a set of input bacterial species, using Roary. Roary code and manual on github Roary paper Pre-requisites background: pan genomes a mGVL instance Start via local Terminal : in your terminal, ssh into your mGVL, but make sure you put in -X -Y after ssh so that xquartz can view files later. (FIXME: word better) or via virtual desktop : Go to your mGVL dashboard. Click on the link to the Lubuntu desktop. A virtual desktop will open in a new browser window. Enter username: ubunutu; and your GVL password. Click on terminal in the top left corner. Input data Roary takes .gff files produced by Prokka. A gff file has sequences and annotations. file formats FIXME: link to proper page Get files into mGVL. (FIXME: explain how, wget etc or from GenomeSpace) put all gff files into a folder FIXME: choose a good sample set - ideally something that usefully shows how AMR genes can be present/absence in a group? for a draft can use the Listeria tutorial at https://github.com/microgenomics/tutorials/blob/master/pangenome.md Run navigate into the place where the gff folder is. roary -f ./results ./gff_files/*.gff -f ./results puts the output into a directory called results How it works Based on the input genomes, Roary works out which genes are shared between all (core) and which are not (accessory). It uses the protein-coding genes from each of the input genomes. converts to protein seqs similar protein seqs are clustered progressively. each sample: will be labelled with presence/absence of orthologous genes. Output summary statistics: more summary_statistics.txt you will see the number of core genes, shell genes, etc. q to exit viewing gene presence/absence graphically: roary2svg.pl gene_presence_absence.csv pan_genome.svg (if you have logged in with -X -Y) firefox pan_genome.svg then enter the makes it run in the background a firefox window should open with the svg image (later: close the firefox window to stop this job running in the background) list of genes that are present/absent: view the gene_presence_absence.csv by (FIXME) lots of information about this file in the roary website (FIXME summarize?) query the pan genome: copy the input .gff files into the results folder (FIXME: do this earlier) cd into this folder query_pan_genome -a intersection *.gff this finds the core genes more pan_genome_results shows the list of genes found in the core genome. q to exit viewing Advanced options FIXME: update firefox on mGVL so can run phandango Run roary and create an alignment of core genes: roary -f ./results -e -n -p 8 ./gff_files/*.gff -f ./results puts the output into a directory called results -e -n creates an alignment of core genes using mafft -p 8 gives 8 threads - optional, if you know how many you have Generate a tree based on the presence/absence of core genes: navigate into the results folder that you want to use. FastTree -nt -gtr core_gene_alignment.aln my_tree.newick (By default, roary will also have created a (very quick) tree from the accessory genes.) FastTree information and options . Use roary_plots.py to generate plots: navigate into the results folder that you want to use. python roary_plots.py core_gene_alignment.nwk gene_presence_absence.csv output: pangenome matrix, frequency plot, pie chart. view these by typing firefox [filename] and a firefox window will open to show the image. You need to close the window before you open the next image. What next? View using Phandango; tutorial here. More information another Roary tutorial","title":"Roary"},{"location":"scrapbook/dna/pan/roary/#roary","text":"cmdline This tutorial demonstrates how to calculate the pan and core genomes of a set of input bacterial species, using Roary. Roary code and manual on github Roary paper","title":"Roary"},{"location":"scrapbook/dna/pan/roary/#pre-requisites","text":"background: pan genomes a mGVL instance","title":"Pre-requisites"},{"location":"scrapbook/dna/pan/roary/#start","text":"via local Terminal : in your terminal, ssh into your mGVL, but make sure you put in -X -Y after ssh so that xquartz can view files later. (FIXME: word better) or via virtual desktop : Go to your mGVL dashboard. Click on the link to the Lubuntu desktop. A virtual desktop will open in a new browser window. Enter username: ubunutu; and your GVL password. Click on terminal in the top left corner.","title":"Start"},{"location":"scrapbook/dna/pan/roary/#input-data","text":"Roary takes .gff files produced by Prokka. A gff file has sequences and annotations. file formats FIXME: link to proper page Get files into mGVL. (FIXME: explain how, wget etc or from GenomeSpace) put all gff files into a folder FIXME: choose a good sample set - ideally something that usefully shows how AMR genes can be present/absence in a group? for a draft can use the Listeria tutorial at https://github.com/microgenomics/tutorials/blob/master/pangenome.md","title":"Input data"},{"location":"scrapbook/dna/pan/roary/#run","text":"navigate into the place where the gff folder is. roary -f ./results ./gff_files/*.gff -f ./results puts the output into a directory called results","title":"Run"},{"location":"scrapbook/dna/pan/roary/#how-it-works","text":"Based on the input genomes, Roary works out which genes are shared between all (core) and which are not (accessory). It uses the protein-coding genes from each of the input genomes. converts to protein seqs similar protein seqs are clustered progressively. each sample: will be labelled with presence/absence of orthologous genes.","title":"How it works"},{"location":"scrapbook/dna/pan/roary/#output","text":"","title":"Output"},{"location":"scrapbook/dna/pan/roary/#summary-statistics","text":"more summary_statistics.txt you will see the number of core genes, shell genes, etc. q to exit viewing","title":"summary statistics:"},{"location":"scrapbook/dna/pan/roary/#gene-presenceabsence-graphically","text":"roary2svg.pl gene_presence_absence.csv pan_genome.svg (if you have logged in with -X -Y) firefox pan_genome.svg then enter the makes it run in the background a firefox window should open with the svg image (later: close the firefox window to stop this job running in the background)","title":"gene presence/absence graphically:"},{"location":"scrapbook/dna/pan/roary/#list-of-genes-that-are-presentabsent","text":"view the gene_presence_absence.csv by (FIXME) lots of information about this file in the roary website (FIXME summarize?)","title":"list of genes that are present/absent:"},{"location":"scrapbook/dna/pan/roary/#query-the-pan-genome","text":"copy the input .gff files into the results folder (FIXME: do this earlier) cd into this folder query_pan_genome -a intersection *.gff this finds the core genes more pan_genome_results shows the list of genes found in the core genome. q to exit viewing","title":"query the pan genome:"},{"location":"scrapbook/dna/pan/roary/#advanced-options","text":"FIXME: update firefox on mGVL so can run phandango","title":"Advanced options"},{"location":"scrapbook/dna/pan/roary/#run-roary-and-create-an-alignment-of-core-genes","text":"roary -f ./results -e -n -p 8 ./gff_files/*.gff -f ./results puts the output into a directory called results -e -n creates an alignment of core genes using mafft -p 8 gives 8 threads - optional, if you know how many you have","title":"Run roary and create an alignment of core genes:"},{"location":"scrapbook/dna/pan/roary/#generate-a-tree-based-on-the-presenceabsence-of-core-genes","text":"navigate into the results folder that you want to use. FastTree -nt -gtr core_gene_alignment.aln my_tree.newick (By default, roary will also have created a (very quick) tree from the accessory genes.) FastTree information and options .","title":"Generate a tree based on the presence/absence of core genes:"},{"location":"scrapbook/dna/pan/roary/#use-roary_plotspy-to-generate-plots","text":"navigate into the results folder that you want to use. python roary_plots.py core_gene_alignment.nwk gene_presence_absence.csv output: pangenome matrix, frequency plot, pie chart. view these by typing firefox [filename] and a firefox window will open to show the image. You need to close the window before you open the next image.","title":"Use roary_plots.py to generate plots:"},{"location":"scrapbook/dna/pan/roary/#what-next","text":"View using Phandango; tutorial here.","title":"What next?"},{"location":"scrapbook/dna/pan/roary/#more-information","text":"another Roary tutorial","title":"More information"},{"location":"scrapbook/dna/qualitycontrol/fastqc_cmdline/","text":"FastQC - commandline Start on your local machine, make sure XQuartz is installed. This doesn t have to be open - it will open automatically later. in terminal, ssh to your virtual machine with -X and -Y, e.g. ssh -X -Y ubuntu@111.111.111.111 (the -X and -Y means it will use your local XQuartz to display some files). module load fastqc_dist_0_10_1 navigate to where you want to make a FastQC analysis folder. Make a folder: mkdir fastqc_analyses Move to that folder: cd fastqc_analyses Input move your input reads into this folder. Run fastqc R1reads.fastq [this runs fastqc] type fastqc --help to see settings that you can change, and defaults ! FIXME: any to change FIXME: repeat for R2reads? Output R1reads_fastqc: folder containing the output, e.g. fastqc_report.html to view this, type: firefox fastqc_report.html firefox should open and display the report (you may get an error message in terminal but ignore this).","title":"FastQC - commandline"},{"location":"scrapbook/dna/qualitycontrol/fastqc_cmdline/#fastqc-commandline","text":"","title":"FastQC - commandline"},{"location":"scrapbook/dna/qualitycontrol/fastqc_cmdline/#start","text":"on your local machine, make sure XQuartz is installed. This doesn t have to be open - it will open automatically later. in terminal, ssh to your virtual machine with -X and -Y, e.g. ssh -X -Y ubuntu@111.111.111.111 (the -X and -Y means it will use your local XQuartz to display some files). module load fastqc_dist_0_10_1 navigate to where you want to make a FastQC analysis folder. Make a folder: mkdir fastqc_analyses Move to that folder: cd fastqc_analyses","title":"Start"},{"location":"scrapbook/dna/qualitycontrol/fastqc_cmdline/#input","text":"move your input reads into this folder.","title":"Input"},{"location":"scrapbook/dna/qualitycontrol/fastqc_cmdline/#run","text":"fastqc R1reads.fastq [this runs fastqc] type fastqc --help to see settings that you can change, and defaults ! FIXME: any to change FIXME: repeat for R2reads?","title":"Run"},{"location":"scrapbook/dna/qualitycontrol/fastqc_cmdline/#output","text":"R1reads_fastqc: folder containing the output, e.g. fastqc_report.html to view this, type: firefox fastqc_report.html firefox should open and display the report (you may get an error message in terminal but ignore this).","title":"Output"},{"location":"scrapbook/dna/qualitycontrol/qc/","text":"Genomics QC Phred Adapters Trimming Clipping Depth","title":"Genomics QC"},{"location":"scrapbook/dna/qualitycontrol/qc/#genomics-qc","text":"Phred Adapters Trimming Clipping Depth","title":"Genomics QC"},{"location":"scrapbook/dna/qualitycontrol/trimmomatic_cmdline/","text":"Trimmomatic - command line [from LSCC docs; FIXME: add info data; test] for Paired End Files: java -cp path to trimmomatic jar org.usadellab.trimmomatic.TrimmomaticPE for Single End Files java -cp path to trimmomatic jar org.usadellab.trimmomatic.TrimmomaticSE Trimmomatic can perform many read trimming functions sequentially.","title":"Trimmomatic - command line"},{"location":"scrapbook/dna/qualitycontrol/trimmomatic_cmdline/#trimmomatic-command-line","text":"[from LSCC docs; FIXME: add info data; test] for Paired End Files: java -cp path to trimmomatic jar org.usadellab.trimmomatic.TrimmomaticPE for Single End Files java -cp path to trimmomatic jar org.usadellab.trimmomatic.TrimmomaticSE Trimmomatic can perform many read trimming functions sequentially.","title":"Trimmomatic - command line"},{"location":"scrapbook/dna/snps/bwa_mem/","text":"BWA-MEM in Galaxy Introduction This tool is used to find variants. (Note: BWA-MEM can also be used to map sequence reads to a reference for the purposes of assembly.) BWA stands for Burrows-Wheeler Aligner ; more information and manual here . Learning Objectives At the end of this tutorial you should be able to: map reads to reference using BWA-MEM, and identify variants in the output files. Data Run Go to Tools NGS Analysis NGS: Mapping Map with BWA-MEM Output SAM file What next? FIXME","title":"BWA-MEM in Galaxy"},{"location":"scrapbook/dna/snps/bwa_mem/#bwa-mem-in-galaxy","text":"","title":"BWA-MEM in Galaxy"},{"location":"scrapbook/dna/snps/bwa_mem/#introduction","text":"This tool is used to find variants. (Note: BWA-MEM can also be used to map sequence reads to a reference for the purposes of assembly.) BWA stands for Burrows-Wheeler Aligner ; more information and manual here .","title":"Introduction"},{"location":"scrapbook/dna/snps/bwa_mem/#learning-objectives","text":"At the end of this tutorial you should be able to: map reads to reference using BWA-MEM, and identify variants in the output files.","title":"Learning Objectives"},{"location":"scrapbook/dna/snps/bwa_mem/#data","text":"","title":"Data"},{"location":"scrapbook/dna/snps/bwa_mem/#run","text":"Go to Tools NGS Analysis NGS: Mapping Map with BWA-MEM","title":"Run"},{"location":"scrapbook/dna/snps/bwa_mem/#output","text":"SAM file","title":"Output"},{"location":"scrapbook/dna/snps/bwa_mem/#what-next","text":"FIXME","title":"What next?"},{"location":"scrapbook/dna/snps/galaxy/","text":"Galaxy need to make a workflow BWA samtools or varscan vcf filter?","title":"Galaxy"},{"location":"scrapbook/dna/snps/galaxy/#galaxy","text":"need to make a workflow BWA samtools or varscan vcf filter?","title":"Galaxy"},{"location":"scrapbook/dna/snps/snippy/","text":"Snippy cmdline This tutorial will demonstrate how to find variants in a bacterial genome using Snippy. Variants are found by comparing to a reference genome of the same species. Github link to Snippy Pre-requisites connect to your GVL - cmdline background knowledge: variant calling Start log in to your virtual machine via terminal navigate to the place where you want Snippy to run. make a folder called snippy - mkdir snippy move into that folder - cd snippy Input Raw sequence reads Illumina paired-end reads from a bacteria in FASTQ format. These reads are from Pasteurella multocida , from EMBL-EBI ENA. We will use wget to download them via FTP. in the snippy folder, wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR125/003/SRR1257473/SRR1257473_1.fastq.gz wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR125/003/SRR1257473/SRR1257473_2.fastq.gz These files are compressed and so end in .gz. Snippy can use these .gz read files directly without unzipping. Reference genome Reference genome from the same species, Pasteurella multocida , from EMBL-EBI Ensembl genomes, in FASTA format. in the snippy folder, wget ftp://ftp.ensemblgenomes.org/pub/bacteria/release-31/fasta/bacteria_104_collection/pasteurella_multocida_subsp_multocida_gca_001027695/dna/Pasteurella_multocida_subsp_multocida_gca_001027695.ASM102769v1.31.dna.genome.fa.gz This file is also compressed into .gz format. This needs to be unzipped: gunzip ftp://ftp.ensemblgenomes.org/pub/bacteria/release-31/fasta/bacteria_104_collection/pasteurella_multocida_subsp_multocida_gca_001027695/dna/Pasteurella_multocida_subsp_multocida_gca_001027695.ASM102769v1.31.dna.genome.fa.gz The file will now end in .fa (which is fasta format, and Snippy can use). How it works Reads are mapped to the reference genome using BWA: this makes a BAM file BAM file and the ref genome sequence sent to Freebayes Freebayes finds differences between the reads and the reference, and calls the variants. Run Snippy cpus: choose number of cpus to use [or it uses a default] - here we will use 16 outdir: choose a name for the output directory, where results will go - here we will use mysnps ref: the input reference genome filename R1: the input R1 reads filename R2: the input R2 reads filename to run snippy: snippy --cpus 16 --outdir mysnps --ref [ filename.fa ] --R1 [ R1.fastq.gz ] --R2 [ R2.fastq.gz ] Output 17 output files list all the output files (that were put into the mysnps folder): ls mysnps look at the first 10 lines of the snps.tab file head -10 mysnps/snps.tab look at these columns: chromosome (CHROM), genomic position (POS), variant type (TYPE), nucleotide state in the ref (REF), nucleotide state in the input sample (ALT), and the frequency counts of REF and ALT (EVIDENCE). FIXME: screenshot with arrows FIXME: filter for quality? FIXME: load reference and the tabular vcf file into JBrowse/Artemis/IGV to view the genome and the snps. FIXME: is there anything we are looking for in particular? e.g. number of variants, existing known variants, variants in particular genes, AMR variants? Next","title":"Snippy"},{"location":"scrapbook/dna/snps/snippy/#snippy","text":"cmdline This tutorial will demonstrate how to find variants in a bacterial genome using Snippy. Variants are found by comparing to a reference genome of the same species. Github link to Snippy","title":"Snippy"},{"location":"scrapbook/dna/snps/snippy/#pre-requisites","text":"connect to your GVL - cmdline background knowledge: variant calling","title":"Pre-requisites"},{"location":"scrapbook/dna/snps/snippy/#start","text":"log in to your virtual machine via terminal navigate to the place where you want Snippy to run. make a folder called snippy - mkdir snippy move into that folder - cd snippy","title":"Start"},{"location":"scrapbook/dna/snps/snippy/#input","text":"","title":"Input"},{"location":"scrapbook/dna/snps/snippy/#raw-sequence-reads","text":"Illumina paired-end reads from a bacteria in FASTQ format. These reads are from Pasteurella multocida , from EMBL-EBI ENA. We will use wget to download them via FTP. in the snippy folder, wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR125/003/SRR1257473/SRR1257473_1.fastq.gz wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR125/003/SRR1257473/SRR1257473_2.fastq.gz These files are compressed and so end in .gz. Snippy can use these .gz read files directly without unzipping.","title":"Raw sequence reads"},{"location":"scrapbook/dna/snps/snippy/#reference-genome","text":"Reference genome from the same species, Pasteurella multocida , from EMBL-EBI Ensembl genomes, in FASTA format. in the snippy folder, wget ftp://ftp.ensemblgenomes.org/pub/bacteria/release-31/fasta/bacteria_104_collection/pasteurella_multocida_subsp_multocida_gca_001027695/dna/Pasteurella_multocida_subsp_multocida_gca_001027695.ASM102769v1.31.dna.genome.fa.gz This file is also compressed into .gz format. This needs to be unzipped: gunzip ftp://ftp.ensemblgenomes.org/pub/bacteria/release-31/fasta/bacteria_104_collection/pasteurella_multocida_subsp_multocida_gca_001027695/dna/Pasteurella_multocida_subsp_multocida_gca_001027695.ASM102769v1.31.dna.genome.fa.gz The file will now end in .fa (which is fasta format, and Snippy can use).","title":"Reference genome"},{"location":"scrapbook/dna/snps/snippy/#how-it-works","text":"Reads are mapped to the reference genome using BWA: this makes a BAM file BAM file and the ref genome sequence sent to Freebayes Freebayes finds differences between the reads and the reference, and calls the variants.","title":"How it works"},{"location":"scrapbook/dna/snps/snippy/#run-snippy","text":"cpus: choose number of cpus to use [or it uses a default] - here we will use 16 outdir: choose a name for the output directory, where results will go - here we will use mysnps ref: the input reference genome filename R1: the input R1 reads filename R2: the input R2 reads filename to run snippy: snippy --cpus 16 --outdir mysnps --ref [ filename.fa ] --R1 [ R1.fastq.gz ] --R2 [ R2.fastq.gz ]","title":"Run Snippy"},{"location":"scrapbook/dna/snps/snippy/#output","text":"17 output files list all the output files (that were put into the mysnps folder): ls mysnps look at the first 10 lines of the snps.tab file head -10 mysnps/snps.tab look at these columns: chromosome (CHROM), genomic position (POS), variant type (TYPE), nucleotide state in the ref (REF), nucleotide state in the input sample (ALT), and the frequency counts of REF and ALT (EVIDENCE). FIXME: screenshot with arrows FIXME: filter for quality? FIXME: load reference and the tabular vcf file into JBrowse/Artemis/IGV to view the genome and the snps. FIXME: is there anything we are looking for in particular? e.g. number of variants, existing known variants, variants in particular genes, AMR variants?","title":"Output"},{"location":"scrapbook/dna/snps/snippy/#next","text":"","title":"Next"},{"location":"scrapbook/dna/snps/background/background/","text":"Variants: Background [from LSCC docs] Identifying SNPs There are many methods of identifying SNPs, but all rely on the per-base evidence provided by all the reads that have mapped to particular position in the sequence. So, it s useful to aggregate the evidence from all reads that relate to a particular base in the sequence. One method is to generate a pileup: a summary of sequence information from the entire set of reads across each relevant base in the reference sequence along with quality metrics relating to base quality and the mapping quality for each/all reads. see Pileup file information FIXME: can t link","title":"Variants: Background"},{"location":"scrapbook/dna/snps/background/background/#variants-background","text":"[from LSCC docs]","title":"Variants: Background"},{"location":"scrapbook/dna/snps/background/background/#identifying-snps","text":"There are many methods of identifying SNPs, but all rely on the per-base evidence provided by all the reads that have mapped to particular position in the sequence. So, it s useful to aggregate the evidence from all reads that relate to a particular base in the sequence. One method is to generate a pileup: a summary of sequence information from the entire set of reads across each relevant base in the reference sequence along with quality metrics relating to base quality and the mapping quality for each/all reads. see Pileup file information FIXME: can t link","title":"Identifying SNPs"},{"location":"scrapbook/how_to/overview/","text":"Overview - Advanced Training This section contains training modules for microbial genomics analyses. All analyses are performed using the microbial GVL (and occasionally, additional web-based tools), via the Galaxy interface or using the commandline. Genome assembly: from raw DNA sequence reads to assembled genome. in Galaxy - using Illumina data on the commandline - using Illumina data in the mGVL SMRTPortal - using PacBio data on the commandline - using both PacBio data and Illumina data Genome annotation: from assembled genome to annotated genome. in Galaxy on the commandline Core and pan genomes: from assembled genomes to lists of shared genes and accessory genes. on the commandline Differential gene expression: from raw RNA sequence reads from two conditions to a list of differentially expressed genes. in Galaxy For more information about using the GVL see http://genome.edu.au","title":"Overview - Advanced Training"},{"location":"scrapbook/how_to/overview/#overview-advanced-training","text":"This section contains training modules for microbial genomics analyses. All analyses are performed using the microbial GVL (and occasionally, additional web-based tools), via the Galaxy interface or using the commandline. Genome assembly: from raw DNA sequence reads to assembled genome. in Galaxy - using Illumina data on the commandline - using Illumina data in the mGVL SMRTPortal - using PacBio data on the commandline - using both PacBio data and Illumina data Genome annotation: from assembled genome to annotated genome. in Galaxy on the commandline Core and pan genomes: from assembled genomes to lists of shared genes and accessory genes. on the commandline Differential gene expression: from raw RNA sequence reads from two conditions to a list of differentially expressed genes. in Galaxy For more information about using the GVL see http://genome.edu.au","title":"Overview - Advanced Training"},{"location":"scrapbook/how_to/annotation/anno_galaxy/","text":"Annotation - Galaxy","title":"Annotation - Galaxy"},{"location":"scrapbook/how_to/annotation/anno_galaxy/#annotation-galaxy","text":"","title":"Annotation - Galaxy"},{"location":"scrapbook/how_to/assembly/assembly_galaxy/","text":"Assembly - Galaxy","title":"Assembly - Galaxy"},{"location":"scrapbook/how_to/assembly/assembly_galaxy/#assembly-galaxy","text":"","title":"Assembly - Galaxy"},{"location":"scrapbook/how_to/pan/roary/","text":"Roary","title":"Roary"},{"location":"scrapbook/how_to/pan/roary/#roary","text":"","title":"Roary"},{"location":"scrapbook/met/data/","text":"Metabolomics Data Even less idea!","title":"Metabolomics Data"},{"location":"scrapbook/met/data/#metabolomics-data","text":"Even less idea!","title":"Metabolomics Data"},{"location":"scrapbook/met/qc/","text":"Metabolomics QC","title":"Metabolomics QC"},{"location":"scrapbook/met/qc/#metabolomics-qc","text":"","title":"Metabolomics QC"},{"location":"scrapbook/pro/data/","text":"Proteomics Data No idea!","title":"Proteomics Data"},{"location":"scrapbook/pro/data/#proteomics-data","text":"No idea!","title":"Proteomics Data"},{"location":"scrapbook/pro/qc/","text":"Proteomics QC Need a good protein reference FASTA to match against. Quantitative?","title":"Proteomics QC"},{"location":"scrapbook/pro/qc/#proteomics-qc","text":"Need a good protein reference FASTA to match against. Quantitative?","title":"Proteomics QC"},{"location":"scrapbook/protocols/annotation/","text":"Annotation from raw reads to annotated genome Introduction FIXME Learning Objectives At the end of this tutorial you should be able to: FIXME FIXME FIXME Main Tutorial FIXME Note Please take note of this because we care about you. Hint This is a useful tip for you because we like you. Warning Please be careful here, you could really mess things up. What next? FIXME","title":"Annotation"},{"location":"scrapbook/protocols/annotation/#annotation","text":"from raw reads to annotated genome","title":"Annotation"},{"location":"scrapbook/protocols/annotation/#introduction","text":"FIXME","title":"Introduction"},{"location":"scrapbook/protocols/annotation/#learning-objectives","text":"At the end of this tutorial you should be able to: FIXME FIXME FIXME","title":"Learning Objectives"},{"location":"scrapbook/protocols/annotation/#main-tutorial","text":"FIXME Note Please take note of this because we care about you. Hint This is a useful tip for you because we like you. Warning Please be careful here, you could really mess things up.","title":"Main Tutorial"},{"location":"scrapbook/protocols/annotation/#what-next","text":"FIXME","title":"What next?"},{"location":"scrapbook/protocols/assembly/","text":"De novo assembly Introduction FIXME Learning Objectives At the end of this tutorial you should be able to: FIXME FIXME FIXME Main Tutorial FIXME Note Please take note of this because we care about you. Hint This is a useful tip for you because we like you. Warning Please be careful here, you could really mess things up. What next? FIXME","title":"De novo assembly"},{"location":"scrapbook/protocols/assembly/#de-novo-assembly","text":"","title":"De novo assembly"},{"location":"scrapbook/protocols/assembly/#introduction","text":"FIXME","title":"Introduction"},{"location":"scrapbook/protocols/assembly/#learning-objectives","text":"At the end of this tutorial you should be able to: FIXME FIXME FIXME","title":"Learning Objectives"},{"location":"scrapbook/protocols/assembly/#main-tutorial","text":"FIXME Note Please take note of this because we care about you. Hint This is a useful tip for you because we like you. Warning Please be careful here, you could really mess things up.","title":"Main Tutorial"},{"location":"scrapbook/protocols/assembly/#what-next","text":"FIXME","title":"What next?"},{"location":"scrapbook/protocols/dge/","text":"Differential gene expression from raw RNA reads to statistical tests and viz of DGE Introduction FIXME Learning Objectives At the end of this tutorial you should be able to: FIXME FIXME FIXME Main Tutorial FIXME Note Please take note of this because we care about you. Hint This is a useful tip for you because we like you. Warning Please be careful here, you could really mess things up. What next? FIXME","title":"Differential gene expression"},{"location":"scrapbook/protocols/dge/#differential-gene-expression","text":"from raw RNA reads to statistical tests and viz of DGE","title":"Differential gene expression"},{"location":"scrapbook/protocols/dge/#introduction","text":"FIXME","title":"Introduction"},{"location":"scrapbook/protocols/dge/#learning-objectives","text":"At the end of this tutorial you should be able to: FIXME FIXME FIXME","title":"Learning Objectives"},{"location":"scrapbook/protocols/dge/#main-tutorial","text":"FIXME Note Please take note of this because we care about you. Hint This is a useful tip for you because we like you. Warning Please be careful here, you could really mess things up.","title":"Main Tutorial"},{"location":"scrapbook/protocols/dge/#what-next","text":"FIXME","title":"What next?"},{"location":"scrapbook/protocols/overview/","text":"Overview FIXME: ideas - but we can delete this section if we want Workflows in Galaxy, with flowcharts Command line pipelines, make flowcharts Show various options at each stage (e.g. use tool x or y) Exemplar pipelines e.g. DNA reads to genome (llumina) e.g. FastQC, trimmomatic, Spades, view DNA reads to genome (PacBio) e.g. PacBio smrt portal DNA reads to annotated genome e.g. FastQC, trimmomatic, Spades, Prokka, view DNA reads to variants e.g. FastQC, trimmomatic, snippy, view several sets of DNA reads to genomes: core/pan e.g. FastQC, trimmomatic, Spades, Prokka, Roary, phandango several sets of DNA to compare structural variants e.g. FastQC, trimmomatic, Spades, Mauve RNA reads to list of DGE e.g. xxx","title":"Overview"},{"location":"scrapbook/protocols/overview/#overview","text":"FIXME: ideas - but we can delete this section if we want Workflows in Galaxy, with flowcharts Command line pipelines, make flowcharts Show various options at each stage (e.g. use tool x or y) Exemplar pipelines e.g. DNA reads to genome (llumina) e.g. FastQC, trimmomatic, Spades, view DNA reads to genome (PacBio) e.g. PacBio smrt portal DNA reads to annotated genome e.g. FastQC, trimmomatic, Spades, Prokka, view DNA reads to variants e.g. FastQC, trimmomatic, snippy, view several sets of DNA reads to genomes: core/pan e.g. FastQC, trimmomatic, Spades, Prokka, Roary, phandango several sets of DNA to compare structural variants e.g. FastQC, trimmomatic, Spades, Mauve RNA reads to list of DGE e.g. xxx","title":"Overview"},{"location":"scrapbook/protocols/variants/","text":"Variant finding from raw reads to snps Introduction FIXME Learning Objectives At the end of this tutorial you should be able to: FIXME FIXME FIXME Main Tutorial FIXME Note Please take note of this because we care about you. Hint This is a useful tip for you because we like you. Warning Please be careful here, you could really mess things up. What next? FIXME","title":"Variant finding"},{"location":"scrapbook/protocols/variants/#variant-finding","text":"from raw reads to snps","title":"Variant finding"},{"location":"scrapbook/protocols/variants/#introduction","text":"FIXME","title":"Introduction"},{"location":"scrapbook/protocols/variants/#learning-objectives","text":"At the end of this tutorial you should be able to: FIXME FIXME FIXME","title":"Learning Objectives"},{"location":"scrapbook/protocols/variants/#main-tutorial","text":"FIXME Note Please take note of this because we care about you. Hint This is a useful tip for you because we like you. Warning Please be careful here, you could really mess things up.","title":"Main Tutorial"},{"location":"scrapbook/protocols/variants/#what-next","text":"FIXME","title":"What next?"},{"location":"scrapbook/rna/data/","text":"Transcriptomics Data RNA-Seq FASTQ","title":"Transcriptomics Data"},{"location":"scrapbook/rna/data/#transcriptomics-data","text":"RNA-Seq FASTQ","title":"Transcriptomics Data"},{"location":"scrapbook/rna/denovo-cmdline-trinity/","text":"","title":"Denovo cmdline trinity"},{"location":"scrapbook/rna/denovo/","text":"De novo transcriptome assembly Command line Oases StringTie Trinity Galaxy Trinity","title":"*De novo* transcriptome assembly"},{"location":"scrapbook/rna/denovo/#de-novo-transcriptome-assembly","text":"","title":"De novo transcriptome assembly"},{"location":"scrapbook/rna/denovo/#command-line","text":"Oases StringTie Trinity","title":"Command line"},{"location":"scrapbook/rna/denovo/#galaxy","text":"Trinity","title":"Galaxy"},{"location":"scrapbook/rna/dge/","text":"","title":"Dge"},{"location":"scrapbook/rna/qc/","text":"Transcriptomics QC Align to ref? rRNA levels","title":"Transcriptomics QC"},{"location":"scrapbook/rna/qc/#transcriptomics-qc","text":"Align to ref? rRNA levels","title":"Transcriptomics QC"},{"location":"scrapbook/rna/denovo/oases/","text":"Oases cmdline might be in galaxy?","title":"Oases"},{"location":"scrapbook/rna/denovo/oases/#oases","text":"cmdline might be in galaxy?","title":"Oases"},{"location":"scrapbook/rna/denovo/stringtie/","text":"StringTie cmd line","title":"StringTie"},{"location":"scrapbook/rna/denovo/stringtie/#stringtie","text":"cmd line","title":"StringTie"},{"location":"scrapbook/rna/denovo/trinity/","text":"Trinity on cmdline In Galaxy too?","title":"Trinity"},{"location":"scrapbook/rna/denovo/trinity/#trinity","text":"on cmdline In Galaxy too?","title":"Trinity"},{"location":"scrapbook/rna/dge/DESeq/","text":"DESeq","title":"DESeq"},{"location":"scrapbook/rna/dge/DESeq/#deseq","text":"","title":"DESeq"},{"location":"scrapbook/rna/dge/HTSeq/","text":"HTSeq","title":"HTSeq"},{"location":"scrapbook/rna/dge/HTSeq/#htseq","text":"","title":"HTSeq"},{"location":"scrapbook/rna/dge/degust/","text":"Degust for viz of DGE results Following the test of differential gene expresssion DGE in Galaxy , we can use the tool Degust to visualize the results. Link to Degust on github FIXME: Degust to be installed on mGVL Galaxy Run Input: read counts (htseq output) Configuration: settings Name - for data Info columns - info for each gene - how to choose Add condition ( for all conditions) Set min read count to 10 ( 10 reads = gene omitted) Save, view Execute Output Compare the expression of genes between conditions 1 and 2","title":"Degust"},{"location":"scrapbook/rna/dge/degust/#degust","text":"for viz of DGE results Following the test of differential gene expresssion DGE in Galaxy , we can use the tool Degust to visualize the results. Link to Degust on github FIXME: Degust to be installed on mGVL Galaxy","title":"Degust"},{"location":"scrapbook/rna/dge/degust/#run","text":"Input: read counts (htseq output) Configuration: settings Name - for data Info columns - info for each gene - how to choose Add condition ( for all conditions) Set min read count to 10 ( 10 reads = gene omitted) Save, view Execute","title":"Run"},{"location":"scrapbook/rna/dge/degust/#output","text":"Compare the expression of genes between conditions 1 and 2","title":"Output"},{"location":"scrapbook/rna/dge/galaxy/","text":"DGE using Galaxy No idea - i think there are some workflows but we dont need Cufflinks etc This tutorial demonstrates how to quantify differences in gene expression between two bacterial samples. We will use RNA-seq data from the samples, and then use several tools to test for differential gene expression. We can then examine the results in the program Degust Pre-requisites your own mGVL instance knowledge: Galaxy knowledge: Differential gene expression Start open your mGVL galaxy instance. Input data RNA-seq reads from the samples Bacterial species E. coli from study SRP027344 from EBI ENA We will use wildtype (called WTA and WTB) vs. condition 23 (called 23A and 23B) [what was this condition? grown in different media?] Illumina, single-end, 51-bp a reference genome E coli: NCBI Reference Sequence: NC_000913.3 in fasta format FIXME: Get data from GenomeSpace/ or saved galaxy history Map transcripts We need to map the transcripts to a reference genome. Bacteria don t need splice-aware mapping (don t have introns) Galaxy: tools: NGS Analysis: NGS Mapping: Map with BWA-MEM Will you select a reference genome from your history or use a built-in index?: Use a genome from history and build index Use the following dataset as the reference sequence: E_coli_ref_genome Single or Paired-end reads: single Select FASTQ dataset : click on the multiple files button (image) in centre make sure all 4 FASTQ files are in there hold down shift to select them all (they turn blue) this will map each set of transcripts to the ref genome, so there will be 4 output files Visualize the mapped reads The mapped reads are now as .bam files which can t be viewed by just clicking on them. [should we use JBrowse?] on local computer, install/open IGV Genomes- load genome from file- ../igv/genomes/NC_000913.2 (Ecoli ref) then in galaxy, go to the mapped transcripts and click on IGV local in IGV, zoom in (top right) the bottom pane is the ref sequence / zoom in and out to see the reads. Count reads Generate read counts per gene For each transcriptome, count the number of transcripts per gene/feature E.g. with HTSeq data: this is part of the Colombus database of bacterial expression http://nar.oxfordjournals.org/content/suppl/2013/10/29/gkt1086.DC1/nar-02461-data-e-2013-File001.pdf wildtype and two mutants (all grown under minimal conditions or just mutants?) hopefully some expression changes bn condition 23 and wt so that can be viewed in the Degust plots","title":"DGE using Galaxy"},{"location":"scrapbook/rna/dge/galaxy/#dge-using-galaxy","text":"No idea - i think there are some workflows but we dont need Cufflinks etc This tutorial demonstrates how to quantify differences in gene expression between two bacterial samples. We will use RNA-seq data from the samples, and then use several tools to test for differential gene expression. We can then examine the results in the program Degust","title":"DGE using Galaxy"},{"location":"scrapbook/rna/dge/galaxy/#pre-requisites","text":"your own mGVL instance knowledge: Galaxy knowledge: Differential gene expression","title":"Pre-requisites"},{"location":"scrapbook/rna/dge/galaxy/#start","text":"open your mGVL galaxy instance.","title":"Start"},{"location":"scrapbook/rna/dge/galaxy/#input-data","text":"RNA-seq reads from the samples Bacterial species E. coli from study SRP027344 from EBI ENA We will use wildtype (called WTA and WTB) vs. condition 23 (called 23A and 23B) [what was this condition? grown in different media?] Illumina, single-end, 51-bp a reference genome E coli: NCBI Reference Sequence: NC_000913.3 in fasta format FIXME: Get data from GenomeSpace/ or saved galaxy history","title":"Input data"},{"location":"scrapbook/rna/dge/galaxy/#map-transcripts","text":"We need to map the transcripts to a reference genome. Bacteria don t need splice-aware mapping (don t have introns) Galaxy: tools: NGS Analysis: NGS Mapping: Map with BWA-MEM Will you select a reference genome from your history or use a built-in index?: Use a genome from history and build index Use the following dataset as the reference sequence: E_coli_ref_genome Single or Paired-end reads: single Select FASTQ dataset : click on the multiple files button (image) in centre make sure all 4 FASTQ files are in there hold down shift to select them all (they turn blue) this will map each set of transcripts to the ref genome, so there will be 4 output files","title":"Map transcripts"},{"location":"scrapbook/rna/dge/galaxy/#visualize-the-mapped-reads","text":"The mapped reads are now as .bam files which can t be viewed by just clicking on them. [should we use JBrowse?] on local computer, install/open IGV Genomes- load genome from file- ../igv/genomes/NC_000913.2 (Ecoli ref) then in galaxy, go to the mapped transcripts and click on IGV local in IGV, zoom in (top right) the bottom pane is the ref sequence / zoom in and out to see the reads.","title":"Visualize the mapped reads"},{"location":"scrapbook/rna/dge/galaxy/#count-reads","text":"Generate read counts per gene For each transcriptome, count the number of transcripts per gene/feature E.g. with HTSeq data: this is part of the Colombus database of bacterial expression http://nar.oxfordjournals.org/content/suppl/2013/10/29/gkt1086.DC1/nar-02461-data-e-2013-File001.pdf wildtype and two mutants (all grown under minimal conditions or just mutants?) hopefully some expression changes bn condition 23 and wt so that can be viewed in the Degust plots","title":"Count reads"},{"location":"scrapbook/snps/","text":"Variant calling Background Variant calling is the process of identifying differences between to genome samples. Usually differences are limited to single nucleotide polymorphisms (SNPs) and small insertions and deletions (indels). Larger structural variation such as inversions, duplications and large deletions are not typically covered by variant calling . Learning Objectives At the end of this tutorial you should be able to: Find variants between a reference genome and a set of reads Determine the effect of those variants on genomic features Understand if the SNP is potentially affecting the phenotype Experiment FIXME: You are working on a bug and you notice one of them is forming smaller colonies than normal. You want to find out why this msall colony vairant (SCV) is doing at the DNA level. Prepare reference FIXME: need FASTA or prefer an annotated genome eg. GBK or GFF3 of the original strain you used Note Please make sure your reference genome includes all chromosomes and plasmids Align reads FIXME: BWA MEM align the SCV reads Call variants FIXME: freebayes? varscan2 ? FIXME: talk about multimapping reads? Filter variants FIXME: vcffilter? something else? mindepth, homozygous? Annotate consequencs FIXME: snpEff - but it is hard to add a genome Hint Just use Snippy and all this will happen magically? What next? SNPs can be used to build phylogentic trees .","title":"Variant calling"},{"location":"scrapbook/snps/#variant-calling","text":"","title":"Variant calling"},{"location":"scrapbook/snps/#background","text":"Variant calling is the process of identifying differences between to genome samples. Usually differences are limited to single nucleotide polymorphisms (SNPs) and small insertions and deletions (indels). Larger structural variation such as inversions, duplications and large deletions are not typically covered by variant calling .","title":"Background"},{"location":"scrapbook/snps/#learning-objectives","text":"At the end of this tutorial you should be able to: Find variants between a reference genome and a set of reads Determine the effect of those variants on genomic features Understand if the SNP is potentially affecting the phenotype","title":"Learning Objectives"},{"location":"scrapbook/snps/#experiment","text":"FIXME: You are working on a bug and you notice one of them is forming smaller colonies than normal. You want to find out why this msall colony vairant (SCV) is doing at the DNA level.","title":"Experiment"},{"location":"scrapbook/snps/#prepare-reference","text":"FIXME: need FASTA or prefer an annotated genome eg. GBK or GFF3 of the original strain you used Note Please make sure your reference genome includes all chromosomes and plasmids","title":"Prepare reference"},{"location":"scrapbook/snps/#align-reads","text":"FIXME: BWA MEM align the SCV reads","title":"Align reads"},{"location":"scrapbook/snps/#call-variants","text":"FIXME: freebayes? varscan2 ? FIXME: talk about multimapping reads?","title":"Call variants"},{"location":"scrapbook/snps/#filter-variants","text":"FIXME: vcffilter? something else? mindepth, homozygous?","title":"Filter variants"},{"location":"scrapbook/snps/#annotate-consequencs","text":"FIXME: snpEff - but it is hard to add a genome Hint Just use Snippy and all this will happen magically?","title":"Annotate consequencs"},{"location":"scrapbook/snps/#what-next","text":"SNPs can be used to build phylogentic trees .","title":"What next?"},{"location":"scrapbook/trees/","text":"Phylogenomics Introduction FIXME Learning Objectives At the end of this tutorial you should be able to: FIXME FIXME FIXME Main Tutorial FIXME Note Please take note of this because we care about you. Hint This is a useful tip for you because we like you. Warning Please be careful here, you could really mess things up. What next? FIXME","title":"Phylogenomics"},{"location":"scrapbook/trees/#phylogenomics","text":"","title":"Phylogenomics"},{"location":"scrapbook/trees/#introduction","text":"FIXME","title":"Introduction"},{"location":"scrapbook/trees/#learning-objectives","text":"At the end of this tutorial you should be able to: FIXME FIXME FIXME","title":"Learning Objectives"},{"location":"scrapbook/trees/#main-tutorial","text":"FIXME Note Please take note of this because we care about you. Hint This is a useful tip for you because we like you. Warning Please be careful here, you could really mess things up.","title":"Main Tutorial"},{"location":"scrapbook/trees/#what-next","text":"FIXME","title":"What next?"},{"location":"scrapbook/viz/artemis/","text":"Artemis link to this tutorial/ or vice versa. FIXME: change/make more generic for this section?","title":"Artemis"},{"location":"scrapbook/viz/artemis/#artemis","text":"link to this tutorial/ or vice versa. FIXME: change/make more generic for this section?","title":"Artemis"},{"location":"scrapbook/viz/bandage/","text":"Bandage Bandage is a program to visualize a genome assembly. Link to code on github. Start Open your mGVL dashboard and click on the link to the Lubuntu desktop. A web window should appear called noVNC . This shows the virtual desktop. Open the Terminal in this VNC. type in: /mnt/gvl/apps/bandage/Bandage The Bandage window will appear. Input data Click on File: Load graph . Select a graph to upload. FIXME: put in example bacterial genome graph with a feature that bandage reveals. E.g. a FASTG output file from Spades. Run Bandage Under Graph drawing , click on Draw graph . A graph of the contigs and their connections will appear. FIXME: add a series of steps that can be done that will show some important feature. e.g. Graph display : Colour by read depth Node labels , check box next to Read depth . select some nodes Output : Web BLAST selected nodes etc. Output Output : Save entire graph to GFA (FIXME: where does this save; what can you use it for next). Next More information","title":"Bandage"},{"location":"scrapbook/viz/bandage/#bandage","text":"Bandage is a program to visualize a genome assembly. Link to code on github.","title":"Bandage"},{"location":"scrapbook/viz/bandage/#start","text":"Open your mGVL dashboard and click on the link to the Lubuntu desktop. A web window should appear called noVNC . This shows the virtual desktop. Open the Terminal in this VNC. type in: /mnt/gvl/apps/bandage/Bandage The Bandage window will appear.","title":"Start"},{"location":"scrapbook/viz/bandage/#input-data","text":"Click on File: Load graph . Select a graph to upload. FIXME: put in example bacterial genome graph with a feature that bandage reveals. E.g. a FASTG output file from Spades.","title":"Input data"},{"location":"scrapbook/viz/bandage/#run-bandage","text":"Under Graph drawing , click on Draw graph . A graph of the contigs and their connections will appear. FIXME: add a series of steps that can be done that will show some important feature. e.g. Graph display : Colour by read depth Node labels , check box next to Read depth . select some nodes Output : Web BLAST selected nodes etc.","title":"Run Bandage"},{"location":"scrapbook/viz/bandage/#output","text":"Output : Save entire graph to GFA (FIXME: where does this save; what can you use it for next).","title":"Output"},{"location":"scrapbook/viz/bandage/#next","text":"","title":"Next"},{"location":"scrapbook/viz/bandage/#more-information","text":"","title":"More information"},{"location":"scrapbook/viz/degust/","text":"","title":"Home"},{"location":"scrapbook/viz/jbrowse/","text":"","title":"Home"},{"location":"scrapbook/viz/phandango/","text":"Phandango FIXME: current Firefox on mGVL Lubuntu desktop is too old to use Phandango FIXME: ideally: sepsis related: would be good to see clade x has AMR genes? e.g. this example shows a subclade which is ceftriaxone susceptable but azithromycin resistant https://github.com/jameshadfield/phandango/wiki/Panel%20Layout#metadata Phandango is a tool to vizualize genome phylogenies. Start prepare input files (e.g. from Roary output): gene_presence_absence.csv : gene name, various information, then a column for each sample - if the gene is present in that sample, it is listed here with an appended gene ID. tree.newick : a phylogenetic tree based on an alignment of core genes. (Note: this is not a default Roary output and must be specified by creating an alignment of core genes, and then building a phylogenetic tree). navigate to these files in your mGVL (e.g. ssh in terminal) and move them to your public_html folder. open your public_html folder (e.g. http://mgvl_IP/public/username or similar), and download to your local computer. Run open the Phandango webpage. drag and drop these files onto the webpage. now we can see the tree on the left, and the core and accessory genome aligned to each sample. things to look at: top tabs: settings - change the displayed labels change panel sizes - drag grey circles at the edges of each panel line graph? Save output image press p to save the displayed data as a vector SVG file. What next","title":"Phandango"},{"location":"scrapbook/viz/phandango/#phandango","text":"FIXME: current Firefox on mGVL Lubuntu desktop is too old to use Phandango FIXME: ideally: sepsis related: would be good to see clade x has AMR genes? e.g. this example shows a subclade which is ceftriaxone susceptable but azithromycin resistant https://github.com/jameshadfield/phandango/wiki/Panel%20Layout#metadata Phandango is a tool to vizualize genome phylogenies.","title":"Phandango"},{"location":"scrapbook/viz/phandango/#start","text":"prepare input files (e.g. from Roary output): gene_presence_absence.csv : gene name, various information, then a column for each sample - if the gene is present in that sample, it is listed here with an appended gene ID. tree.newick : a phylogenetic tree based on an alignment of core genes. (Note: this is not a default Roary output and must be specified by creating an alignment of core genes, and then building a phylogenetic tree). navigate to these files in your mGVL (e.g. ssh in terminal) and move them to your public_html folder. open your public_html folder (e.g. http://mgvl_IP/public/username or similar), and download to your local computer.","title":"Start"},{"location":"scrapbook/viz/phandango/#run","text":"open the Phandango webpage. drag and drop these files onto the webpage. now we can see the tree on the left, and the core and accessory genome aligned to each sample. things to look at: top tabs: settings - change the displayed labels change panel sizes - drag grey circles at the edges of each panel line graph?","title":"Run"},{"location":"scrapbook/viz/phandango/#save-output-image","text":"press p to save the displayed data as a vector SVG file.","title":"Save output image"},{"location":"scrapbook/viz/phandango/#what-next","text":"","title":"What next"}]}